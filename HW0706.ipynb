{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:09:56.757845Z",
     "start_time": "2018-07-06T03:09:55.761830Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sitibanc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Can network fit random labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:19:24.922964Z",
     "start_time": "2018-07-06T03:19:24.527348Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Normalize\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# Randomized Training Labels\n",
    "y_train_random = y_train\n",
    "np.random.shuffle(y_train_random)\n",
    "# Get One-Hot Labels\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_train_random = to_categorical(y_train_random, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:10:51.933596Z",
     "start_time": "2018-07-06T03:10:51.884190Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 32)                25120     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 25,818\n",
      "Trainable params: 25,818\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model0 = Sequential()\n",
    "model0.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model0.add(Dense(32, activation=\"relu\", name=\"fc1\"))\n",
    "model0.add(Dense(16, activation=\"relu\", name=\"fc2\"))\n",
    "model0.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model0.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:01:49.501603Z",
     "start_time": "2018-07-06T03:01:20.289386Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3000 samples, validate on 3000 samples\n",
      "Epoch 1/500\n",
      "3000/3000 [==============================] - 0s 61us/step - loss: 2.3178 - acc: 0.1087 - val_loss: 2.3040 - val_acc: 0.1033\n",
      "Epoch 2/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 2.2938 - acc: 0.1233 - val_loss: 2.2990 - val_acc: 0.0850\n",
      "Epoch 3/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2843 - acc: 0.1367 - val_loss: 2.2984 - val_acc: 0.0863\n",
      "Epoch 4/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2763 - acc: 0.1483 - val_loss: 2.3033 - val_acc: 0.0840\n",
      "Epoch 5/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2676 - acc: 0.1610 - val_loss: 2.3042 - val_acc: 0.0863\n",
      "Epoch 6/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2568 - acc: 0.1687 - val_loss: 2.3112 - val_acc: 0.0723\n",
      "Epoch 7/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2483 - acc: 0.1657 - val_loss: 2.3236 - val_acc: 0.0670\n",
      "Epoch 8/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2337 - acc: 0.1920 - val_loss: 2.3166 - val_acc: 0.0830\n",
      "Epoch 9/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 2.2191 - acc: 0.1977 - val_loss: 2.3319 - val_acc: 0.0697\n",
      "Epoch 10/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.2039 - acc: 0.2003 - val_loss: 2.3434 - val_acc: 0.0720\n",
      "Epoch 11/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.1894 - acc: 0.2167 - val_loss: 2.3504 - val_acc: 0.0853\n",
      "Epoch 12/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.1720 - acc: 0.2220 - val_loss: 2.3388 - val_acc: 0.0903\n",
      "Epoch 13/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.1546 - acc: 0.2410 - val_loss: 2.3816 - val_acc: 0.0750\n",
      "Epoch 14/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 2.1361 - acc: 0.2437 - val_loss: 2.3844 - val_acc: 0.0640\n",
      "Epoch 15/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.1214 - acc: 0.2453 - val_loss: 2.4013 - val_acc: 0.0760\n",
      "Epoch 16/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.1029 - acc: 0.2540 - val_loss: 2.4071 - val_acc: 0.0790\n",
      "Epoch 17/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.0851 - acc: 0.2663 - val_loss: 2.4152 - val_acc: 0.0877\n",
      "Epoch 18/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.0570 - acc: 0.2830 - val_loss: 2.4476 - val_acc: 0.0750\n",
      "Epoch 19/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 2.0324 - acc: 0.2980 - val_loss: 2.4521 - val_acc: 0.0813\n",
      "Epoch 20/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.0120 - acc: 0.3100 - val_loss: 2.4852 - val_acc: 0.0730\n",
      "Epoch 21/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.9844 - acc: 0.3253 - val_loss: 2.4923 - val_acc: 0.0823\n",
      "Epoch 22/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.9655 - acc: 0.3240 - val_loss: 2.5064 - val_acc: 0.0813\n",
      "Epoch 23/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.9361 - acc: 0.3320 - val_loss: 2.5169 - val_acc: 0.0837\n",
      "Epoch 24/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.9163 - acc: 0.3550 - val_loss: 2.5580 - val_acc: 0.0850\n",
      "Epoch 25/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.8917 - acc: 0.3493 - val_loss: 2.5605 - val_acc: 0.0837\n",
      "Epoch 26/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.8663 - acc: 0.3733 - val_loss: 2.5904 - val_acc: 0.0773\n",
      "Epoch 27/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.8417 - acc: 0.3827 - val_loss: 2.5954 - val_acc: 0.0843\n",
      "Epoch 28/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.8277 - acc: 0.3747 - val_loss: 2.6348 - val_acc: 0.0850\n",
      "Epoch 29/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.8046 - acc: 0.3897 - val_loss: 2.6410 - val_acc: 0.0817\n",
      "Epoch 30/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.7775 - acc: 0.4047 - val_loss: 2.6668 - val_acc: 0.0887\n",
      "Epoch 31/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.7537 - acc: 0.4043 - val_loss: 2.6545 - val_acc: 0.0863\n",
      "Epoch 32/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.7344 - acc: 0.4177 - val_loss: 2.7186 - val_acc: 0.0843\n",
      "Epoch 33/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.7116 - acc: 0.4220 - val_loss: 2.7286 - val_acc: 0.0830\n",
      "Epoch 34/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.6911 - acc: 0.4383 - val_loss: 2.7384 - val_acc: 0.0850\n",
      "Epoch 35/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.6640 - acc: 0.4453 - val_loss: 2.7754 - val_acc: 0.0843\n",
      "Epoch 36/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 1.6566 - acc: 0.4497 - val_loss: 2.8130 - val_acc: 0.0790\n",
      "Epoch 37/500\n",
      "3000/3000 [==============================] - 0s 23us/step - loss: 1.6253 - acc: 0.4497 - val_loss: 2.8122 - val_acc: 0.0930\n",
      "Epoch 38/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 1.6058 - acc: 0.4647 - val_loss: 2.8552 - val_acc: 0.0820\n",
      "Epoch 39/500\n",
      "3000/3000 [==============================] - 0s 32us/step - loss: 1.5771 - acc: 0.4810 - val_loss: 2.8890 - val_acc: 0.0783\n",
      "Epoch 40/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 1.5583 - acc: 0.4843 - val_loss: 2.8960 - val_acc: 0.0830\n",
      "Epoch 41/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.5487 - acc: 0.4893 - val_loss: 2.9197 - val_acc: 0.0857\n",
      "Epoch 42/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.5250 - acc: 0.4977 - val_loss: 2.9410 - val_acc: 0.0913\n",
      "Epoch 43/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.4968 - acc: 0.5077 - val_loss: 2.9791 - val_acc: 0.0850\n",
      "Epoch 44/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.4775 - acc: 0.5160 - val_loss: 2.9867 - val_acc: 0.0860\n",
      "Epoch 45/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.4639 - acc: 0.5173 - val_loss: 3.0141 - val_acc: 0.0893\n",
      "Epoch 46/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.4452 - acc: 0.5373 - val_loss: 3.1238 - val_acc: 0.0803\n",
      "Epoch 47/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.4275 - acc: 0.5287 - val_loss: 3.0821 - val_acc: 0.0917\n",
      "Epoch 48/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.4044 - acc: 0.5447 - val_loss: 3.0972 - val_acc: 0.0833\n",
      "Epoch 49/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.3891 - acc: 0.5517 - val_loss: 3.1490 - val_acc: 0.0923\n",
      "Epoch 50/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.3752 - acc: 0.5483 - val_loss: 3.1712 - val_acc: 0.0820\n",
      "Epoch 51/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.3522 - acc: 0.5583 - val_loss: 3.1861 - val_acc: 0.0853\n",
      "Epoch 52/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.3347 - acc: 0.5713 - val_loss: 3.2583 - val_acc: 0.0820\n",
      "Epoch 53/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.3193 - acc: 0.5803 - val_loss: 3.2327 - val_acc: 0.0887\n",
      "Epoch 54/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.3105 - acc: 0.5753 - val_loss: 3.2779 - val_acc: 0.0830\n",
      "Epoch 55/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.2908 - acc: 0.5900 - val_loss: 3.3304 - val_acc: 0.0810\n",
      "Epoch 56/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.2695 - acc: 0.5973 - val_loss: 3.3243 - val_acc: 0.0817\n",
      "Epoch 57/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.2555 - acc: 0.5960 - val_loss: 3.4386 - val_acc: 0.0807\n",
      "Epoch 58/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.2410 - acc: 0.6060 - val_loss: 3.4099 - val_acc: 0.0887\n",
      "Epoch 59/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.2213 - acc: 0.6130 - val_loss: 3.4509 - val_acc: 0.0800\n",
      "Epoch 60/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.2047 - acc: 0.6253 - val_loss: 3.4720 - val_acc: 0.0840\n",
      "Epoch 61/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.1950 - acc: 0.6237 - val_loss: 3.5063 - val_acc: 0.0830\n",
      "Epoch 62/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.1784 - acc: 0.6253 - val_loss: 3.5525 - val_acc: 0.0827\n",
      "Epoch 63/500\n",
      "3000/3000 [==============================] - 0s 34us/step - loss: 1.1585 - acc: 0.6343 - val_loss: 3.6003 - val_acc: 0.0830\n",
      "Epoch 64/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 1.1534 - acc: 0.6320 - val_loss: 3.6106 - val_acc: 0.0847\n",
      "Epoch 65/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.1276 - acc: 0.6483 - val_loss: 3.5888 - val_acc: 0.0853\n",
      "Epoch 66/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.1168 - acc: 0.6583 - val_loss: 3.6840 - val_acc: 0.0820\n",
      "Epoch 67/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.1108 - acc: 0.6520 - val_loss: 3.7028 - val_acc: 0.0817\n",
      "Epoch 68/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0959 - acc: 0.6593 - val_loss: 3.7673 - val_acc: 0.0850\n",
      "Epoch 69/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0963 - acc: 0.6537 - val_loss: 3.7461 - val_acc: 0.0867\n",
      "Epoch 70/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0821 - acc: 0.6540 - val_loss: 3.7477 - val_acc: 0.0787\n",
      "Epoch 71/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0571 - acc: 0.6673 - val_loss: 3.7681 - val_acc: 0.0880\n",
      "Epoch 72/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0331 - acc: 0.6750 - val_loss: 3.8621 - val_acc: 0.0850\n",
      "Epoch 73/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0253 - acc: 0.6847 - val_loss: 3.8615 - val_acc: 0.0827\n",
      "Epoch 74/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0148 - acc: 0.6850 - val_loss: 3.9325 - val_acc: 0.0867\n",
      "Epoch 75/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0018 - acc: 0.6893 - val_loss: 3.9095 - val_acc: 0.0890\n",
      "Epoch 76/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9883 - acc: 0.6970 - val_loss: 4.0123 - val_acc: 0.0820\n",
      "Epoch 77/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9754 - acc: 0.7057 - val_loss: 4.0277 - val_acc: 0.0840\n",
      "Epoch 78/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9591 - acc: 0.7113 - val_loss: 3.9962 - val_acc: 0.0843\n",
      "Epoch 79/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9425 - acc: 0.7187 - val_loss: 4.1044 - val_acc: 0.0843\n",
      "Epoch 80/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9360 - acc: 0.7190 - val_loss: 4.1327 - val_acc: 0.0837\n",
      "Epoch 81/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9206 - acc: 0.7217 - val_loss: 4.0884 - val_acc: 0.0867\n",
      "Epoch 82/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9057 - acc: 0.7260 - val_loss: 4.1721 - val_acc: 0.0903\n",
      "Epoch 83/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.9113 - acc: 0.7243 - val_loss: 4.2396 - val_acc: 0.0837\n",
      "Epoch 84/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8854 - acc: 0.7403 - val_loss: 4.2241 - val_acc: 0.0883\n",
      "Epoch 85/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8832 - acc: 0.7330 - val_loss: 4.2742 - val_acc: 0.0880\n",
      "Epoch 86/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8701 - acc: 0.7387 - val_loss: 4.3324 - val_acc: 0.0827\n",
      "Epoch 87/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8600 - acc: 0.7403 - val_loss: 4.3396 - val_acc: 0.0880\n",
      "Epoch 88/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8399 - acc: 0.7507 - val_loss: 4.3875 - val_acc: 0.0867\n",
      "Epoch 89/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.8389 - acc: 0.7447 - val_loss: 4.4886 - val_acc: 0.0797\n",
      "Epoch 90/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8309 - acc: 0.7467 - val_loss: 4.4659 - val_acc: 0.0870\n",
      "Epoch 91/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8151 - acc: 0.7677 - val_loss: 4.5502 - val_acc: 0.0853\n",
      "Epoch 92/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8021 - acc: 0.7627 - val_loss: 4.5246 - val_acc: 0.0880\n",
      "Epoch 93/500\n",
      "3000/3000 [==============================] - 0s 22us/step - loss: 0.7928 - acc: 0.7650 - val_loss: 4.5493 - val_acc: 0.0850\n",
      "Epoch 94/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 0.7763 - acc: 0.7737 - val_loss: 4.5884 - val_acc: 0.0857\n",
      "Epoch 95/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 0.7723 - acc: 0.7747 - val_loss: 4.6325 - val_acc: 0.0880\n",
      "Epoch 96/500\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.7682 - acc: 0.7753 - val_loss: 4.6559 - val_acc: 0.0867\n",
      "Epoch 97/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 0.7561 - acc: 0.7830 - val_loss: 4.7492 - val_acc: 0.0857\n",
      "Epoch 98/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.7446 - acc: 0.7837 - val_loss: 4.6604 - val_acc: 0.0927\n",
      "Epoch 99/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.7287 - acc: 0.7930 - val_loss: 4.7717 - val_acc: 0.0837\n",
      "Epoch 100/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.7194 - acc: 0.7973 - val_loss: 4.8380 - val_acc: 0.0830\n",
      "Epoch 101/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.7165 - acc: 0.7987 - val_loss: 4.7970 - val_acc: 0.0927\n",
      "Epoch 102/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6971 - acc: 0.8063 - val_loss: 4.8745 - val_acc: 0.0837\n",
      "Epoch 103/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6892 - acc: 0.8123 - val_loss: 4.9200 - val_acc: 0.0927\n",
      "Epoch 104/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6884 - acc: 0.8097 - val_loss: 4.9359 - val_acc: 0.0877\n",
      "Epoch 105/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6787 - acc: 0.8073 - val_loss: 4.9917 - val_acc: 0.0887\n",
      "Epoch 106/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.6649 - acc: 0.8127 - val_loss: 5.0322 - val_acc: 0.0910\n",
      "Epoch 107/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6608 - acc: 0.8167 - val_loss: 5.0304 - val_acc: 0.0903\n",
      "Epoch 108/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6514 - acc: 0.8183 - val_loss: 5.0471 - val_acc: 0.0910\n",
      "Epoch 109/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6400 - acc: 0.8200 - val_loss: 5.1256 - val_acc: 0.0890\n",
      "Epoch 110/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6257 - acc: 0.8297 - val_loss: 5.1042 - val_acc: 0.0930\n",
      "Epoch 111/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.6259 - acc: 0.8277 - val_loss: 5.1870 - val_acc: 0.0910\n",
      "Epoch 112/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6190 - acc: 0.8310 - val_loss: 5.2631 - val_acc: 0.0903\n",
      "Epoch 113/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6220 - acc: 0.8303 - val_loss: 5.1869 - val_acc: 0.0897\n",
      "Epoch 114/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.6095 - acc: 0.8327 - val_loss: 5.2920 - val_acc: 0.0847\n",
      "Epoch 115/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5917 - acc: 0.8383 - val_loss: 5.3740 - val_acc: 0.0850\n",
      "Epoch 116/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5844 - acc: 0.8440 - val_loss: 5.3261 - val_acc: 0.0920\n",
      "Epoch 117/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5715 - acc: 0.8477 - val_loss: 5.3668 - val_acc: 0.0897\n",
      "Epoch 118/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5719 - acc: 0.8430 - val_loss: 5.4245 - val_acc: 0.0917\n",
      "Epoch 119/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5644 - acc: 0.8437 - val_loss: 5.4487 - val_acc: 0.0860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5548 - acc: 0.8553 - val_loss: 5.5233 - val_acc: 0.0833\n",
      "Epoch 121/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5486 - acc: 0.8527 - val_loss: 5.5180 - val_acc: 0.0910\n",
      "Epoch 122/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5443 - acc: 0.8560 - val_loss: 5.6017 - val_acc: 0.0890\n",
      "Epoch 123/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5399 - acc: 0.8583 - val_loss: 5.5726 - val_acc: 0.0867\n",
      "Epoch 124/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5238 - acc: 0.8643 - val_loss: 5.6542 - val_acc: 0.0850\n",
      "Epoch 125/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5256 - acc: 0.8590 - val_loss: 5.6402 - val_acc: 0.0917\n",
      "Epoch 126/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5161 - acc: 0.8650 - val_loss: 5.6894 - val_acc: 0.0903\n",
      "Epoch 127/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5147 - acc: 0.8687 - val_loss: 5.7354 - val_acc: 0.0903\n",
      "Epoch 128/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5057 - acc: 0.8703 - val_loss: 5.6940 - val_acc: 0.0850\n",
      "Epoch 129/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4968 - acc: 0.8717 - val_loss: 5.7751 - val_acc: 0.0890\n",
      "Epoch 130/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4805 - acc: 0.8813 - val_loss: 5.8318 - val_acc: 0.0897\n",
      "Epoch 131/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4770 - acc: 0.8793 - val_loss: 5.8672 - val_acc: 0.0863\n",
      "Epoch 132/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4693 - acc: 0.8790 - val_loss: 5.8858 - val_acc: 0.0903\n",
      "Epoch 133/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4670 - acc: 0.8870 - val_loss: 5.9136 - val_acc: 0.0907\n",
      "Epoch 134/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4537 - acc: 0.8897 - val_loss: 6.0005 - val_acc: 0.0953\n",
      "Epoch 135/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.4543 - acc: 0.8877 - val_loss: 5.9906 - val_acc: 0.0887\n",
      "Epoch 136/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4466 - acc: 0.8910 - val_loss: 6.0518 - val_acc: 0.0883\n",
      "Epoch 137/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4407 - acc: 0.8933 - val_loss: 6.0476 - val_acc: 0.0860\n",
      "Epoch 138/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4319 - acc: 0.8970 - val_loss: 6.1443 - val_acc: 0.0880\n",
      "Epoch 139/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4348 - acc: 0.8920 - val_loss: 6.1609 - val_acc: 0.0883\n",
      "Epoch 140/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4229 - acc: 0.9003 - val_loss: 6.1872 - val_acc: 0.0917\n",
      "Epoch 141/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4198 - acc: 0.9010 - val_loss: 6.1892 - val_acc: 0.0880\n",
      "Epoch 142/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4137 - acc: 0.9033 - val_loss: 6.2445 - val_acc: 0.0863\n",
      "Epoch 143/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4056 - acc: 0.9030 - val_loss: 6.2785 - val_acc: 0.0870\n",
      "Epoch 144/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3977 - acc: 0.9083 - val_loss: 6.3289 - val_acc: 0.0897\n",
      "Epoch 145/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.4008 - acc: 0.9053 - val_loss: 6.3828 - val_acc: 0.0827\n",
      "Epoch 146/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3992 - acc: 0.9070 - val_loss: 6.3548 - val_acc: 0.0877\n",
      "Epoch 147/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3920 - acc: 0.9087 - val_loss: 6.4362 - val_acc: 0.0863\n",
      "Epoch 148/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3868 - acc: 0.9143 - val_loss: 6.4290 - val_acc: 0.0863\n",
      "Epoch 149/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3799 - acc: 0.9153 - val_loss: 6.5273 - val_acc: 0.0837\n",
      "Epoch 150/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3755 - acc: 0.9160 - val_loss: 6.5043 - val_acc: 0.0917\n",
      "Epoch 151/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3643 - acc: 0.9180 - val_loss: 6.5316 - val_acc: 0.0867\n",
      "Epoch 152/500\n",
      "3000/3000 [==============================] - 0s 23us/step - loss: 0.3641 - acc: 0.9187 - val_loss: 6.6208 - val_acc: 0.0833\n",
      "Epoch 153/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 0.3591 - acc: 0.9187 - val_loss: 6.5977 - val_acc: 0.0880\n",
      "Epoch 154/500\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3550 - acc: 0.9230 - val_loss: 6.6396 - val_acc: 0.0870\n",
      "Epoch 155/500\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.3480 - acc: 0.9227 - val_loss: 6.6899 - val_acc: 0.0883\n",
      "Epoch 156/500\n",
      "3000/3000 [==============================] - 0s 24us/step - loss: 0.3446 - acc: 0.9217 - val_loss: 6.7355 - val_acc: 0.0887\n",
      "Epoch 157/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.3346 - acc: 0.9287 - val_loss: 6.7762 - val_acc: 0.0887\n",
      "Epoch 158/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.3273 - acc: 0.9323 - val_loss: 6.7911 - val_acc: 0.0873\n",
      "Epoch 159/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.3251 - acc: 0.9287 - val_loss: 6.8221 - val_acc: 0.0943\n",
      "Epoch 160/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.3231 - acc: 0.9350 - val_loss: 6.8273 - val_acc: 0.0923\n",
      "Epoch 161/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.3203 - acc: 0.9353 - val_loss: 6.8575 - val_acc: 0.0913\n",
      "Epoch 162/500\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 0.3133 - acc: 0.9367 - val_loss: 6.8767 - val_acc: 0.0917\n",
      "Epoch 163/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.3103 - acc: 0.9357 - val_loss: 6.8906 - val_acc: 0.0940\n",
      "Epoch 164/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.2994 - acc: 0.9400 - val_loss: 7.0049 - val_acc: 0.0890\n",
      "Epoch 165/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2952 - acc: 0.9420 - val_loss: 7.0388 - val_acc: 0.0877\n",
      "Epoch 166/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.3016 - acc: 0.9353 - val_loss: 7.0285 - val_acc: 0.0900\n",
      "Epoch 167/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.2956 - acc: 0.9410 - val_loss: 7.0266 - val_acc: 0.0907\n",
      "Epoch 168/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.2885 - acc: 0.9420 - val_loss: 7.0551 - val_acc: 0.0917\n",
      "Epoch 169/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2813 - acc: 0.9453 - val_loss: 7.2039 - val_acc: 0.0883\n",
      "Epoch 170/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.2790 - acc: 0.9473 - val_loss: 7.1165 - val_acc: 0.0910\n",
      "Epoch 171/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2708 - acc: 0.9520 - val_loss: 7.2068 - val_acc: 0.0897\n",
      "Epoch 172/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2678 - acc: 0.9513 - val_loss: 7.2160 - val_acc: 0.0900\n",
      "Epoch 173/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2676 - acc: 0.9467 - val_loss: 7.2567 - val_acc: 0.0917\n",
      "Epoch 174/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2693 - acc: 0.9457 - val_loss: 7.2546 - val_acc: 0.0927\n",
      "Epoch 175/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2603 - acc: 0.9487 - val_loss: 7.3143 - val_acc: 0.0897\n",
      "Epoch 176/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2597 - acc: 0.9493 - val_loss: 7.3095 - val_acc: 0.0907\n",
      "Epoch 177/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2507 - acc: 0.9533 - val_loss: 7.3572 - val_acc: 0.0920\n",
      "Epoch 178/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2481 - acc: 0.9553 - val_loss: 7.3964 - val_acc: 0.0927\n",
      "Epoch 179/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2464 - acc: 0.9557 - val_loss: 7.4134 - val_acc: 0.0927\n",
      "Epoch 180/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2407 - acc: 0.9550 - val_loss: 7.4714 - val_acc: 0.0910\n",
      "Epoch 181/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2372 - acc: 0.9580 - val_loss: 7.5222 - val_acc: 0.0900\n",
      "Epoch 182/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2402 - acc: 0.9543 - val_loss: 7.5125 - val_acc: 0.0877\n",
      "Epoch 183/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2316 - acc: 0.9573 - val_loss: 7.5027 - val_acc: 0.0933\n",
      "Epoch 184/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2227 - acc: 0.9607 - val_loss: 7.5496 - val_acc: 0.0910\n",
      "Epoch 185/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2214 - acc: 0.9593 - val_loss: 7.5027 - val_acc: 0.0940\n",
      "Epoch 186/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2208 - acc: 0.9613 - val_loss: 7.5982 - val_acc: 0.0967\n",
      "Epoch 187/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2192 - acc: 0.9627 - val_loss: 7.6627 - val_acc: 0.0917\n",
      "Epoch 188/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.2182 - acc: 0.9630 - val_loss: 7.6495 - val_acc: 0.0960\n",
      "Epoch 189/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2124 - acc: 0.9650 - val_loss: 7.7572 - val_acc: 0.0907\n",
      "Epoch 190/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2131 - acc: 0.9630 - val_loss: 7.7650 - val_acc: 0.0937\n",
      "Epoch 191/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2031 - acc: 0.9683 - val_loss: 7.7656 - val_acc: 0.0937\n",
      "Epoch 192/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1997 - acc: 0.9670 - val_loss: 7.7834 - val_acc: 0.0883\n",
      "Epoch 193/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1966 - acc: 0.9687 - val_loss: 7.8251 - val_acc: 0.0907\n",
      "Epoch 194/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1960 - acc: 0.9673 - val_loss: 7.8585 - val_acc: 0.0930\n",
      "Epoch 195/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1926 - acc: 0.9697 - val_loss: 7.9409 - val_acc: 0.0887\n",
      "Epoch 196/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1921 - acc: 0.9680 - val_loss: 7.9206 - val_acc: 0.0903\n",
      "Epoch 197/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1901 - acc: 0.9710 - val_loss: 7.8914 - val_acc: 0.0927\n",
      "Epoch 198/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1876 - acc: 0.9697 - val_loss: 7.9456 - val_acc: 0.0927\n",
      "Epoch 199/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1821 - acc: 0.9730 - val_loss: 8.0419 - val_acc: 0.0883\n",
      "Epoch 200/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1799 - acc: 0.9703 - val_loss: 7.9792 - val_acc: 0.0933\n",
      "Epoch 201/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1764 - acc: 0.9717 - val_loss: 8.0231 - val_acc: 0.0970\n",
      "Epoch 202/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1789 - acc: 0.9710 - val_loss: 8.1145 - val_acc: 0.0873\n",
      "Epoch 203/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1778 - acc: 0.9700 - val_loss: 8.1201 - val_acc: 0.0943\n",
      "Epoch 204/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1679 - acc: 0.9743 - val_loss: 8.1819 - val_acc: 0.0890\n",
      "Epoch 205/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1658 - acc: 0.9783 - val_loss: 8.1183 - val_acc: 0.0930\n",
      "Epoch 206/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1633 - acc: 0.9740 - val_loss: 8.1305 - val_acc: 0.0990\n",
      "Epoch 207/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1586 - acc: 0.9770 - val_loss: 8.2190 - val_acc: 0.0930\n",
      "Epoch 208/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1587 - acc: 0.9773 - val_loss: 8.1867 - val_acc: 0.0953\n",
      "Epoch 209/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1575 - acc: 0.9770 - val_loss: 8.3056 - val_acc: 0.0910\n",
      "Epoch 210/500\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 0.1578 - acc: 0.9747 - val_loss: 8.3206 - val_acc: 0.0880\n",
      "Epoch 211/500\n",
      "3000/3000 [==============================] - 0s 24us/step - loss: 0.1569 - acc: 0.9767 - val_loss: 8.3163 - val_acc: 0.0890\n",
      "Epoch 212/500\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.1550 - acc: 0.9780 - val_loss: 8.3566 - val_acc: 0.0920\n",
      "Epoch 213/500\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.1512 - acc: 0.9777 - val_loss: 8.3530 - val_acc: 0.0877\n",
      "Epoch 214/500\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.1469 - acc: 0.9783 - val_loss: 8.4100 - val_acc: 0.0880\n",
      "Epoch 215/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1455 - acc: 0.9817 - val_loss: 8.4072 - val_acc: 0.0907\n",
      "Epoch 216/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1429 - acc: 0.9823 - val_loss: 8.4499 - val_acc: 0.0947\n",
      "Epoch 217/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1413 - acc: 0.9820 - val_loss: 8.5143 - val_acc: 0.0893\n",
      "Epoch 218/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1355 - acc: 0.9813 - val_loss: 8.4732 - val_acc: 0.0920\n",
      "Epoch 219/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1330 - acc: 0.9807 - val_loss: 8.5270 - val_acc: 0.0883\n",
      "Epoch 220/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1353 - acc: 0.9830 - val_loss: 8.4717 - val_acc: 0.0933\n",
      "Epoch 221/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1332 - acc: 0.9833 - val_loss: 8.5933 - val_acc: 0.0853\n",
      "Epoch 222/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1297 - acc: 0.9810 - val_loss: 8.5727 - val_acc: 0.0917\n",
      "Epoch 223/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1295 - acc: 0.9837 - val_loss: 8.6006 - val_acc: 0.0907\n",
      "Epoch 224/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1253 - acc: 0.9830 - val_loss: 8.5843 - val_acc: 0.0933\n",
      "Epoch 225/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1243 - acc: 0.9853 - val_loss: 8.6747 - val_acc: 0.0913\n",
      "Epoch 226/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1199 - acc: 0.9853 - val_loss: 8.6582 - val_acc: 0.0923\n",
      "Epoch 227/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1193 - acc: 0.9843 - val_loss: 8.7035 - val_acc: 0.0933\n",
      "Epoch 228/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1180 - acc: 0.9853 - val_loss: 8.7487 - val_acc: 0.0920\n",
      "Epoch 229/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1166 - acc: 0.9857 - val_loss: 8.7187 - val_acc: 0.0910\n",
      "Epoch 230/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1130 - acc: 0.9847 - val_loss: 8.7407 - val_acc: 0.0963\n",
      "Epoch 231/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1146 - acc: 0.9853 - val_loss: 8.8113 - val_acc: 0.0920\n",
      "Epoch 232/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1127 - acc: 0.9863 - val_loss: 8.8314 - val_acc: 0.0890\n",
      "Epoch 233/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1079 - acc: 0.9887 - val_loss: 8.7963 - val_acc: 0.0950\n",
      "Epoch 234/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1064 - acc: 0.9883 - val_loss: 8.8978 - val_acc: 0.0893\n",
      "Epoch 235/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1071 - acc: 0.9887 - val_loss: 8.8717 - val_acc: 0.0900\n",
      "Epoch 236/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1041 - acc: 0.9890 - val_loss: 8.9854 - val_acc: 0.0897\n",
      "Epoch 237/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1033 - acc: 0.9867 - val_loss: 8.9292 - val_acc: 0.0900\n",
      "Epoch 238/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 36us/step - loss: 0.1001 - acc: 0.9883 - val_loss: 9.0112 - val_acc: 0.0840\n",
      "Epoch 239/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.1071 - acc: 0.9847 - val_loss: 8.9831 - val_acc: 0.0910\n",
      "Epoch 240/500\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 0.1029 - acc: 0.9907 - val_loss: 9.0234 - val_acc: 0.0897\n",
      "Epoch 241/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0991 - acc: 0.9900 - val_loss: 9.0434 - val_acc: 0.0900\n",
      "Epoch 242/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0937 - acc: 0.9917 - val_loss: 9.0417 - val_acc: 0.0887\n",
      "Epoch 243/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0933 - acc: 0.9893 - val_loss: 9.0667 - val_acc: 0.0923\n",
      "Epoch 244/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0911 - acc: 0.9913 - val_loss: 9.1288 - val_acc: 0.0917\n",
      "Epoch 245/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0910 - acc: 0.9920 - val_loss: 9.0732 - val_acc: 0.0950\n",
      "Epoch 246/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0899 - acc: 0.9917 - val_loss: 9.1566 - val_acc: 0.0883\n",
      "Epoch 247/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0916 - acc: 0.9910 - val_loss: 9.0839 - val_acc: 0.0933\n",
      "Epoch 248/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0904 - acc: 0.9893 - val_loss: 9.1857 - val_acc: 0.0930\n",
      "Epoch 249/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0903 - acc: 0.9920 - val_loss: 9.2030 - val_acc: 0.0950\n",
      "Epoch 250/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0967 - acc: 0.9900 - val_loss: 9.2112 - val_acc: 0.0903\n",
      "Epoch 251/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0878 - acc: 0.9900 - val_loss: 9.2141 - val_acc: 0.0910\n",
      "Epoch 252/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0845 - acc: 0.9897 - val_loss: 9.2611 - val_acc: 0.0933\n",
      "Epoch 253/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0813 - acc: 0.9913 - val_loss: 9.3047 - val_acc: 0.0900\n",
      "Epoch 254/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0803 - acc: 0.9933 - val_loss: 9.2896 - val_acc: 0.0893\n",
      "Epoch 255/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0786 - acc: 0.9933 - val_loss: 9.2472 - val_acc: 0.0937\n",
      "Epoch 256/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0779 - acc: 0.9930 - val_loss: 9.3629 - val_acc: 0.0910\n",
      "Epoch 257/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0760 - acc: 0.9943 - val_loss: 9.3287 - val_acc: 0.0903\n",
      "Epoch 258/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0733 - acc: 0.9950 - val_loss: 9.3722 - val_acc: 0.0927\n",
      "Epoch 259/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0750 - acc: 0.9930 - val_loss: 9.3866 - val_acc: 0.0913\n",
      "Epoch 260/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0734 - acc: 0.9930 - val_loss: 9.4358 - val_acc: 0.0937\n",
      "Epoch 261/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0717 - acc: 0.9937 - val_loss: 9.3910 - val_acc: 0.0937\n",
      "Epoch 262/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0682 - acc: 0.9950 - val_loss: 9.4659 - val_acc: 0.0900\n",
      "Epoch 263/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0668 - acc: 0.9953 - val_loss: 9.4677 - val_acc: 0.0927\n",
      "Epoch 264/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0666 - acc: 0.9947 - val_loss: 9.5075 - val_acc: 0.0900\n",
      "Epoch 265/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0635 - acc: 0.9947 - val_loss: 9.4512 - val_acc: 0.0923\n",
      "Epoch 266/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0635 - acc: 0.9957 - val_loss: 9.5222 - val_acc: 0.0973\n",
      "Epoch 267/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0644 - acc: 0.9960 - val_loss: 9.5233 - val_acc: 0.0917\n",
      "Epoch 268/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 0.0629 - acc: 0.9953 - val_loss: 9.5845 - val_acc: 0.0910\n",
      "Epoch 269/500\n",
      "3000/3000 [==============================] - 0s 23us/step - loss: 0.0623 - acc: 0.9957 - val_loss: 9.6024 - val_acc: 0.0913\n",
      "Epoch 270/500\n",
      "3000/3000 [==============================] - 0s 25us/step - loss: 0.0614 - acc: 0.9953 - val_loss: 9.5965 - val_acc: 0.0893\n",
      "Epoch 271/500\n",
      "3000/3000 [==============================] - 0s 32us/step - loss: 0.0623 - acc: 0.9960 - val_loss: 9.6426 - val_acc: 0.0943\n",
      "Epoch 272/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 0.0614 - acc: 0.9940 - val_loss: 9.6341 - val_acc: 0.0910\n",
      "Epoch 273/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0573 - acc: 0.9957 - val_loss: 9.6249 - val_acc: 0.0967\n",
      "Epoch 274/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0559 - acc: 0.9953 - val_loss: 9.6625 - val_acc: 0.0943\n",
      "Epoch 275/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0541 - acc: 0.9957 - val_loss: 9.7404 - val_acc: 0.0933\n",
      "Epoch 276/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0532 - acc: 0.9967 - val_loss: 9.6963 - val_acc: 0.0920\n",
      "Epoch 277/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0534 - acc: 0.9960 - val_loss: 9.7089 - val_acc: 0.0940\n",
      "Epoch 278/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0545 - acc: 0.9967 - val_loss: 9.7427 - val_acc: 0.0943\n",
      "Epoch 279/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0551 - acc: 0.9963 - val_loss: 9.7481 - val_acc: 0.0943\n",
      "Epoch 280/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0543 - acc: 0.9973 - val_loss: 9.8300 - val_acc: 0.0897\n",
      "Epoch 281/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0514 - acc: 0.9973 - val_loss: 9.7214 - val_acc: 0.0947\n",
      "Epoch 282/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0512 - acc: 0.9973 - val_loss: 9.7983 - val_acc: 0.0907\n",
      "Epoch 283/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0485 - acc: 0.9973 - val_loss: 9.8861 - val_acc: 0.0910\n",
      "Epoch 284/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0483 - acc: 0.9973 - val_loss: 9.8780 - val_acc: 0.0917\n",
      "Epoch 285/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0458 - acc: 0.9977 - val_loss: 9.8588 - val_acc: 0.0913\n",
      "Epoch 286/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0466 - acc: 0.9973 - val_loss: 9.8859 - val_acc: 0.0943\n",
      "Epoch 287/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0461 - acc: 0.9973 - val_loss: 9.9112 - val_acc: 0.0920\n",
      "Epoch 288/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0463 - acc: 0.9970 - val_loss: 9.9066 - val_acc: 0.0917\n",
      "Epoch 289/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0446 - acc: 0.9983 - val_loss: 9.9811 - val_acc: 0.0920\n",
      "Epoch 290/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0439 - acc: 0.9980 - val_loss: 9.9097 - val_acc: 0.0933\n",
      "Epoch 291/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0448 - acc: 0.9983 - val_loss: 9.9575 - val_acc: 0.0930\n",
      "Epoch 292/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0537 - acc: 0.9963 - val_loss: 10.0039 - val_acc: 0.0913\n",
      "Epoch 293/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0528 - acc: 0.9980 - val_loss: 10.0122 - val_acc: 0.0907\n",
      "Epoch 294/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0517 - acc: 0.9980 - val_loss: 9.9854 - val_acc: 0.0957\n",
      "Epoch 295/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0487 - acc: 0.9980 - val_loss: 9.9724 - val_acc: 0.0993\n",
      "Epoch 296/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0458 - acc: 0.9977 - val_loss: 10.0612 - val_acc: 0.0907\n",
      "Epoch 297/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0444 - acc: 0.9983 - val_loss: 9.9956 - val_acc: 0.0933\n",
      "Epoch 298/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0441 - acc: 0.9977 - val_loss: 10.0035 - val_acc: 0.0967\n",
      "Epoch 299/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0450 - acc: 0.9983 - val_loss: 10.1008 - val_acc: 0.0947\n",
      "Epoch 300/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0391 - acc: 0.9990 - val_loss: 10.1175 - val_acc: 0.0917\n",
      "Epoch 301/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0394 - acc: 0.9987 - val_loss: 10.0993 - val_acc: 0.0917\n",
      "Epoch 302/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0372 - acc: 0.9990 - val_loss: 10.1470 - val_acc: 0.0920\n",
      "Epoch 303/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0357 - acc: 0.9987 - val_loss: 10.1088 - val_acc: 0.0930\n",
      "Epoch 304/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0336 - acc: 0.9990 - val_loss: 10.1810 - val_acc: 0.0947\n",
      "Epoch 305/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0345 - acc: 0.9987 - val_loss: 10.2123 - val_acc: 0.0920\n",
      "Epoch 306/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0332 - acc: 0.9990 - val_loss: 10.2265 - val_acc: 0.0883\n",
      "Epoch 307/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0346 - acc: 0.9983 - val_loss: 10.2053 - val_acc: 0.0893\n",
      "Epoch 308/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0324 - acc: 0.9987 - val_loss: 10.2261 - val_acc: 0.0903\n",
      "Epoch 309/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0312 - acc: 0.9993 - val_loss: 10.2701 - val_acc: 0.0900\n",
      "Epoch 310/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0303 - acc: 0.9990 - val_loss: 10.2244 - val_acc: 0.0940\n",
      "Epoch 311/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0296 - acc: 0.9990 - val_loss: 10.2595 - val_acc: 0.0927\n",
      "Epoch 312/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0294 - acc: 0.9990 - val_loss: 10.3189 - val_acc: 0.0880\n",
      "Epoch 313/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0297 - acc: 0.9993 - val_loss: 10.3087 - val_acc: 0.0903\n",
      "Epoch 314/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0300 - acc: 0.9993 - val_loss: 10.2846 - val_acc: 0.0920\n",
      "Epoch 315/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0283 - acc: 0.9993 - val_loss: 10.3020 - val_acc: 0.0910\n",
      "Epoch 316/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0277 - acc: 0.9993 - val_loss: 10.3261 - val_acc: 0.0910\n",
      "Epoch 317/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0270 - acc: 0.9997 - val_loss: 10.3390 - val_acc: 0.0897\n",
      "Epoch 318/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0266 - acc: 0.9997 - val_loss: 10.3395 - val_acc: 0.0917\n",
      "Epoch 319/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0274 - acc: 0.9993 - val_loss: 10.3703 - val_acc: 0.0913\n",
      "Epoch 320/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0277 - acc: 0.9997 - val_loss: 10.3785 - val_acc: 0.0923\n",
      "Epoch 321/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0262 - acc: 0.9997 - val_loss: 10.4184 - val_acc: 0.0893\n",
      "Epoch 322/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0248 - acc: 0.9997 - val_loss: 10.4128 - val_acc: 0.0923\n",
      "Epoch 323/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0244 - acc: 0.9997 - val_loss: 10.4803 - val_acc: 0.0890\n",
      "Epoch 324/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0237 - acc: 0.9993 - val_loss: 10.4312 - val_acc: 0.0923\n",
      "Epoch 325/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0245 - acc: 0.9993 - val_loss: 10.4822 - val_acc: 0.0897\n",
      "Epoch 326/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 0.0246 - acc: 0.9997 - val_loss: 10.4684 - val_acc: 0.0873\n",
      "Epoch 327/500\n",
      "3000/3000 [==============================] - 0s 24us/step - loss: 0.0234 - acc: 0.9997 - val_loss: 10.4733 - val_acc: 0.0893\n",
      "Epoch 328/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 0.0229 - acc: 0.9997 - val_loss: 10.4652 - val_acc: 0.0893\n",
      "Epoch 329/500\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.0225 - acc: 0.9997 - val_loss: 10.5369 - val_acc: 0.0857\n",
      "Epoch 330/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 0.0226 - acc: 0.9997 - val_loss: 10.4980 - val_acc: 0.0900\n",
      "Epoch 331/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0230 - acc: 0.9993 - val_loss: 10.5051 - val_acc: 0.0933\n",
      "Epoch 332/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0227 - acc: 0.9993 - val_loss: 10.5538 - val_acc: 0.0877\n",
      "Epoch 333/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0208 - acc: 0.9997 - val_loss: 10.5335 - val_acc: 0.0907\n",
      "Epoch 334/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0203 - acc: 0.9997 - val_loss: 10.5824 - val_acc: 0.0870\n",
      "Epoch 335/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0205 - acc: 0.9997 - val_loss: 10.5965 - val_acc: 0.0897\n",
      "Epoch 336/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0198 - acc: 0.9997 - val_loss: 10.6071 - val_acc: 0.0900\n",
      "Epoch 337/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0192 - acc: 0.9997 - val_loss: 10.6149 - val_acc: 0.0880\n",
      "Epoch 338/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0191 - acc: 0.9997 - val_loss: 10.6373 - val_acc: 0.0887\n",
      "Epoch 339/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0186 - acc: 0.9997 - val_loss: 10.6430 - val_acc: 0.0887\n",
      "Epoch 340/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0184 - acc: 0.9997 - val_loss: 10.6268 - val_acc: 0.0913\n",
      "Epoch 341/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0181 - acc: 0.9997 - val_loss: 10.6507 - val_acc: 0.0880\n",
      "Epoch 342/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0182 - acc: 0.9997 - val_loss: 10.6878 - val_acc: 0.0900\n",
      "Epoch 343/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0189 - acc: 0.9997 - val_loss: 10.6950 - val_acc: 0.0903\n",
      "Epoch 344/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0179 - acc: 0.9997 - val_loss: 10.6833 - val_acc: 0.0903\n",
      "Epoch 345/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0174 - acc: 0.9997 - val_loss: 10.6714 - val_acc: 0.0890\n",
      "Epoch 346/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0166 - acc: 0.9997 - val_loss: 10.7181 - val_acc: 0.0910\n",
      "Epoch 347/500\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 0.0168 - acc: 0.9997 - val_loss: 10.7255 - val_acc: 0.0890\n",
      "Epoch 348/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0168 - acc: 0.9997 - val_loss: 10.7147 - val_acc: 0.0890\n",
      "Epoch 349/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0167 - acc: 0.9997 - val_loss: 10.7183 - val_acc: 0.0917\n",
      "Epoch 350/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0163 - acc: 0.9997 - val_loss: 10.7684 - val_acc: 0.0903\n",
      "Epoch 351/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0161 - acc: 0.9997 - val_loss: 10.7300 - val_acc: 0.0923\n",
      "Epoch 352/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0161 - acc: 0.9997 - val_loss: 10.8032 - val_acc: 0.0907\n",
      "Epoch 353/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0168 - acc: 0.9997 - val_loss: 10.7897 - val_acc: 0.0887\n",
      "Epoch 354/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0152 - acc: 0.9997 - val_loss: 10.7973 - val_acc: 0.0917\n",
      "Epoch 355/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0151 - acc: 0.9997 - val_loss: 10.8290 - val_acc: 0.0900\n",
      "Epoch 356/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0152 - acc: 0.9997 - val_loss: 10.8117 - val_acc: 0.0903\n",
      "Epoch 357/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0144 - acc: 0.9997 - val_loss: 10.8404 - val_acc: 0.0893\n",
      "Epoch 358/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0146 - acc: 0.9997 - val_loss: 10.7963 - val_acc: 0.0910\n",
      "Epoch 359/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0141 - acc: 0.9997 - val_loss: 10.8542 - val_acc: 0.0900\n",
      "Epoch 360/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0137 - acc: 0.9997 - val_loss: 10.8554 - val_acc: 0.0897\n",
      "Epoch 361/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0133 - acc: 0.9997 - val_loss: 10.8933 - val_acc: 0.0897\n",
      "Epoch 362/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0129 - acc: 0.9997 - val_loss: 10.9175 - val_acc: 0.0873\n",
      "Epoch 363/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0132 - acc: 0.9997 - val_loss: 10.8791 - val_acc: 0.0900\n",
      "Epoch 364/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0128 - acc: 0.9997 - val_loss: 10.8958 - val_acc: 0.0887\n",
      "Epoch 365/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0124 - acc: 0.9997 - val_loss: 10.9198 - val_acc: 0.0873\n",
      "Epoch 366/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0123 - acc: 0.9997 - val_loss: 10.9349 - val_acc: 0.0897\n",
      "Epoch 367/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0122 - acc: 0.9997 - val_loss: 10.9390 - val_acc: 0.0903\n",
      "Epoch 368/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0119 - acc: 0.9997 - val_loss: 10.9427 - val_acc: 0.0910\n",
      "Epoch 369/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0120 - acc: 0.9997 - val_loss: 10.9370 - val_acc: 0.0897\n",
      "Epoch 370/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0117 - acc: 0.9997 - val_loss: 11.0016 - val_acc: 0.0897\n",
      "Epoch 371/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0116 - acc: 0.9997 - val_loss: 10.9819 - val_acc: 0.0890\n",
      "Epoch 372/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0114 - acc: 0.9997 - val_loss: 10.9678 - val_acc: 0.0910\n",
      "Epoch 373/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0112 - acc: 0.9997 - val_loss: 10.9925 - val_acc: 0.0900\n",
      "Epoch 374/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0118 - acc: 0.9997 - val_loss: 11.0127 - val_acc: 0.0913\n",
      "Epoch 375/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0111 - acc: 0.9997 - val_loss: 11.0137 - val_acc: 0.0923\n",
      "Epoch 376/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0110 - acc: 0.9997 - val_loss: 11.0125 - val_acc: 0.0910\n",
      "Epoch 377/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0106 - acc: 0.9997 - val_loss: 11.0302 - val_acc: 0.0903\n",
      "Epoch 378/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0104 - acc: 0.9997 - val_loss: 11.0404 - val_acc: 0.0893\n",
      "Epoch 379/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0101 - acc: 0.9997 - val_loss: 11.0272 - val_acc: 0.0900\n",
      "Epoch 380/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0102 - acc: 0.9997 - val_loss: 11.0515 - val_acc: 0.0897\n",
      "Epoch 381/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0099 - acc: 0.9997 - val_loss: 11.0916 - val_acc: 0.0903\n",
      "Epoch 382/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0098 - acc: 0.9997 - val_loss: 11.0743 - val_acc: 0.0917\n",
      "Epoch 383/500\n",
      "3000/3000 [==============================] - 0s 25us/step - loss: 0.0098 - acc: 0.9997 - val_loss: 11.0647 - val_acc: 0.0893\n",
      "Epoch 384/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 0.0095 - acc: 0.9997 - val_loss: 11.0906 - val_acc: 0.0903\n",
      "Epoch 385/500\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.0093 - acc: 0.9997 - val_loss: 11.1225 - val_acc: 0.0893\n",
      "Epoch 386/500\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.0090 - acc: 0.9997 - val_loss: 11.0962 - val_acc: 0.0910\n",
      "Epoch 387/500\n",
      "3000/3000 [==============================] - 0s 23us/step - loss: 0.0089 - acc: 0.9997 - val_loss: 11.1385 - val_acc: 0.0890\n",
      "Epoch 388/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0088 - acc: 0.9997 - val_loss: 11.1237 - val_acc: 0.0883\n",
      "Epoch 389/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0089 - acc: 0.9997 - val_loss: 11.1276 - val_acc: 0.0927\n",
      "Epoch 390/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0085 - acc: 0.9997 - val_loss: 11.1569 - val_acc: 0.0893\n",
      "Epoch 391/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0083 - acc: 0.9997 - val_loss: 11.1855 - val_acc: 0.0883\n",
      "Epoch 392/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0086 - acc: 0.9997 - val_loss: 11.1449 - val_acc: 0.0917\n",
      "Epoch 393/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0626 - acc: 0.9860 - val_loss: 10.8910 - val_acc: 0.1077\n",
      "Epoch 394/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.8548 - acc: 0.7963 - val_loss: 11.1547 - val_acc: 0.0837\n",
      "Epoch 395/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.5289 - acc: 0.8500 - val_loss: 11.0531 - val_acc: 0.0880\n",
      "Epoch 396/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.2518 - acc: 0.9223 - val_loss: 10.9189 - val_acc: 0.1000\n",
      "Epoch 397/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.1167 - acc: 0.9670 - val_loss: 11.0634 - val_acc: 0.0943\n",
      "Epoch 398/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0642 - acc: 0.9863 - val_loss: 10.9335 - val_acc: 0.0927\n",
      "Epoch 399/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0353 - acc: 0.9950 - val_loss: 11.0978 - val_acc: 0.0910\n",
      "Epoch 400/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0339 - acc: 0.9953 - val_loss: 11.1381 - val_acc: 0.0910\n",
      "Epoch 401/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0214 - acc: 0.9987 - val_loss: 10.9763 - val_acc: 0.0957\n",
      "Epoch 402/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0153 - acc: 0.9990 - val_loss: 11.0779 - val_acc: 0.0900\n",
      "Epoch 403/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0131 - acc: 0.9997 - val_loss: 11.0643 - val_acc: 0.0917\n",
      "Epoch 404/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0116 - acc: 0.9997 - val_loss: 11.0389 - val_acc: 0.0903\n",
      "Epoch 405/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0113 - acc: 0.9997 - val_loss: 11.0651 - val_acc: 0.0910\n",
      "Epoch 406/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0108 - acc: 0.9997 - val_loss: 11.0537 - val_acc: 0.0910\n",
      "Epoch 407/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0106 - acc: 0.9997 - val_loss: 11.0709 - val_acc: 0.0903\n",
      "Epoch 408/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0104 - acc: 0.9997 - val_loss: 11.0781 - val_acc: 0.0900\n",
      "Epoch 409/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0102 - acc: 0.9997 - val_loss: 11.0908 - val_acc: 0.0903\n",
      "Epoch 410/500\n",
      "3000/3000 [==============================] - 0s 37us/step - loss: 0.0100 - acc: 0.9997 - val_loss: 11.0826 - val_acc: 0.0903\n",
      "Epoch 411/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0099 - acc: 0.9997 - val_loss: 11.0770 - val_acc: 0.0907\n",
      "Epoch 412/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0097 - acc: 0.9997 - val_loss: 11.0804 - val_acc: 0.0913\n",
      "Epoch 413/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0095 - acc: 0.9997 - val_loss: 11.0985 - val_acc: 0.0910\n",
      "Epoch 414/500\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 0.0094 - acc: 0.9997 - val_loss: 11.1041 - val_acc: 0.0917\n",
      "Epoch 415/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0093 - acc: 0.9997 - val_loss: 11.0972 - val_acc: 0.0913\n",
      "Epoch 416/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0091 - acc: 0.9997 - val_loss: 11.1134 - val_acc: 0.0910\n",
      "Epoch 417/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0090 - acc: 0.9997 - val_loss: 11.1078 - val_acc: 0.0913\n",
      "Epoch 418/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0090 - acc: 0.9997 - val_loss: 11.1141 - val_acc: 0.0910\n",
      "Epoch 419/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0088 - acc: 0.9997 - val_loss: 11.1197 - val_acc: 0.0920\n",
      "Epoch 420/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0087 - acc: 0.9997 - val_loss: 11.1269 - val_acc: 0.0907\n",
      "Epoch 421/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0087 - acc: 0.9997 - val_loss: 11.1218 - val_acc: 0.0917\n",
      "Epoch 422/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0086 - acc: 0.9997 - val_loss: 11.1255 - val_acc: 0.0910\n",
      "Epoch 423/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0085 - acc: 0.9997 - val_loss: 11.1382 - val_acc: 0.0910\n",
      "Epoch 424/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0084 - acc: 0.9997 - val_loss: 11.1341 - val_acc: 0.0917\n",
      "Epoch 425/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0083 - acc: 0.9997 - val_loss: 11.1394 - val_acc: 0.0927\n",
      "Epoch 426/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0082 - acc: 0.9997 - val_loss: 11.1490 - val_acc: 0.0917\n",
      "Epoch 427/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0081 - acc: 0.9997 - val_loss: 11.1532 - val_acc: 0.0903\n",
      "Epoch 428/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0081 - acc: 0.9997 - val_loss: 11.1496 - val_acc: 0.0923\n",
      "Epoch 429/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0080 - acc: 0.9997 - val_loss: 11.1533 - val_acc: 0.0917\n",
      "Epoch 430/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0079 - acc: 0.9997 - val_loss: 11.1629 - val_acc: 0.0923\n",
      "Epoch 431/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0079 - acc: 0.9997 - val_loss: 11.1458 - val_acc: 0.0923\n",
      "Epoch 432/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0078 - acc: 0.9997 - val_loss: 11.1734 - val_acc: 0.0927\n",
      "Epoch 433/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0077 - acc: 0.9997 - val_loss: 11.1750 - val_acc: 0.0917\n",
      "Epoch 434/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0077 - acc: 0.9997 - val_loss: 11.1658 - val_acc: 0.0930\n",
      "Epoch 435/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0077 - acc: 0.9997 - val_loss: 11.1801 - val_acc: 0.0920\n",
      "Epoch 436/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0075 - acc: 0.9997 - val_loss: 11.1739 - val_acc: 0.0923\n",
      "Epoch 437/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0075 - acc: 0.9997 - val_loss: 11.1810 - val_acc: 0.0930\n",
      "Epoch 438/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0075 - acc: 0.9997 - val_loss: 11.1819 - val_acc: 0.0900\n",
      "Epoch 439/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0074 - acc: 0.9997 - val_loss: 11.1903 - val_acc: 0.0923\n",
      "Epoch 440/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 11.1841 - val_acc: 0.0927\n",
      "Epoch 441/500\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 0.0072 - acc: 0.9997 - val_loss: 11.1880 - val_acc: 0.0920\n",
      "Epoch 442/500\n",
      "3000/3000 [==============================] - 0s 25us/step - loss: 0.0072 - acc: 0.9997 - val_loss: 11.2025 - val_acc: 0.0923\n",
      "Epoch 443/500\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.0071 - acc: 0.9997 - val_loss: 11.1909 - val_acc: 0.0927\n",
      "Epoch 444/500\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 11.2113 - val_acc: 0.0903\n",
      "Epoch 445/500\n",
      "3000/3000 [==============================] - 0s 22us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 11.2063 - val_acc: 0.0930\n",
      "Epoch 446/500\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 0.0070 - acc: 0.9997 - val_loss: 11.2197 - val_acc: 0.0917\n",
      "Epoch 447/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 11.2176 - val_acc: 0.0927\n",
      "Epoch 448/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 11.2109 - val_acc: 0.0923\n",
      "Epoch 449/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 11.2214 - val_acc: 0.0930\n",
      "Epoch 450/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 11.2210 - val_acc: 0.0930\n",
      "Epoch 451/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 11.2281 - val_acc: 0.0920\n",
      "Epoch 452/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 11.2261 - val_acc: 0.0930\n",
      "Epoch 453/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 11.2468 - val_acc: 0.0923\n",
      "Epoch 454/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 11.2328 - val_acc: 0.0917\n",
      "Epoch 455/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 11.2361 - val_acc: 0.0923\n",
      "Epoch 456/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 11.2573 - val_acc: 0.0927\n",
      "Epoch 457/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 11.2420 - val_acc: 0.0920\n",
      "Epoch 458/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 11.2510 - val_acc: 0.0913\n",
      "Epoch 459/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 11.2552 - val_acc: 0.0913\n",
      "Epoch 460/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 11.2553 - val_acc: 0.0930\n",
      "Epoch 461/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 11.2597 - val_acc: 0.0933\n",
      "Epoch 462/500\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 11.2775 - val_acc: 0.0910\n",
      "Epoch 463/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 11.2635 - val_acc: 0.0920\n",
      "Epoch 464/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 11.2860 - val_acc: 0.0913\n",
      "Epoch 465/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 11.2793 - val_acc: 0.0913\n",
      "Epoch 466/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 11.2807 - val_acc: 0.0917\n",
      "Epoch 467/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 11.2900 - val_acc: 0.0910\n",
      "Epoch 468/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 11.2891 - val_acc: 0.0903\n",
      "Epoch 469/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 11.2851 - val_acc: 0.0903\n",
      "Epoch 470/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 11.2850 - val_acc: 0.0927\n",
      "Epoch 471/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 11.2945 - val_acc: 0.0923\n",
      "Epoch 472/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 11.2986 - val_acc: 0.0897\n",
      "Epoch 473/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 11.2978 - val_acc: 0.0903\n",
      "Epoch 474/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 11.3128 - val_acc: 0.0917\n",
      "Epoch 475/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 11.3097 - val_acc: 0.0903\n",
      "Epoch 476/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 11.3102 - val_acc: 0.0923\n",
      "Epoch 477/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 11.3117 - val_acc: 0.0933\n",
      "Epoch 478/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 11.3192 - val_acc: 0.0910\n",
      "Epoch 479/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 11.3188 - val_acc: 0.0923\n",
      "Epoch 480/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 11.3171 - val_acc: 0.0930\n",
      "Epoch 481/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 11.3251 - val_acc: 0.0930\n",
      "Epoch 482/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 11.3360 - val_acc: 0.0923\n",
      "Epoch 483/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 11.3415 - val_acc: 0.0917\n",
      "Epoch 484/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 11.3510 - val_acc: 0.0920\n",
      "Epoch 485/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 11.3443 - val_acc: 0.0923\n",
      "Epoch 486/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 11.3534 - val_acc: 0.0930\n",
      "Epoch 487/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 11.3601 - val_acc: 0.0920\n",
      "Epoch 488/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 11.3677 - val_acc: 0.0903\n",
      "Epoch 489/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 11.3664 - val_acc: 0.0920\n",
      "Epoch 490/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 11.3685 - val_acc: 0.0917\n",
      "Epoch 491/500\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 11.3645 - val_acc: 0.0923\n",
      "Epoch 492/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 11.3753 - val_acc: 0.0910\n",
      "Epoch 493/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 11.3893 - val_acc: 0.0903\n",
      "Epoch 494/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 11.3844 - val_acc: 0.0933\n",
      "Epoch 495/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 11.3845 - val_acc: 0.0923\n",
      "Epoch 496/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 11.3808 - val_acc: 0.0913\n",
      "Epoch 497/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 11.3704 - val_acc: 0.0940\n",
      "Epoch 498/500\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 11.3897 - val_acc: 0.0923\n",
      "Epoch 499/500\n",
      "3000/3000 [==============================] - 0s 23us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 11.4002 - val_acc: 0.0927\n",
      "Epoch 500/500\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 11.4070 - val_acc: 0.0923\n"
     ]
    }
   ],
   "source": [
    "model0.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history0 = model0.fit(x_train[:3000], y_train[:3000], batch_size=128, epochs=500, validation_data=(x_test[:3000], y_test[:3000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:01:49.662086Z",
     "start_time": "2018-07-06T03:01:49.502828Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history0.history[\"loss\"], label=\"Train\")\n",
    "plt.plot(history0.history[\"val_loss\"], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Loss)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:01:49.793385Z",
     "start_time": "2018-07-06T03:01:49.663491Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history0.history[\"acc\"], label=\"Train\")\n",
    "plt.plot(history0.history[\"val_acc\"], label=\"Validation\")\n",
    "plt.legend()\n",
    "plt.title(\"Learning Curve (Accuracy)\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Number of parameters v.s. Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:14:29.639649Z",
     "start_time": "2018-07-06T03:14:29.593846Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 53,018\n",
      "Trainable params: 53,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model1.add(Dense(64, activation=\"relu\", name=\"fc1\"))\n",
    "model1.add(Dense(32, activation=\"relu\", name=\"fc2\"))\n",
    "model1.add(Dense(16, activation=\"relu\", name=\"fc3\"))\n",
    "model1.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:16:27.999834Z",
     "start_time": "2018-07-06T03:16:27.954685Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 56)                43960     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 28)                1596      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 14)                406       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                150       \n",
      "=================================================================\n",
      "Total params: 46,112\n",
      "Trainable params: 46,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model2.add(Dense(56, activation=\"relu\", name=\"fc1\"))\n",
    "model2.add(Dense(28, activation=\"relu\", name=\"fc2\"))\n",
    "model2.add(Dense(14, activation=\"relu\", name=\"fc3\"))\n",
    "model2.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:16:58.213628Z",
     "start_time": "2018-07-06T03:16:58.169767Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 48)                37680     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 24)                1176      \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 12)                300       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                130       \n",
      "=================================================================\n",
      "Total params: 39,286\n",
      "Trainable params: 39,286\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model3.add(Dense(48, activation=\"relu\", name=\"fc1\"))\n",
    "model3.add(Dense(24, activation=\"relu\", name=\"fc2\"))\n",
    "model3.add(Dense(12, activation=\"relu\", name=\"fc3\"))\n",
    "model3.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:18:08.935068Z",
     "start_time": "2018-07-06T03:18:08.807358Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 40)                31400     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 20)                820       \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 32,540\n",
      "Trainable params: 32,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model4.add(Dense(40, activation=\"relu\", name=\"fc1\"))\n",
    "model4.add(Dense(20, activation=\"relu\", name=\"fc2\"))\n",
    "model4.add(Dense(10, activation=\"relu\", name=\"fc3\"))\n",
    "model4.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:34:58.431102Z",
     "start_time": "2018-07-06T03:34:58.387069Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 30)                23550     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 20)                620       \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                110       \n",
      "=================================================================\n",
      "Total params: 24,490\n",
      "Trainable params: 24,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model5.add(Dense(30, activation=\"relu\", name=\"fc1\"))\n",
    "model5.add(Dense(20, activation=\"relu\", name=\"fc2\"))\n",
    "model5.add(Dense(10, activation=\"relu\", name=\"fc3\"))\n",
    "model5.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Model6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Model7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:23:47.330774Z",
     "start_time": "2018-07-06T03:23:38.290882Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 130us/step - loss: 2.3094 - acc: 0.0965 - val_loss: 2.3048 - val_acc: 0.1060\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2879 - acc: 0.1370 - val_loss: 2.3120 - val_acc: 0.0950\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2730 - acc: 0.1495 - val_loss: 2.3119 - val_acc: 0.0990\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2511 - acc: 0.1675 - val_loss: 2.3190 - val_acc: 0.0985\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2280 - acc: 0.1940 - val_loss: 2.3193 - val_acc: 0.1085\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2057 - acc: 0.2060 - val_loss: 2.3383 - val_acc: 0.0995\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1746 - acc: 0.2315 - val_loss: 2.3389 - val_acc: 0.0945\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1467 - acc: 0.2455 - val_loss: 2.3561 - val_acc: 0.0945\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1042 - acc: 0.2740 - val_loss: 2.3774 - val_acc: 0.0970\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.0717 - acc: 0.2820 - val_loss: 2.3920 - val_acc: 0.0940\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.0297 - acc: 0.2920 - val_loss: 2.4073 - val_acc: 0.0950\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.9813 - acc: 0.3135 - val_loss: 2.4297 - val_acc: 0.0900\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.9305 - acc: 0.3420 - val_loss: 2.4557 - val_acc: 0.0920\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.8849 - acc: 0.3640 - val_loss: 2.4789 - val_acc: 0.0960\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.8297 - acc: 0.3780 - val_loss: 2.5082 - val_acc: 0.0970\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.7739 - acc: 0.4110 - val_loss: 2.5499 - val_acc: 0.1050\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.7266 - acc: 0.4195 - val_loss: 2.5942 - val_acc: 0.1045\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.6617 - acc: 0.4495 - val_loss: 2.6386 - val_acc: 0.0985\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.6183 - acc: 0.4675 - val_loss: 2.6903 - val_acc: 0.0980\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.5527 - acc: 0.4730 - val_loss: 2.7405 - val_acc: 0.0985\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.5097 - acc: 0.5055 - val_loss: 2.7704 - val_acc: 0.1015\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.4396 - acc: 0.5330 - val_loss: 2.8234 - val_acc: 0.0900\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.3902 - acc: 0.5505 - val_loss: 2.8757 - val_acc: 0.0945\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.3280 - acc: 0.5810 - val_loss: 2.9257 - val_acc: 0.0940\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 1.2806 - acc: 0.5980 - val_loss: 2.9873 - val_acc: 0.0930\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 1.2196 - acc: 0.6175 - val_loss: 3.0536 - val_acc: 0.1040\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 1.1900 - acc: 0.6260 - val_loss: 3.1401 - val_acc: 0.1005\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 1.1371 - acc: 0.6455 - val_loss: 3.1966 - val_acc: 0.0875\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 1.0839 - acc: 0.6765 - val_loss: 3.3015 - val_acc: 0.0985\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 1.0526 - acc: 0.6875 - val_loss: 3.3318 - val_acc: 0.0920\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 1.0118 - acc: 0.6955 - val_loss: 3.3949 - val_acc: 0.0995\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.9939 - acc: 0.6940 - val_loss: 3.4860 - val_acc: 0.0940\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.9352 - acc: 0.7210 - val_loss: 3.5660 - val_acc: 0.0910\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.8869 - acc: 0.7365 - val_loss: 3.6464 - val_acc: 0.0930\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.8547 - acc: 0.7500 - val_loss: 3.6865 - val_acc: 0.0980\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.8068 - acc: 0.7785 - val_loss: 3.7815 - val_acc: 0.0980\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.7852 - acc: 0.7670 - val_loss: 3.8700 - val_acc: 0.0975\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7627 - acc: 0.7790 - val_loss: 3.9570 - val_acc: 0.0950\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.7310 - acc: 0.7930 - val_loss: 3.9923 - val_acc: 0.0990\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.6975 - acc: 0.8125 - val_loss: 4.1080 - val_acc: 0.0960\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6469 - acc: 0.8320 - val_loss: 4.1718 - val_acc: 0.0965\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6189 - acc: 0.8420 - val_loss: 4.2447 - val_acc: 0.0985\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5985 - acc: 0.8425 - val_loss: 4.3803 - val_acc: 0.0940\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5817 - acc: 0.8465 - val_loss: 4.4508 - val_acc: 0.0985\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5647 - acc: 0.8510 - val_loss: 4.5722 - val_acc: 0.0985\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5328 - acc: 0.8705 - val_loss: 4.5965 - val_acc: 0.1045\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5091 - acc: 0.8715 - val_loss: 4.6601 - val_acc: 0.0975\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4769 - acc: 0.8815 - val_loss: 4.7858 - val_acc: 0.0960\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4521 - acc: 0.8905 - val_loss: 4.8327 - val_acc: 0.1020\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4309 - acc: 0.9010 - val_loss: 4.9246 - val_acc: 0.0965\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4136 - acc: 0.9020 - val_loss: 4.9973 - val_acc: 0.0940\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3889 - acc: 0.9160 - val_loss: 5.0987 - val_acc: 0.1000\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3679 - acc: 0.9220 - val_loss: 5.2055 - val_acc: 0.0995\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3582 - acc: 0.9270 - val_loss: 5.2976 - val_acc: 0.1020\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3375 - acc: 0.9335 - val_loss: 5.3759 - val_acc: 0.1015\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3257 - acc: 0.9400 - val_loss: 5.4349 - val_acc: 0.1000\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3227 - acc: 0.9280 - val_loss: 5.5247 - val_acc: 0.1035\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3062 - acc: 0.9425 - val_loss: 5.5809 - val_acc: 0.1055\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2904 - acc: 0.9505 - val_loss: 5.6819 - val_acc: 0.1050\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2761 - acc: 0.9530 - val_loss: 5.7383 - val_acc: 0.0935\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2638 - acc: 0.9545 - val_loss: 5.8394 - val_acc: 0.1060\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2499 - acc: 0.9575 - val_loss: 5.9323 - val_acc: 0.0975\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2362 - acc: 0.9635 - val_loss: 5.9954 - val_acc: 0.1035\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2259 - acc: 0.9640 - val_loss: 6.0789 - val_acc: 0.1015\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2153 - acc: 0.9665 - val_loss: 6.1298 - val_acc: 0.1025\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2065 - acc: 0.9695 - val_loss: 6.2160 - val_acc: 0.1090\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1975 - acc: 0.9660 - val_loss: 6.3173 - val_acc: 0.1000\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1924 - acc: 0.9715 - val_loss: 6.3493 - val_acc: 0.1000\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1763 - acc: 0.9770 - val_loss: 6.4879 - val_acc: 0.1050\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1699 - acc: 0.9765 - val_loss: 6.5246 - val_acc: 0.0995\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1636 - acc: 0.9785 - val_loss: 6.5612 - val_acc: 0.1020\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1571 - acc: 0.9775 - val_loss: 6.6450 - val_acc: 0.0995\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1511 - acc: 0.9825 - val_loss: 6.7077 - val_acc: 0.1005\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1471 - acc: 0.9805 - val_loss: 6.7989 - val_acc: 0.1035\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1403 - acc: 0.9810 - val_loss: 6.8232 - val_acc: 0.1010\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1362 - acc: 0.9815 - val_loss: 6.8634 - val_acc: 0.0995\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1290 - acc: 0.9835 - val_loss: 6.9515 - val_acc: 0.1015\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1227 - acc: 0.9855 - val_loss: 7.0115 - val_acc: 0.1035\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1143 - acc: 0.9865 - val_loss: 7.0881 - val_acc: 0.1015\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1078 - acc: 0.9860 - val_loss: 7.1335 - val_acc: 0.1015\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1046 - acc: 0.9890 - val_loss: 7.1777 - val_acc: 0.1040\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1000 - acc: 0.9885 - val_loss: 7.2327 - val_acc: 0.1025\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0971 - acc: 0.9900 - val_loss: 7.2831 - val_acc: 0.1060\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0946 - acc: 0.9905 - val_loss: 7.3516 - val_acc: 0.0995\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0882 - acc: 0.9900 - val_loss: 7.4157 - val_acc: 0.1025\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0865 - acc: 0.9910 - val_loss: 7.4430 - val_acc: 0.1075\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0806 - acc: 0.9935 - val_loss: 7.5151 - val_acc: 0.1025\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0781 - acc: 0.9945 - val_loss: 7.5738 - val_acc: 0.1025\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0770 - acc: 0.9940 - val_loss: 7.6414 - val_acc: 0.1025\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0801 - acc: 0.9915 - val_loss: 7.6583 - val_acc: 0.1040\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0768 - acc: 0.9925 - val_loss: 7.6725 - val_acc: 0.1070\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0734 - acc: 0.9945 - val_loss: 7.7504 - val_acc: 0.1030\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0750 - acc: 0.9900 - val_loss: 7.7586 - val_acc: 0.1010\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0704 - acc: 0.9940 - val_loss: 7.8747 - val_acc: 0.1070\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0648 - acc: 0.9945 - val_loss: 7.8861 - val_acc: 0.1000\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0599 - acc: 0.9955 - val_loss: 7.9331 - val_acc: 0.1050\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0542 - acc: 0.9970 - val_loss: 7.9696 - val_acc: 0.1035\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0542 - acc: 0.9980 - val_loss: 8.0436 - val_acc: 0.1060\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0512 - acc: 0.9965 - val_loss: 8.0461 - val_acc: 0.1060\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0488 - acc: 0.9960 - val_loss: 8.0981 - val_acc: 0.1040\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0474 - acc: 0.9970 - val_loss: 8.1493 - val_acc: 0.1035\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0458 - acc: 0.9985 - val_loss: 8.1879 - val_acc: 0.1030\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0456 - acc: 0.9970 - val_loss: 8.2149 - val_acc: 0.1060\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0417 - acc: 0.9985 - val_loss: 8.2514 - val_acc: 0.1005\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.0421 - acc: 0.9975 - val_loss: 8.2617 - val_acc: 0.1020\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.0392 - acc: 0.9985 - val_loss: 8.3215 - val_acc: 0.1045\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.0374 - acc: 0.9985 - val_loss: 8.3598 - val_acc: 0.1020\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.0360 - acc: 0.9995 - val_loss: 8.4181 - val_acc: 0.1025\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.0349 - acc: 0.9995 - val_loss: 8.4242 - val_acc: 0.1030\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.0348 - acc: 0.9990 - val_loss: 8.4852 - val_acc: 0.1025\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.0337 - acc: 0.9990 - val_loss: 8.4708 - val_acc: 0.1000\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0319 - acc: 0.9995 - val_loss: 8.5350 - val_acc: 0.1020\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0304 - acc: 1.0000 - val_loss: 8.5515 - val_acc: 0.1045\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0299 - acc: 0.9995 - val_loss: 8.6037 - val_acc: 0.1025\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 8.6156 - val_acc: 0.1015\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0273 - acc: 1.0000 - val_loss: 8.6603 - val_acc: 0.1020\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0259 - acc: 1.0000 - val_loss: 8.7027 - val_acc: 0.1025\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0262 - acc: 1.0000 - val_loss: 8.7214 - val_acc: 0.1060\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0250 - acc: 0.9990 - val_loss: 8.7511 - val_acc: 0.1025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0243 - acc: 1.0000 - val_loss: 8.7641 - val_acc: 0.1035\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0240 - acc: 0.9995 - val_loss: 8.8068 - val_acc: 0.1020\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0223 - acc: 1.0000 - val_loss: 8.8542 - val_acc: 0.1065\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 8.8488 - val_acc: 0.1005\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 8.8779 - val_acc: 0.1040\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 8.9066 - val_acc: 0.1020\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 8.9439 - val_acc: 0.1005\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 8.9689 - val_acc: 0.1040\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 8.9950 - val_acc: 0.1005\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 9.0181 - val_acc: 0.1035\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 9.0411 - val_acc: 0.1080\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 9.0855 - val_acc: 0.1030\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 9.0785 - val_acc: 0.1050\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 9.1189 - val_acc: 0.1015\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0154 - acc: 1.0000 - val_loss: 9.1584 - val_acc: 0.1000\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 9.1468 - val_acc: 0.1030\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 9.1821 - val_acc: 0.1055\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 9.2008 - val_acc: 0.1020\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0136 - acc: 1.0000 - val_loss: 9.2388 - val_acc: 0.1055\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 9.2457 - val_acc: 0.1040\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 9.2681 - val_acc: 0.1050\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 9.2890 - val_acc: 0.1065\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.0123 - acc: 1.0000 - val_loss: 9.3140 - val_acc: 0.1075\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 9.3265 - val_acc: 0.1025\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0118 - acc: 1.0000 - val_loss: 9.3172 - val_acc: 0.1035\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 9.3805 - val_acc: 0.1020\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 9.3830 - val_acc: 0.1000\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 9.3990 - val_acc: 0.1050\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 9.4175 - val_acc: 0.1030\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 9.4378 - val_acc: 0.1030\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 9.4524 - val_acc: 0.1020\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 9.4653 - val_acc: 0.1055\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 9.4938 - val_acc: 0.1010\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 9.5101 - val_acc: 0.1040\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 9.5406 - val_acc: 0.1035\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 9.5436 - val_acc: 0.1035\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0086 - acc: 1.0000 - val_loss: 9.5564 - val_acc: 0.1040\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 9.5723 - val_acc: 0.1025\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 9.5991 - val_acc: 0.1010\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 9.6065 - val_acc: 0.1030\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 9.6090 - val_acc: 0.1035\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 9.6366 - val_acc: 0.1030\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 9.6577 - val_acc: 0.0990\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 9.6732 - val_acc: 0.0990\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 9.6850 - val_acc: 0.1055\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 9.7101 - val_acc: 0.1030\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 9.7166 - val_acc: 0.1010\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 9.7218 - val_acc: 0.1060\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 9.7275 - val_acc: 0.1040\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 9.7581 - val_acc: 0.1025\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 9.7725 - val_acc: 0.1040\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 9.7887 - val_acc: 0.1030\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 9.7911 - val_acc: 0.1045\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 9.8089 - val_acc: 0.1030\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 9.8281 - val_acc: 0.1040\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 9.8436 - val_acc: 0.1035\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 9.8557 - val_acc: 0.1040\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 9.8696 - val_acc: 0.1045\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 9.8790 - val_acc: 0.1015\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 9.9018 - val_acc: 0.1005\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 9.9068 - val_acc: 0.1025\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 9.9173 - val_acc: 0.1035\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 9.9383 - val_acc: 0.1020\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 9.9417 - val_acc: 0.1040\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 9.9518 - val_acc: 0.1010\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 9.9636 - val_acc: 0.1020\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 9.9766 - val_acc: 0.1030\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 38us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 9.9910 - val_acc: 0.1015\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 9.9920 - val_acc: 0.1050\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 10.0165 - val_acc: 0.1010\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 10.0272 - val_acc: 0.1035\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 10.0459 - val_acc: 0.1015\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 10.0463 - val_acc: 0.0995\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 10.0547 - val_acc: 0.1020\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 10.0709 - val_acc: 0.1045\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 10.0801 - val_acc: 0.0990\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 10.0803 - val_acc: 0.1025\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 10.1014 - val_acc: 0.0990\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 10.1117 - val_acc: 0.1020\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 10.1309 - val_acc: 0.0980\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 10.1343 - val_acc: 0.1030\n"
     ]
    }
   ],
   "source": [
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history1 = model1.fit(x_train[:4000], y_train[:4000], batch_size=128, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:24:26.248618Z",
     "start_time": "2018-07-06T03:24:16.839101Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 99us/step - loss: 2.3120 - acc: 0.0950 - val_loss: 2.3031 - val_acc: 0.1055\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2956 - acc: 0.1250 - val_loss: 2.3040 - val_acc: 0.1060\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.2856 - acc: 0.1485 - val_loss: 2.3086 - val_acc: 0.1060\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.2712 - acc: 0.1630 - val_loss: 2.3102 - val_acc: 0.1045\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2532 - acc: 0.1730 - val_loss: 2.3150 - val_acc: 0.1075\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2310 - acc: 0.2005 - val_loss: 2.3335 - val_acc: 0.1040\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2160 - acc: 0.1860 - val_loss: 2.3302 - val_acc: 0.1110\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.1817 - acc: 0.2220 - val_loss: 2.3445 - val_acc: 0.1140\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.1517 - acc: 0.2495 - val_loss: 2.3654 - val_acc: 0.1040\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.1233 - acc: 0.2405 - val_loss: 2.3702 - val_acc: 0.1100\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.0896 - acc: 0.2560 - val_loss: 2.4013 - val_acc: 0.1080\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.0502 - acc: 0.2790 - val_loss: 2.4023 - val_acc: 0.0980\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.0112 - acc: 0.3100 - val_loss: 2.4512 - val_acc: 0.1100\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.9659 - acc: 0.3175 - val_loss: 2.4618 - val_acc: 0.1030\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.9302 - acc: 0.3395 - val_loss: 2.4755 - val_acc: 0.1020\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.8758 - acc: 0.3605 - val_loss: 2.5150 - val_acc: 0.1015\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.8371 - acc: 0.3675 - val_loss: 2.5384 - val_acc: 0.1065\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.7932 - acc: 0.3855 - val_loss: 2.6003 - val_acc: 0.1065\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.7436 - acc: 0.4080 - val_loss: 2.6056 - val_acc: 0.1025\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.7057 - acc: 0.4235 - val_loss: 2.6519 - val_acc: 0.1065\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.6497 - acc: 0.4570 - val_loss: 2.6973 - val_acc: 0.1045\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.6168 - acc: 0.4645 - val_loss: 2.7253 - val_acc: 0.1030\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.5529 - acc: 0.4830 - val_loss: 2.7943 - val_acc: 0.0990\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.4950 - acc: 0.5265 - val_loss: 2.8394 - val_acc: 0.0980\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.4563 - acc: 0.5350 - val_loss: 2.8735 - val_acc: 0.0995\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.4144 - acc: 0.5405 - val_loss: 2.9106 - val_acc: 0.0920\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.3739 - acc: 0.5660 - val_loss: 2.9808 - val_acc: 0.1000\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.3279 - acc: 0.5745 - val_loss: 2.9921 - val_acc: 0.0990\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.2952 - acc: 0.5910 - val_loss: 3.1002 - val_acc: 0.0945\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.2689 - acc: 0.5920 - val_loss: 3.1524 - val_acc: 0.1105\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.2210 - acc: 0.6195 - val_loss: 3.2056 - val_acc: 0.1040\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.1758 - acc: 0.6380 - val_loss: 3.2795 - val_acc: 0.0950\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.1400 - acc: 0.6550 - val_loss: 3.3361 - val_acc: 0.1040\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.0881 - acc: 0.6710 - val_loss: 3.3959 - val_acc: 0.0950\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.0531 - acc: 0.6910 - val_loss: 3.4955 - val_acc: 0.0995\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.0307 - acc: 0.6875 - val_loss: 3.5679 - val_acc: 0.1015\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9808 - acc: 0.7185 - val_loss: 3.6206 - val_acc: 0.1010\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9531 - acc: 0.7250 - val_loss: 3.6494 - val_acc: 0.0980\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9215 - acc: 0.7310 - val_loss: 3.7700 - val_acc: 0.1020\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8811 - acc: 0.7470 - val_loss: 3.8306 - val_acc: 0.1010\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8547 - acc: 0.7580 - val_loss: 3.9152 - val_acc: 0.1015\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8194 - acc: 0.7665 - val_loss: 4.0050 - val_acc: 0.0940\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7917 - acc: 0.7880 - val_loss: 4.0696 - val_acc: 0.1055\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7511 - acc: 0.7955 - val_loss: 4.1267 - val_acc: 0.1035\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7219 - acc: 0.8080 - val_loss: 4.2736 - val_acc: 0.1030\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7067 - acc: 0.8095 - val_loss: 4.2807 - val_acc: 0.0990\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6698 - acc: 0.8205 - val_loss: 4.4046 - val_acc: 0.1050\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6429 - acc: 0.8300 - val_loss: 4.4269 - val_acc: 0.1000\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6234 - acc: 0.8350 - val_loss: 4.5702 - val_acc: 0.1045\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5779 - acc: 0.8610 - val_loss: 4.6654 - val_acc: 0.1015\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5709 - acc: 0.8585 - val_loss: 4.7467 - val_acc: 0.0990\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5353 - acc: 0.8685 - val_loss: 4.8438 - val_acc: 0.1025\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5249 - acc: 0.8710 - val_loss: 4.9420 - val_acc: 0.1045\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5118 - acc: 0.8690 - val_loss: 5.0481 - val_acc: 0.0965\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.4864 - acc: 0.8870 - val_loss: 5.1316 - val_acc: 0.1040\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.4702 - acc: 0.8930 - val_loss: 5.2007 - val_acc: 0.1045\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.4422 - acc: 0.9045 - val_loss: 5.2913 - val_acc: 0.1030\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.4269 - acc: 0.9080 - val_loss: 5.3408 - val_acc: 0.1105\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 33us/step - loss: 0.4156 - acc: 0.9045 - val_loss: 5.4081 - val_acc: 0.1025\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 0.3969 - acc: 0.9150 - val_loss: 5.4987 - val_acc: 0.1040\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.3750 - acc: 0.9210 - val_loss: 5.5314 - val_acc: 0.1055\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3618 - acc: 0.9235 - val_loss: 5.6579 - val_acc: 0.1070\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3523 - acc: 0.9300 - val_loss: 5.7830 - val_acc: 0.0990\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3325 - acc: 0.9355 - val_loss: 5.8316 - val_acc: 0.1040\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3145 - acc: 0.9355 - val_loss: 5.8869 - val_acc: 0.0985\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3042 - acc: 0.9425 - val_loss: 5.9730 - val_acc: 0.0990\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.3012 - acc: 0.9385 - val_loss: 6.0249 - val_acc: 0.1050\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.2900 - acc: 0.9400 - val_loss: 6.1253 - val_acc: 0.0975\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2698 - acc: 0.9535 - val_loss: 6.1579 - val_acc: 0.1030\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2550 - acc: 0.9535 - val_loss: 6.2955 - val_acc: 0.0990\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2466 - acc: 0.9555 - val_loss: 6.3291 - val_acc: 0.0980\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2382 - acc: 0.9540 - val_loss: 6.4315 - val_acc: 0.1005\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2251 - acc: 0.9620 - val_loss: 6.5438 - val_acc: 0.1030\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.2244 - acc: 0.9615 - val_loss: 6.5654 - val_acc: 0.0975\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.2092 - acc: 0.9655 - val_loss: 6.6737 - val_acc: 0.1030\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1975 - acc: 0.9680 - val_loss: 6.7334 - val_acc: 0.0935\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1950 - acc: 0.9685 - val_loss: 6.7519 - val_acc: 0.1000\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1876 - acc: 0.9760 - val_loss: 6.8280 - val_acc: 0.0975\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1759 - acc: 0.9745 - val_loss: 6.8956 - val_acc: 0.0985\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1717 - acc: 0.9790 - val_loss: 6.9620 - val_acc: 0.0955\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1634 - acc: 0.9800 - val_loss: 7.0096 - val_acc: 0.1020\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1661 - acc: 0.9745 - val_loss: 7.1041 - val_acc: 0.0980\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1530 - acc: 0.9810 - val_loss: 7.1825 - val_acc: 0.0975\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1434 - acc: 0.9830 - val_loss: 7.2239 - val_acc: 0.1005\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1389 - acc: 0.9850 - val_loss: 7.2534 - val_acc: 0.0935\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1329 - acc: 0.9845 - val_loss: 7.3398 - val_acc: 0.1005\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1233 - acc: 0.9865 - val_loss: 7.4000 - val_acc: 0.0975\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.1217 - acc: 0.9875 - val_loss: 7.4991 - val_acc: 0.0955\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1201 - acc: 0.9890 - val_loss: 7.5115 - val_acc: 0.1050\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1132 - acc: 0.9880 - val_loss: 7.5748 - val_acc: 0.0985\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1091 - acc: 0.9875 - val_loss: 7.5965 - val_acc: 0.0970\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1067 - acc: 0.9890 - val_loss: 7.6674 - val_acc: 0.0970\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0997 - acc: 0.9905 - val_loss: 7.6915 - val_acc: 0.0950\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0962 - acc: 0.9910 - val_loss: 7.7872 - val_acc: 0.1000\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0932 - acc: 0.9910 - val_loss: 7.7975 - val_acc: 0.1010\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0891 - acc: 0.9905 - val_loss: 7.8570 - val_acc: 0.0975\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0839 - acc: 0.9940 - val_loss: 7.9041 - val_acc: 0.0990\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0830 - acc: 0.9915 - val_loss: 7.9068 - val_acc: 0.0960\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0816 - acc: 0.9935 - val_loss: 7.9907 - val_acc: 0.0980\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0804 - acc: 0.9940 - val_loss: 8.0046 - val_acc: 0.1060\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0772 - acc: 0.9950 - val_loss: 8.0850 - val_acc: 0.1015\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0741 - acc: 0.9955 - val_loss: 8.1271 - val_acc: 0.1010\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0701 - acc: 0.9960 - val_loss: 8.1311 - val_acc: 0.1010\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0665 - acc: 0.9970 - val_loss: 8.1843 - val_acc: 0.1010\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0627 - acc: 0.9960 - val_loss: 8.2490 - val_acc: 0.1020\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.0619 - acc: 0.9960 - val_loss: 8.2395 - val_acc: 0.0985\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0581 - acc: 0.9970 - val_loss: 8.3077 - val_acc: 0.0980\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0568 - acc: 0.9970 - val_loss: 8.3473 - val_acc: 0.0975\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0557 - acc: 0.9975 - val_loss: 8.3626 - val_acc: 0.1025\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0526 - acc: 0.9970 - val_loss: 8.4061 - val_acc: 0.1005\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0513 - acc: 0.9970 - val_loss: 8.4704 - val_acc: 0.1015\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0495 - acc: 0.9985 - val_loss: 8.4683 - val_acc: 0.1030\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0481 - acc: 0.9970 - val_loss: 8.5021 - val_acc: 0.1010\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0457 - acc: 0.9975 - val_loss: 8.5501 - val_acc: 0.1010\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0448 - acc: 0.9975 - val_loss: 8.5868 - val_acc: 0.1015\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0424 - acc: 0.9975 - val_loss: 8.6180 - val_acc: 0.0995\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0414 - acc: 0.9980 - val_loss: 8.6416 - val_acc: 0.1095\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0384 - acc: 0.9990 - val_loss: 8.7102 - val_acc: 0.0990\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0397 - acc: 0.9980 - val_loss: 8.7455 - val_acc: 0.1030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0390 - acc: 0.9985 - val_loss: 8.7374 - val_acc: 0.0985\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0365 - acc: 0.9990 - val_loss: 8.7594 - val_acc: 0.1000\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0347 - acc: 0.9985 - val_loss: 8.8417 - val_acc: 0.1060\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0350 - acc: 0.9980 - val_loss: 8.8111 - val_acc: 0.1025\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0335 - acc: 0.9995 - val_loss: 8.8734 - val_acc: 0.1050\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0332 - acc: 0.9990 - val_loss: 8.8910 - val_acc: 0.1010\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0338 - acc: 0.9990 - val_loss: 8.9223 - val_acc: 0.1000\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.0315 - acc: 0.9985 - val_loss: 8.9377 - val_acc: 0.1020\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.0316 - acc: 0.9995 - val_loss: 8.9931 - val_acc: 0.1085\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.0298 - acc: 0.9990 - val_loss: 9.0281 - val_acc: 0.1030\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.0269 - acc: 0.9995 - val_loss: 9.0370 - val_acc: 0.1060\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.0269 - acc: 0.9995 - val_loss: 9.0496 - val_acc: 0.1020\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 9.0999 - val_acc: 0.1055\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0238 - acc: 1.0000 - val_loss: 9.1054 - val_acc: 0.1025\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0249 - acc: 0.9985 - val_loss: 9.1225 - val_acc: 0.1000\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 9.1636 - val_acc: 0.1035\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0234 - acc: 0.9990 - val_loss: 9.1924 - val_acc: 0.1060\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0227 - acc: 0.9995 - val_loss: 9.1945 - val_acc: 0.1075\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 9.2367 - val_acc: 0.1040\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0227 - acc: 0.9990 - val_loss: 9.2271 - val_acc: 0.1030\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 9.2715 - val_acc: 0.1065\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 9.3365 - val_acc: 0.1085\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 9.2936 - val_acc: 0.1045\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0190 - acc: 1.0000 - val_loss: 9.3510 - val_acc: 0.1050\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 9.3606 - val_acc: 0.1055\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 9.3810 - val_acc: 0.1045\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0161 - acc: 1.0000 - val_loss: 9.4134 - val_acc: 0.1065\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 9.4095 - val_acc: 0.1050\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 9.4453 - val_acc: 0.1020\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0153 - acc: 1.0000 - val_loss: 9.4609 - val_acc: 0.1040\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0145 - acc: 1.0000 - val_loss: 9.4844 - val_acc: 0.1035\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 9.4886 - val_acc: 0.1055\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 9.5139 - val_acc: 0.1080\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 9.5215 - val_acc: 0.1035\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 9.5332 - val_acc: 0.1040\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 9.5748 - val_acc: 0.1050\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 9.5806 - val_acc: 0.1065\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 9.5964 - val_acc: 0.1040\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 9.6275 - val_acc: 0.1060\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0114 - acc: 1.0000 - val_loss: 9.6289 - val_acc: 0.1040\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 9.6359 - val_acc: 0.1035\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0110 - acc: 1.0000 - val_loss: 9.6596 - val_acc: 0.1045\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 9.6808 - val_acc: 0.1080\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 9.6868 - val_acc: 0.1045\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 9.7127 - val_acc: 0.1005\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 9.7243 - val_acc: 0.1025\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 9.7517 - val_acc: 0.1075\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 9.7562 - val_acc: 0.1070\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 46us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 9.7594 - val_acc: 0.1030\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 9.7957 - val_acc: 0.1075\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 9.7945 - val_acc: 0.1060\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 9.8100 - val_acc: 0.1050\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 9.8405 - val_acc: 0.1065\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 9.8307 - val_acc: 0.1055\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 9.8577 - val_acc: 0.1035\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 9.8643 - val_acc: 0.1025\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 9.8802 - val_acc: 0.1045\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 9.8955 - val_acc: 0.1025\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 9.9053 - val_acc: 0.1045\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 9.9226 - val_acc: 0.1060\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 9.9386 - val_acc: 0.1060\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 9.9393 - val_acc: 0.1045\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 9.9463 - val_acc: 0.1035\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 9.9737 - val_acc: 0.1065\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 9.9804 - val_acc: 0.1035\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 9.9820 - val_acc: 0.1035\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 10.0006 - val_acc: 0.1040\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 10.0140 - val_acc: 0.1020\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 10.0374 - val_acc: 0.1070\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 10.0350 - val_acc: 0.1035\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 10.0570 - val_acc: 0.1050\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 10.0586 - val_acc: 0.1030\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 10.0797 - val_acc: 0.1040\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 10.0748 - val_acc: 0.1060\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 10.0921 - val_acc: 0.1030\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 10.1096 - val_acc: 0.1055\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 10.1249 - val_acc: 0.1055\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 10.1261 - val_acc: 0.1075\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 10.1386 - val_acc: 0.1035\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 10.1513 - val_acc: 0.1040\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 10.1586 - val_acc: 0.1045\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history2 = model2.fit(x_train[:4000], y_train[:4000], batch_size=128, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:25:28.891325Z",
     "start_time": "2018-07-06T03:25:20.460184Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 107us/step - loss: 2.3087 - acc: 0.1055 - val_loss: 2.3085 - val_acc: 0.0935\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2963 - acc: 0.1195 - val_loss: 2.3028 - val_acc: 0.1080\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2866 - acc: 0.1235 - val_loss: 2.3034 - val_acc: 0.1040\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2789 - acc: 0.1375 - val_loss: 2.3079 - val_acc: 0.0970\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2656 - acc: 0.1475 - val_loss: 2.3104 - val_acc: 0.0985\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2514 - acc: 0.1565 - val_loss: 2.3197 - val_acc: 0.0955\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2304 - acc: 0.1715 - val_loss: 2.3315 - val_acc: 0.0905\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2105 - acc: 0.1745 - val_loss: 2.3379 - val_acc: 0.0925\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.1924 - acc: 0.1830 - val_loss: 2.3519 - val_acc: 0.0945\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.1619 - acc: 0.2025 - val_loss: 2.3814 - val_acc: 0.0915\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.1364 - acc: 0.2165 - val_loss: 2.3709 - val_acc: 0.0865\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.1106 - acc: 0.2405 - val_loss: 2.3990 - val_acc: 0.0865\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.0686 - acc: 0.2525 - val_loss: 2.4293 - val_acc: 0.0945\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.0437 - acc: 0.2585 - val_loss: 2.4516 - val_acc: 0.0950\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.0116 - acc: 0.2830 - val_loss: 2.4611 - val_acc: 0.0935\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.9704 - acc: 0.3015 - val_loss: 2.4851 - val_acc: 0.0835\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.9267 - acc: 0.3260 - val_loss: 2.4962 - val_acc: 0.0955\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.8846 - acc: 0.3420 - val_loss: 2.5378 - val_acc: 0.0910\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.8440 - acc: 0.3465 - val_loss: 2.5757 - val_acc: 0.0925\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.8050 - acc: 0.3735 - val_loss: 2.6264 - val_acc: 0.0960\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.7742 - acc: 0.3860 - val_loss: 2.6520 - val_acc: 0.0880\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.7414 - acc: 0.4005 - val_loss: 2.6990 - val_acc: 0.0900\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.7017 - acc: 0.4205 - val_loss: 2.7098 - val_acc: 0.0915\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.6457 - acc: 0.4365 - val_loss: 2.7598 - val_acc: 0.0965\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.6030 - acc: 0.4430 - val_loss: 2.8153 - val_acc: 0.0930\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.5802 - acc: 0.4615 - val_loss: 2.8590 - val_acc: 0.0950\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.5365 - acc: 0.4765 - val_loss: 2.8922 - val_acc: 0.0950\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.5003 - acc: 0.4910 - val_loss: 2.9158 - val_acc: 0.0930\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.4598 - acc: 0.5115 - val_loss: 3.0215 - val_acc: 0.0965\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.4233 - acc: 0.5235 - val_loss: 3.0641 - val_acc: 0.1010\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.3714 - acc: 0.5455 - val_loss: 3.0811 - val_acc: 0.0900\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.3469 - acc: 0.5550 - val_loss: 3.1533 - val_acc: 0.1025\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.3023 - acc: 0.5765 - val_loss: 3.1952 - val_acc: 0.1015\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.2797 - acc: 0.5845 - val_loss: 3.2284 - val_acc: 0.0990\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.2431 - acc: 0.6005 - val_loss: 3.3189 - val_acc: 0.1065\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.2103 - acc: 0.6145 - val_loss: 3.3905 - val_acc: 0.0945\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.1575 - acc: 0.6380 - val_loss: 3.4747 - val_acc: 0.0935\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.1400 - acc: 0.6455 - val_loss: 3.5175 - val_acc: 0.1005\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.1039 - acc: 0.6525 - val_loss: 3.5649 - val_acc: 0.0980\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.0672 - acc: 0.6670 - val_loss: 3.6181 - val_acc: 0.0935\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.0333 - acc: 0.6875 - val_loss: 3.6936 - val_acc: 0.0960\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.0037 - acc: 0.6900 - val_loss: 3.7339 - val_acc: 0.1020\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.9773 - acc: 0.7105 - val_loss: 3.8528 - val_acc: 0.0945\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.9468 - acc: 0.7180 - val_loss: 3.9408 - val_acc: 0.0975\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.9142 - acc: 0.7340 - val_loss: 3.9991 - val_acc: 0.0850\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.9343 - acc: 0.7165 - val_loss: 4.0538 - val_acc: 0.0945\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.8960 - acc: 0.7350 - val_loss: 4.1213 - val_acc: 0.0990\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.8465 - acc: 0.7670 - val_loss: 4.1668 - val_acc: 0.0965\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.8106 - acc: 0.7810 - val_loss: 4.2293 - val_acc: 0.0970\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.7784 - acc: 0.7940 - val_loss: 4.3229 - val_acc: 0.0970\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.7559 - acc: 0.7955 - val_loss: 4.4097 - val_acc: 0.1000\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 0.7450 - acc: 0.8005 - val_loss: 4.4803 - val_acc: 0.0945\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7198 - acc: 0.8045 - val_loss: 4.5400 - val_acc: 0.1035\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.6863 - acc: 0.8210 - val_loss: 4.6180 - val_acc: 0.1010\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.6675 - acc: 0.8300 - val_loss: 4.7065 - val_acc: 0.0990\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.6384 - acc: 0.8435 - val_loss: 4.8638 - val_acc: 0.1010\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.6249 - acc: 0.8365 - val_loss: 4.8449 - val_acc: 0.1025\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.5966 - acc: 0.8565 - val_loss: 4.9324 - val_acc: 0.0995\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.5846 - acc: 0.8595 - val_loss: 4.9888 - val_acc: 0.1015\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.5664 - acc: 0.8665 - val_loss: 5.0327 - val_acc: 0.1075\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.5653 - acc: 0.8535 - val_loss: 5.1681 - val_acc: 0.1000\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.5478 - acc: 0.8630 - val_loss: 5.2622 - val_acc: 0.1030\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.5403 - acc: 0.8610 - val_loss: 5.3527 - val_acc: 0.1015\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.5165 - acc: 0.8705 - val_loss: 5.3878 - val_acc: 0.0955\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4975 - acc: 0.8785 - val_loss: 5.4783 - val_acc: 0.0960\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4789 - acc: 0.8840 - val_loss: 5.6373 - val_acc: 0.0990\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4534 - acc: 0.8940 - val_loss: 5.6290 - val_acc: 0.0900\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4453 - acc: 0.9010 - val_loss: 5.6667 - val_acc: 0.0975\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.4183 - acc: 0.9040 - val_loss: 5.7979 - val_acc: 0.0995\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4087 - acc: 0.9105 - val_loss: 5.9145 - val_acc: 0.0960\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.4031 - acc: 0.9110 - val_loss: 5.9052 - val_acc: 0.1025\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3838 - acc: 0.9155 - val_loss: 6.0467 - val_acc: 0.0960\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3769 - acc: 0.9175 - val_loss: 6.0581 - val_acc: 0.0980\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3595 - acc: 0.9270 - val_loss: 6.0854 - val_acc: 0.0905\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3464 - acc: 0.9325 - val_loss: 6.0863 - val_acc: 0.0980\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3320 - acc: 0.9310 - val_loss: 6.2646 - val_acc: 0.1000\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3220 - acc: 0.9355 - val_loss: 6.4049 - val_acc: 0.1010\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3135 - acc: 0.9375 - val_loss: 6.3682 - val_acc: 0.1050\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3080 - acc: 0.9375 - val_loss: 6.3897 - val_acc: 0.1000\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2862 - acc: 0.9450 - val_loss: 6.5075 - val_acc: 0.0995\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2719 - acc: 0.9515 - val_loss: 6.5900 - val_acc: 0.0950\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2707 - acc: 0.9515 - val_loss: 6.6677 - val_acc: 0.0980\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2621 - acc: 0.9510 - val_loss: 6.7036 - val_acc: 0.0975\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2491 - acc: 0.9585 - val_loss: 6.8431 - val_acc: 0.0975\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2429 - acc: 0.9560 - val_loss: 6.8132 - val_acc: 0.1020\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2356 - acc: 0.9590 - val_loss: 6.8812 - val_acc: 0.0975\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2286 - acc: 0.9600 - val_loss: 6.9293 - val_acc: 0.1015\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2217 - acc: 0.9620 - val_loss: 7.0434 - val_acc: 0.1000\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2160 - acc: 0.9685 - val_loss: 7.1251 - val_acc: 0.0980\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2172 - acc: 0.9595 - val_loss: 7.0726 - val_acc: 0.1010\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2042 - acc: 0.9660 - val_loss: 7.1613 - val_acc: 0.0995\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2020 - acc: 0.9660 - val_loss: 7.2425 - val_acc: 0.1005\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1955 - acc: 0.9655 - val_loss: 7.3235 - val_acc: 0.0955\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1867 - acc: 0.9705 - val_loss: 7.2931 - val_acc: 0.1030\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1760 - acc: 0.9755 - val_loss: 7.3750 - val_acc: 0.0970\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1724 - acc: 0.9730 - val_loss: 7.4455 - val_acc: 0.0965\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1662 - acc: 0.9750 - val_loss: 7.5150 - val_acc: 0.0965\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1601 - acc: 0.9760 - val_loss: 7.5241 - val_acc: 0.1015\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.1526 - acc: 0.9800 - val_loss: 7.6338 - val_acc: 0.0950\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 41us/step - loss: 0.1464 - acc: 0.9800 - val_loss: 7.6623 - val_acc: 0.0990\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1419 - acc: 0.9825 - val_loss: 7.6827 - val_acc: 0.0955\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.1376 - acc: 0.9820 - val_loss: 7.7578 - val_acc: 0.0940\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1346 - acc: 0.9845 - val_loss: 7.7791 - val_acc: 0.1000\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1289 - acc: 0.9860 - val_loss: 7.8175 - val_acc: 0.0985\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1256 - acc: 0.9875 - val_loss: 7.8986 - val_acc: 0.0980\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1157 - acc: 0.9895 - val_loss: 8.0019 - val_acc: 0.1000\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1129 - acc: 0.9890 - val_loss: 8.0063 - val_acc: 0.0980\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1081 - acc: 0.9910 - val_loss: 8.0068 - val_acc: 0.0995\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1077 - acc: 0.9885 - val_loss: 8.0977 - val_acc: 0.0985\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.1012 - acc: 0.9890 - val_loss: 8.1080 - val_acc: 0.0970\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0990 - acc: 0.9905 - val_loss: 8.1664 - val_acc: 0.1000\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0966 - acc: 0.9925 - val_loss: 8.1772 - val_acc: 0.1000\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0947 - acc: 0.9895 - val_loss: 8.2324 - val_acc: 0.1025\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0891 - acc: 0.9925 - val_loss: 8.3235 - val_acc: 0.0975\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0869 - acc: 0.9935 - val_loss: 8.3198 - val_acc: 0.0990\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0862 - acc: 0.9925 - val_loss: 8.3965 - val_acc: 0.0985\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0828 - acc: 0.9940 - val_loss: 8.4042 - val_acc: 0.0955\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0802 - acc: 0.9945 - val_loss: 8.4880 - val_acc: 0.0965\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0804 - acc: 0.9945 - val_loss: 8.4826 - val_acc: 0.1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0782 - acc: 0.9945 - val_loss: 8.4876 - val_acc: 0.0980\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0749 - acc: 0.9955 - val_loss: 8.5799 - val_acc: 0.0970\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0719 - acc: 0.9940 - val_loss: 8.6004 - val_acc: 0.0970\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0742 - acc: 0.9945 - val_loss: 8.6052 - val_acc: 0.0995\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0697 - acc: 0.9955 - val_loss: 8.6587 - val_acc: 0.0975\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0645 - acc: 0.9970 - val_loss: 8.6848 - val_acc: 0.0970\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.0643 - acc: 0.9960 - val_loss: 8.7609 - val_acc: 0.0985\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0633 - acc: 0.9975 - val_loss: 8.7874 - val_acc: 0.1005\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.0597 - acc: 0.9970 - val_loss: 8.7544 - val_acc: 0.1005\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.0585 - acc: 0.9965 - val_loss: 8.8328 - val_acc: 0.0945\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 35us/step - loss: 0.0549 - acc: 0.9975 - val_loss: 8.8386 - val_acc: 0.0955\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.0522 - acc: 0.9975 - val_loss: 8.9043 - val_acc: 0.0960\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0495 - acc: 0.9975 - val_loss: 8.9023 - val_acc: 0.0960\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0489 - acc: 0.9975 - val_loss: 8.9611 - val_acc: 0.0990\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0473 - acc: 0.9985 - val_loss: 8.9938 - val_acc: 0.1000\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0464 - acc: 0.9980 - val_loss: 9.0128 - val_acc: 0.0960\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0458 - acc: 0.9980 - val_loss: 9.0177 - val_acc: 0.0980\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0438 - acc: 0.9980 - val_loss: 9.0695 - val_acc: 0.1000\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0424 - acc: 0.9990 - val_loss: 9.0993 - val_acc: 0.0985\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0401 - acc: 0.9985 - val_loss: 9.1460 - val_acc: 0.0960\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0390 - acc: 0.9985 - val_loss: 9.1403 - val_acc: 0.0980\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0384 - acc: 0.9985 - val_loss: 9.2079 - val_acc: 0.0935\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0380 - acc: 0.9985 - val_loss: 9.1983 - val_acc: 0.0935\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0378 - acc: 0.9980 - val_loss: 9.2375 - val_acc: 0.0940\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0367 - acc: 0.9985 - val_loss: 9.2570 - val_acc: 0.0965\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0336 - acc: 0.9990 - val_loss: 9.2834 - val_acc: 0.0945\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0334 - acc: 0.9990 - val_loss: 9.3335 - val_acc: 0.0965\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0330 - acc: 0.9980 - val_loss: 9.3317 - val_acc: 0.0955\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0317 - acc: 0.9995 - val_loss: 9.3733 - val_acc: 0.0975\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0313 - acc: 0.9990 - val_loss: 9.3833 - val_acc: 0.0940\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 9.4460 - val_acc: 0.0925\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0293 - acc: 0.9995 - val_loss: 9.4125 - val_acc: 0.0965\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0288 - acc: 1.0000 - val_loss: 9.4667 - val_acc: 0.0955\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0272 - acc: 1.0000 - val_loss: 9.4859 - val_acc: 0.0975\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0262 - acc: 0.9995 - val_loss: 9.5059 - val_acc: 0.0925\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0266 - acc: 0.9995 - val_loss: 9.5036 - val_acc: 0.0940\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 9.5494 - val_acc: 0.0965\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 9.5666 - val_acc: 0.0925\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0245 - acc: 1.0000 - val_loss: 9.5938 - val_acc: 0.0965\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0229 - acc: 1.0000 - val_loss: 9.6336 - val_acc: 0.0940\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0224 - acc: 1.0000 - val_loss: 9.6359 - val_acc: 0.0940\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0216 - acc: 0.9995 - val_loss: 9.6714 - val_acc: 0.0945\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 9.6823 - val_acc: 0.0950\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0211 - acc: 1.0000 - val_loss: 9.7016 - val_acc: 0.0965\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0202 - acc: 1.0000 - val_loss: 9.7159 - val_acc: 0.0975\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 9.7427 - val_acc: 0.0935\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 9.7726 - val_acc: 0.0940\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0184 - acc: 1.0000 - val_loss: 9.7926 - val_acc: 0.0925\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 9.8099 - val_acc: 0.0930\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0177 - acc: 1.0000 - val_loss: 9.8290 - val_acc: 0.0940\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 9.8569 - val_acc: 0.0935\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 9.8531 - val_acc: 0.0930\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0171 - acc: 1.0000 - val_loss: 9.8928 - val_acc: 0.0935\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 9.8730 - val_acc: 0.0950\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 9.8997 - val_acc: 0.0945\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 9.9196 - val_acc: 0.0925\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 9.9443 - val_acc: 0.0955\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 9.9507 - val_acc: 0.0945\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 10.0092 - val_acc: 0.0965\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0139 - acc: 1.0000 - val_loss: 10.0184 - val_acc: 0.0930\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 10.0003 - val_acc: 0.0955\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 10.0200 - val_acc: 0.0935\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 10.0655 - val_acc: 0.0910\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 10.0607 - val_acc: 0.0940\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 10.0565 - val_acc: 0.0955\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 10.0969 - val_acc: 0.0925\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 10.1034 - val_acc: 0.0920\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 10.1199 - val_acc: 0.0950\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 10.1272 - val_acc: 0.0920\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 10.1618 - val_acc: 0.0955\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 10.1753 - val_acc: 0.0955\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 10.1833 - val_acc: 0.0955\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 10.2010 - val_acc: 0.0930\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 10.2138 - val_acc: 0.0930\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 10.2290 - val_acc: 0.0945\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 10.2356 - val_acc: 0.0960\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 10.2448 - val_acc: 0.0945\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 10.2629 - val_acc: 0.0930\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 10.2776 - val_acc: 0.0950\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 10.2576 - val_acc: 0.0945\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 10.3165 - val_acc: 0.0950\n"
     ]
    }
   ],
   "source": [
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history3 = model3.fit(x_train[:4000], y_train[:4000], batch_size=128, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:25:45.104373Z",
     "start_time": "2018-07-06T03:25:36.732282Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 114us/step - loss: 2.3031 - acc: 0.1020 - val_loss: 2.3043 - val_acc: 0.1005\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.2921 - acc: 0.1210 - val_loss: 2.3062 - val_acc: 0.1025\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.2815 - acc: 0.1290 - val_loss: 2.3093 - val_acc: 0.1000\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.2691 - acc: 0.1520 - val_loss: 2.3159 - val_acc: 0.0990\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.2590 - acc: 0.1545 - val_loss: 2.3201 - val_acc: 0.0995\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.2459 - acc: 0.1700 - val_loss: 2.3240 - val_acc: 0.0995\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.2281 - acc: 0.1930 - val_loss: 2.3189 - val_acc: 0.1020\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.2169 - acc: 0.1900 - val_loss: 2.3238 - val_acc: 0.1010\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.1981 - acc: 0.2055 - val_loss: 2.3310 - val_acc: 0.1020\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.1791 - acc: 0.2045 - val_loss: 2.3454 - val_acc: 0.0940\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.1616 - acc: 0.2245 - val_loss: 2.3481 - val_acc: 0.0915\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.1388 - acc: 0.2380 - val_loss: 2.3633 - val_acc: 0.0995\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.1213 - acc: 0.2400 - val_loss: 2.3492 - val_acc: 0.1030\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.0985 - acc: 0.2525 - val_loss: 2.3862 - val_acc: 0.0940\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.0680 - acc: 0.2595 - val_loss: 2.3988 - val_acc: 0.1040\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 2.0436 - acc: 0.2725 - val_loss: 2.4181 - val_acc: 0.1025\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 2.0078 - acc: 0.2900 - val_loss: 2.4142 - val_acc: 0.0950\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.9880 - acc: 0.3000 - val_loss: 2.4372 - val_acc: 0.1105\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.9569 - acc: 0.3100 - val_loss: 2.4534 - val_acc: 0.1015\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.9187 - acc: 0.3260 - val_loss: 2.5073 - val_acc: 0.1080\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.9052 - acc: 0.3315 - val_loss: 2.4853 - val_acc: 0.1035\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.8711 - acc: 0.3340 - val_loss: 2.5219 - val_acc: 0.1040\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.8307 - acc: 0.3650 - val_loss: 2.5560 - val_acc: 0.1035\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.7966 - acc: 0.3710 - val_loss: 2.5776 - val_acc: 0.1080\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.7603 - acc: 0.3900 - val_loss: 2.5780 - val_acc: 0.1035\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.7276 - acc: 0.4030 - val_loss: 2.6372 - val_acc: 0.1095\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.7015 - acc: 0.4010 - val_loss: 2.6556 - val_acc: 0.1005\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.6641 - acc: 0.4310 - val_loss: 2.7066 - val_acc: 0.1080\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.6486 - acc: 0.4275 - val_loss: 2.8086 - val_acc: 0.1035\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.6153 - acc: 0.4465 - val_loss: 2.7380 - val_acc: 0.1090\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.5770 - acc: 0.4585 - val_loss: 2.7407 - val_acc: 0.1080\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.5545 - acc: 0.4630 - val_loss: 2.8409 - val_acc: 0.1020\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.5171 - acc: 0.4710 - val_loss: 2.9046 - val_acc: 0.1050\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.4927 - acc: 0.4880 - val_loss: 2.8837 - val_acc: 0.1060\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.4638 - acc: 0.5110 - val_loss: 2.9602 - val_acc: 0.1105\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.4313 - acc: 0.5175 - val_loss: 3.0150 - val_acc: 0.1045\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.3978 - acc: 0.5290 - val_loss: 3.0403 - val_acc: 0.1085\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.3641 - acc: 0.5335 - val_loss: 3.0099 - val_acc: 0.1100\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.3272 - acc: 0.5685 - val_loss: 3.0583 - val_acc: 0.1100\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.3079 - acc: 0.5665 - val_loss: 3.0976 - val_acc: 0.1090\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.2785 - acc: 0.5745 - val_loss: 3.1192 - val_acc: 0.1085\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.2401 - acc: 0.5875 - val_loss: 3.2088 - val_acc: 0.1105\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.2179 - acc: 0.6050 - val_loss: 3.3226 - val_acc: 0.1025\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.1953 - acc: 0.6075 - val_loss: 3.2444 - val_acc: 0.1070\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.1638 - acc: 0.6190 - val_loss: 3.3498 - val_acc: 0.1135\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.1340 - acc: 0.6315 - val_loss: 3.3713 - val_acc: 0.1090\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.1194 - acc: 0.6370 - val_loss: 3.4521 - val_acc: 0.1135\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.0925 - acc: 0.6480 - val_loss: 3.5109 - val_acc: 0.1095\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.0591 - acc: 0.6645 - val_loss: 3.5935 - val_acc: 0.1045\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 1.0387 - acc: 0.6705 - val_loss: 3.6784 - val_acc: 0.1115\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 1.0158 - acc: 0.6925 - val_loss: 3.7736 - val_acc: 0.1025\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.9962 - acc: 0.6850 - val_loss: 3.7231 - val_acc: 0.1065\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.9657 - acc: 0.6985 - val_loss: 3.8452 - val_acc: 0.1030\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9621 - acc: 0.7000 - val_loss: 3.9129 - val_acc: 0.1035\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.9358 - acc: 0.7095 - val_loss: 3.9125 - val_acc: 0.1070\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9128 - acc: 0.7210 - val_loss: 3.9004 - val_acc: 0.1050\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.9075 - acc: 0.7145 - val_loss: 4.0001 - val_acc: 0.1070\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.8842 - acc: 0.7245 - val_loss: 3.9901 - val_acc: 0.1100\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.8587 - acc: 0.7345 - val_loss: 4.1342 - val_acc: 0.1115\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.8171 - acc: 0.7525 - val_loss: 4.2018 - val_acc: 0.1050\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.8083 - acc: 0.7545 - val_loss: 4.1687 - val_acc: 0.1095\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7917 - acc: 0.7610 - val_loss: 4.2181 - val_acc: 0.1070\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7834 - acc: 0.7600 - val_loss: 4.3937 - val_acc: 0.1085\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7648 - acc: 0.7770 - val_loss: 4.5050 - val_acc: 0.1020\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7475 - acc: 0.7730 - val_loss: 4.5616 - val_acc: 0.1015\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7328 - acc: 0.7765 - val_loss: 4.5806 - val_acc: 0.1025\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7093 - acc: 0.7920 - val_loss: 4.5944 - val_acc: 0.1050\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6901 - acc: 0.7985 - val_loss: 4.6362 - val_acc: 0.1030\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6676 - acc: 0.8125 - val_loss: 4.7458 - val_acc: 0.1030\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6600 - acc: 0.8130 - val_loss: 4.7872 - val_acc: 0.1015\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6432 - acc: 0.8185 - val_loss: 4.9075 - val_acc: 0.1020\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6324 - acc: 0.8220 - val_loss: 4.8942 - val_acc: 0.1045\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6234 - acc: 0.8205 - val_loss: 4.9154 - val_acc: 0.1030\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6100 - acc: 0.8295 - val_loss: 5.1046 - val_acc: 0.1015\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5885 - acc: 0.8340 - val_loss: 5.0764 - val_acc: 0.1050\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.5738 - acc: 0.8425 - val_loss: 5.1342 - val_acc: 0.1000\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5648 - acc: 0.8450 - val_loss: 5.3415 - val_acc: 0.0995\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5475 - acc: 0.8485 - val_loss: 5.3239 - val_acc: 0.1000\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5377 - acc: 0.8465 - val_loss: 5.4350 - val_acc: 0.0950\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5299 - acc: 0.8515 - val_loss: 5.5014 - val_acc: 0.0980\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5165 - acc: 0.8550 - val_loss: 5.6105 - val_acc: 0.1035\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5099 - acc: 0.8670 - val_loss: 5.4913 - val_acc: 0.1055\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5029 - acc: 0.8670 - val_loss: 5.5278 - val_acc: 0.1065\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4809 - acc: 0.8625 - val_loss: 5.5874 - val_acc: 0.1060\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4706 - acc: 0.8760 - val_loss: 5.7025 - val_acc: 0.1055\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4572 - acc: 0.8775 - val_loss: 5.6423 - val_acc: 0.1060\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.4556 - acc: 0.8855 - val_loss: 5.7530 - val_acc: 0.1040\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4474 - acc: 0.8845 - val_loss: 5.8756 - val_acc: 0.1035\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.4277 - acc: 0.8905 - val_loss: 6.0192 - val_acc: 0.1050\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.4077 - acc: 0.8945 - val_loss: 5.9762 - val_acc: 0.1050\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.4138 - acc: 0.8965 - val_loss: 6.1123 - val_acc: 0.1005\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3984 - acc: 0.8985 - val_loss: 6.0779 - val_acc: 0.1030\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3894 - acc: 0.9065 - val_loss: 6.1264 - val_acc: 0.1025\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3863 - acc: 0.9065 - val_loss: 6.1798 - val_acc: 0.1065\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3811 - acc: 0.9055 - val_loss: 6.2505 - val_acc: 0.1070\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3631 - acc: 0.9125 - val_loss: 6.3188 - val_acc: 0.1015\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3539 - acc: 0.9160 - val_loss: 6.3363 - val_acc: 0.1100\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3495 - acc: 0.9160 - val_loss: 6.2928 - val_acc: 0.1070\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3471 - acc: 0.9220 - val_loss: 6.5869 - val_acc: 0.1025\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3382 - acc: 0.9205 - val_loss: 6.5638 - val_acc: 0.1030\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3241 - acc: 0.9245 - val_loss: 6.6783 - val_acc: 0.0960\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.3188 - acc: 0.9270 - val_loss: 6.6984 - val_acc: 0.1055\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3075 - acc: 0.9300 - val_loss: 6.6013 - val_acc: 0.1010\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.3011 - acc: 0.9310 - val_loss: 6.6227 - val_acc: 0.1065\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2921 - acc: 0.9360 - val_loss: 6.7116 - val_acc: 0.1095\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2870 - acc: 0.9360 - val_loss: 6.8186 - val_acc: 0.1005\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2788 - acc: 0.9415 - val_loss: 6.8855 - val_acc: 0.0975\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2865 - acc: 0.9325 - val_loss: 6.9419 - val_acc: 0.1035\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2762 - acc: 0.9405 - val_loss: 6.9141 - val_acc: 0.1055\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2679 - acc: 0.9440 - val_loss: 7.0108 - val_acc: 0.0990\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2597 - acc: 0.9435 - val_loss: 6.9903 - val_acc: 0.1045\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2546 - acc: 0.9465 - val_loss: 7.0857 - val_acc: 0.1005\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2425 - acc: 0.9490 - val_loss: 7.0983 - val_acc: 0.1060\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2437 - acc: 0.9495 - val_loss: 7.2664 - val_acc: 0.1020\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2329 - acc: 0.9525 - val_loss: 7.2968 - val_acc: 0.1005\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2253 - acc: 0.9535 - val_loss: 7.3201 - val_acc: 0.0970\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.2244 - acc: 0.9520 - val_loss: 7.2482 - val_acc: 0.1045\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2125 - acc: 0.9570 - val_loss: 7.3320 - val_acc: 0.1065\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2115 - acc: 0.9590 - val_loss: 7.4236 - val_acc: 0.0975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.2053 - acc: 0.9590 - val_loss: 7.4686 - val_acc: 0.0985\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1981 - acc: 0.9605 - val_loss: 7.4729 - val_acc: 0.1000\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1997 - acc: 0.9575 - val_loss: 7.5550 - val_acc: 0.1010\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1966 - acc: 0.9590 - val_loss: 7.6122 - val_acc: 0.1025\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1897 - acc: 0.9600 - val_loss: 7.6352 - val_acc: 0.1000\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1844 - acc: 0.9635 - val_loss: 7.6444 - val_acc: 0.0990\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1761 - acc: 0.9625 - val_loss: 7.6986 - val_acc: 0.0960\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1714 - acc: 0.9650 - val_loss: 7.7951 - val_acc: 0.0965\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1718 - acc: 0.9670 - val_loss: 7.8455 - val_acc: 0.0975\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1778 - acc: 0.9625 - val_loss: 7.8652 - val_acc: 0.0940\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1627 - acc: 0.9670 - val_loss: 7.8919 - val_acc: 0.0975\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1580 - acc: 0.9680 - val_loss: 7.8494 - val_acc: 0.1035\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1600 - acc: 0.9665 - val_loss: 7.9073 - val_acc: 0.0975\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.1575 - acc: 0.9685 - val_loss: 7.9698 - val_acc: 0.0945\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1526 - acc: 0.9675 - val_loss: 7.9571 - val_acc: 0.0990\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1488 - acc: 0.9685 - val_loss: 8.1122 - val_acc: 0.0965\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1460 - acc: 0.9700 - val_loss: 8.0666 - val_acc: 0.0990\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1409 - acc: 0.9725 - val_loss: 8.1275 - val_acc: 0.0970\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 27us/step - loss: 0.1392 - acc: 0.9715 - val_loss: 8.2288 - val_acc: 0.0980\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.1376 - acc: 0.9735 - val_loss: 8.2185 - val_acc: 0.1005\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1390 - acc: 0.9705 - val_loss: 8.2743 - val_acc: 0.0930\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.1327 - acc: 0.9755 - val_loss: 8.3694 - val_acc: 0.0955\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.1290 - acc: 0.9740 - val_loss: 8.3367 - val_acc: 0.0990\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.1316 - acc: 0.9720 - val_loss: 8.3098 - val_acc: 0.0955\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1298 - acc: 0.9735 - val_loss: 8.3698 - val_acc: 0.0980\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1211 - acc: 0.9770 - val_loss: 8.4839 - val_acc: 0.0980\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1184 - acc: 0.9755 - val_loss: 8.4783 - val_acc: 0.1005\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1156 - acc: 0.9750 - val_loss: 8.5276 - val_acc: 0.0960\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1157 - acc: 0.9765 - val_loss: 8.4976 - val_acc: 0.0975\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1155 - acc: 0.9770 - val_loss: 8.5123 - val_acc: 0.0995\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1115 - acc: 0.9775 - val_loss: 8.5670 - val_acc: 0.0975\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1050 - acc: 0.9795 - val_loss: 8.5698 - val_acc: 0.0995\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1010 - acc: 0.9800 - val_loss: 8.7273 - val_acc: 0.0960\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1016 - acc: 0.9790 - val_loss: 8.7269 - val_acc: 0.0945\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0978 - acc: 0.9815 - val_loss: 8.6254 - val_acc: 0.0980\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0960 - acc: 0.9820 - val_loss: 8.7448 - val_acc: 0.0985\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0945 - acc: 0.9810 - val_loss: 8.7829 - val_acc: 0.0980\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0954 - acc: 0.9815 - val_loss: 8.8103 - val_acc: 0.1020\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1044 - acc: 0.9770 - val_loss: 8.9323 - val_acc: 0.0905\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0999 - acc: 0.9815 - val_loss: 8.8322 - val_acc: 0.0990\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0976 - acc: 0.9815 - val_loss: 8.8806 - val_acc: 0.0980\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.1003 - acc: 0.9805 - val_loss: 8.8811 - val_acc: 0.0965\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0900 - acc: 0.9820 - val_loss: 8.9905 - val_acc: 0.0940\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0856 - acc: 0.9830 - val_loss: 9.0322 - val_acc: 0.0980\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0843 - acc: 0.9815 - val_loss: 9.0137 - val_acc: 0.0960\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0841 - acc: 0.9810 - val_loss: 9.0018 - val_acc: 0.0945\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0838 - acc: 0.9830 - val_loss: 8.9887 - val_acc: 0.0955\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0835 - acc: 0.9830 - val_loss: 9.1680 - val_acc: 0.0915\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0816 - acc: 0.9855 - val_loss: 9.1487 - val_acc: 0.0975\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0832 - acc: 0.9810 - val_loss: 9.1405 - val_acc: 0.1000\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0772 - acc: 0.9845 - val_loss: 9.1285 - val_acc: 0.0960\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0723 - acc: 0.9870 - val_loss: 9.2152 - val_acc: 0.0975\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0703 - acc: 0.9865 - val_loss: 9.1493 - val_acc: 0.0970\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0709 - acc: 0.9865 - val_loss: 9.2712 - val_acc: 0.0980\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0668 - acc: 0.9870 - val_loss: 9.2363 - val_acc: 0.1010\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0661 - acc: 0.9870 - val_loss: 9.2832 - val_acc: 0.0910\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0648 - acc: 0.9865 - val_loss: 9.3298 - val_acc: 0.0940\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0625 - acc: 0.9885 - val_loss: 9.3094 - val_acc: 0.0990\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0598 - acc: 0.9885 - val_loss: 9.3574 - val_acc: 0.0960\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0576 - acc: 0.9885 - val_loss: 9.3774 - val_acc: 0.0945\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0563 - acc: 0.9890 - val_loss: 9.4203 - val_acc: 0.0945\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0560 - acc: 0.9895 - val_loss: 9.4374 - val_acc: 0.0980\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0568 - acc: 0.9895 - val_loss: 9.4350 - val_acc: 0.1000\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0586 - acc: 0.9895 - val_loss: 9.5778 - val_acc: 0.0975\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0570 - acc: 0.9900 - val_loss: 9.4865 - val_acc: 0.0955\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0535 - acc: 0.9910 - val_loss: 9.4740 - val_acc: 0.0990\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0511 - acc: 0.9910 - val_loss: 9.5954 - val_acc: 0.0965\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 19us/step - loss: 0.0493 - acc: 0.9920 - val_loss: 9.5654 - val_acc: 0.0955\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0487 - acc: 0.9915 - val_loss: 9.6537 - val_acc: 0.0960\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0492 - acc: 0.9910 - val_loss: 9.7073 - val_acc: 0.0960\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0487 - acc: 0.9920 - val_loss: 9.5704 - val_acc: 0.0990\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0495 - acc: 0.9915 - val_loss: 9.6739 - val_acc: 0.0945\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.0608 - acc: 0.9905 - val_loss: 9.7052 - val_acc: 0.0990\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 18us/step - loss: 0.5075 - acc: 0.8715 - val_loss: 9.5943 - val_acc: 0.1030\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.6089 - acc: 0.8150 - val_loss: 9.2789 - val_acc: 0.1000\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 36us/step - loss: 0.3945 - acc: 0.8750 - val_loss: 9.2985 - val_acc: 0.1000\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3112 - acc: 0.8975 - val_loss: 9.2711 - val_acc: 0.1000\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1785 - acc: 0.9450 - val_loss: 9.4269 - val_acc: 0.0930\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1087 - acc: 0.9745 - val_loss: 9.4058 - val_acc: 0.0960\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0729 - acc: 0.9870 - val_loss: 9.3636 - val_acc: 0.0955\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.0581 - acc: 0.9900 - val_loss: 9.4902 - val_acc: 0.0960\n"
     ]
    }
   ],
   "source": [
    "model4.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history4 = model4.fit(x_train[:4000], y_train[:4000], batch_size=128, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:35:22.533070Z",
     "start_time": "2018-07-06T03:35:13.312961Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/200\n",
      "2000/2000 [==============================] - 0s 145us/step - loss: 2.3071 - acc: 0.1010 - val_loss: 2.3027 - val_acc: 0.1000\n",
      "Epoch 2/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2989 - acc: 0.1250 - val_loss: 2.3037 - val_acc: 0.0985\n",
      "Epoch 3/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.2919 - acc: 0.1190 - val_loss: 2.3045 - val_acc: 0.1080\n",
      "Epoch 4/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.2826 - acc: 0.1295 - val_loss: 2.3092 - val_acc: 0.1015\n",
      "Epoch 5/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2723 - acc: 0.1310 - val_loss: 2.3108 - val_acc: 0.1140\n",
      "Epoch 6/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2630 - acc: 0.1350 - val_loss: 2.3122 - val_acc: 0.1115\n",
      "Epoch 7/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.2491 - acc: 0.1460 - val_loss: 2.3163 - val_acc: 0.1105\n",
      "Epoch 8/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2362 - acc: 0.1590 - val_loss: 2.3250 - val_acc: 0.1080\n",
      "Epoch 9/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.2227 - acc: 0.1655 - val_loss: 2.3248 - val_acc: 0.1060\n",
      "Epoch 10/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.2037 - acc: 0.1755 - val_loss: 2.3386 - val_acc: 0.1055\n",
      "Epoch 11/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1898 - acc: 0.1910 - val_loss: 2.3453 - val_acc: 0.1090\n",
      "Epoch 12/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1673 - acc: 0.1960 - val_loss: 2.3564 - val_acc: 0.1035\n",
      "Epoch 13/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1493 - acc: 0.2010 - val_loss: 2.3584 - val_acc: 0.1085\n",
      "Epoch 14/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.1346 - acc: 0.2155 - val_loss: 2.3742 - val_acc: 0.1025\n",
      "Epoch 15/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.1071 - acc: 0.2250 - val_loss: 2.3834 - val_acc: 0.1130\n",
      "Epoch 16/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 2.0835 - acc: 0.2380 - val_loss: 2.4088 - val_acc: 0.1000\n",
      "Epoch 17/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.0535 - acc: 0.2515 - val_loss: 2.4133 - val_acc: 0.1110\n",
      "Epoch 18/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 2.0204 - acc: 0.2800 - val_loss: 2.4292 - val_acc: 0.0980\n",
      "Epoch 19/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.9911 - acc: 0.2805 - val_loss: 2.4596 - val_acc: 0.0950\n",
      "Epoch 20/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.9813 - acc: 0.2915 - val_loss: 2.4710 - val_acc: 0.1005\n",
      "Epoch 21/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.9563 - acc: 0.2975 - val_loss: 2.5097 - val_acc: 0.0975\n",
      "Epoch 22/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.9222 - acc: 0.3185 - val_loss: 2.5241 - val_acc: 0.0985\n",
      "Epoch 23/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.8830 - acc: 0.3405 - val_loss: 2.5488 - val_acc: 0.0955\n",
      "Epoch 24/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.8603 - acc: 0.3415 - val_loss: 2.5694 - val_acc: 0.1035\n",
      "Epoch 25/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.8272 - acc: 0.3535 - val_loss: 2.5908 - val_acc: 0.0960\n",
      "Epoch 26/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.8031 - acc: 0.3675 - val_loss: 2.6306 - val_acc: 0.0960\n",
      "Epoch 27/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.7729 - acc: 0.3905 - val_loss: 2.6537 - val_acc: 0.1015\n",
      "Epoch 28/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.7469 - acc: 0.3905 - val_loss: 2.6584 - val_acc: 0.1015\n",
      "Epoch 29/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 1.7168 - acc: 0.4045 - val_loss: 2.6928 - val_acc: 0.1060\n",
      "Epoch 30/200\n",
      "2000/2000 [==============================] - 0s 47us/step - loss: 1.6928 - acc: 0.4085 - val_loss: 2.7297 - val_acc: 0.1035\n",
      "Epoch 31/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 1.6669 - acc: 0.4145 - val_loss: 2.7719 - val_acc: 0.1040\n",
      "Epoch 32/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 1.6328 - acc: 0.4335 - val_loss: 2.7852 - val_acc: 0.1060\n",
      "Epoch 33/200\n",
      "2000/2000 [==============================] - 0s 23us/step - loss: 1.6094 - acc: 0.4525 - val_loss: 2.8568 - val_acc: 0.0980\n",
      "Epoch 34/200\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 1.5864 - acc: 0.4555 - val_loss: 2.8661 - val_acc: 0.1005\n",
      "Epoch 35/200\n",
      "2000/2000 [==============================] - 0s 32us/step - loss: 1.5802 - acc: 0.4600 - val_loss: 2.9601 - val_acc: 0.0950\n",
      "Epoch 36/200\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 1.5576 - acc: 0.4655 - val_loss: 2.9558 - val_acc: 0.0975\n",
      "Epoch 37/200\n",
      "2000/2000 [==============================] - 0s 26us/step - loss: 1.5255 - acc: 0.4815 - val_loss: 2.9826 - val_acc: 0.0995\n",
      "Epoch 38/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.4985 - acc: 0.4905 - val_loss: 3.0081 - val_acc: 0.1035\n",
      "Epoch 39/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.4643 - acc: 0.5160 - val_loss: 3.0779 - val_acc: 0.1030\n",
      "Epoch 40/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.4391 - acc: 0.5190 - val_loss: 3.1073 - val_acc: 0.1025\n",
      "Epoch 41/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.4329 - acc: 0.5120 - val_loss: 3.0994 - val_acc: 0.1025\n",
      "Epoch 42/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.4135 - acc: 0.5165 - val_loss: 3.1847 - val_acc: 0.1050\n",
      "Epoch 43/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.3787 - acc: 0.5460 - val_loss: 3.2457 - val_acc: 0.0975\n",
      "Epoch 44/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.3553 - acc: 0.5465 - val_loss: 3.2352 - val_acc: 0.0960\n",
      "Epoch 45/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.3306 - acc: 0.5645 - val_loss: 3.2642 - val_acc: 0.0970\n",
      "Epoch 46/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.3131 - acc: 0.5670 - val_loss: 3.3224 - val_acc: 0.0950\n",
      "Epoch 47/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.2934 - acc: 0.5680 - val_loss: 3.3740 - val_acc: 0.1025\n",
      "Epoch 48/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.2782 - acc: 0.5795 - val_loss: 3.4752 - val_acc: 0.1005\n",
      "Epoch 49/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.2776 - acc: 0.5765 - val_loss: 3.4650 - val_acc: 0.0980\n",
      "Epoch 50/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.2316 - acc: 0.5870 - val_loss: 3.5201 - val_acc: 0.1005\n",
      "Epoch 51/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.2184 - acc: 0.5845 - val_loss: 3.5122 - val_acc: 0.0955\n",
      "Epoch 52/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.1903 - acc: 0.6060 - val_loss: 3.5742 - val_acc: 0.1030\n",
      "Epoch 53/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.1671 - acc: 0.6155 - val_loss: 3.6438 - val_acc: 0.1020\n",
      "Epoch 54/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.1532 - acc: 0.6215 - val_loss: 3.5924 - val_acc: 0.1010\n",
      "Epoch 55/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.1327 - acc: 0.6270 - val_loss: 3.6920 - val_acc: 0.1010\n",
      "Epoch 56/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.1059 - acc: 0.6370 - val_loss: 3.7544 - val_acc: 0.0985\n",
      "Epoch 57/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.0850 - acc: 0.6485 - val_loss: 3.7847 - val_acc: 0.0995\n",
      "Epoch 58/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.0732 - acc: 0.6505 - val_loss: 3.8329 - val_acc: 0.0935\n",
      "Epoch 59/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.0641 - acc: 0.6555 - val_loss: 3.8712 - val_acc: 0.1070\n",
      "Epoch 60/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.0378 - acc: 0.6565 - val_loss: 3.9108 - val_acc: 0.0980\n",
      "Epoch 61/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 1.0225 - acc: 0.6645 - val_loss: 3.9535 - val_acc: 0.1000\n",
      "Epoch 62/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 1.0057 - acc: 0.6840 - val_loss: 4.0166 - val_acc: 0.0975\n",
      "Epoch 63/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.9909 - acc: 0.6745 - val_loss: 4.0715 - val_acc: 0.1020\n",
      "Epoch 64/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.9863 - acc: 0.6885 - val_loss: 4.1082 - val_acc: 0.1025\n",
      "Epoch 65/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9659 - acc: 0.7040 - val_loss: 4.1358 - val_acc: 0.1030\n",
      "Epoch 66/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9362 - acc: 0.6955 - val_loss: 4.1993 - val_acc: 0.0990\n",
      "Epoch 67/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.9245 - acc: 0.7090 - val_loss: 4.2523 - val_acc: 0.1045\n",
      "Epoch 68/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.9123 - acc: 0.7050 - val_loss: 4.2821 - val_acc: 0.0955\n",
      "Epoch 69/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.9007 - acc: 0.7125 - val_loss: 4.3856 - val_acc: 0.1060\n",
      "Epoch 70/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8785 - acc: 0.7300 - val_loss: 4.4221 - val_acc: 0.1010\n",
      "Epoch 71/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.8621 - acc: 0.7355 - val_loss: 4.4494 - val_acc: 0.1010\n",
      "Epoch 72/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.8679 - acc: 0.7220 - val_loss: 4.4972 - val_acc: 0.1020\n",
      "Epoch 73/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8478 - acc: 0.7405 - val_loss: 4.5619 - val_acc: 0.0945\n",
      "Epoch 74/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8478 - acc: 0.7330 - val_loss: 4.6253 - val_acc: 0.1010\n",
      "Epoch 75/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.8146 - acc: 0.7470 - val_loss: 4.6203 - val_acc: 0.1045\n",
      "Epoch 76/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7916 - acc: 0.7560 - val_loss: 4.7248 - val_acc: 0.1080\n",
      "Epoch 77/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7954 - acc: 0.7565 - val_loss: 4.7530 - val_acc: 0.1060\n",
      "Epoch 78/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7785 - acc: 0.7670 - val_loss: 4.7502 - val_acc: 0.1005\n",
      "Epoch 79/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7617 - acc: 0.7700 - val_loss: 4.8720 - val_acc: 0.0960\n",
      "Epoch 80/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7487 - acc: 0.7635 - val_loss: 4.8901 - val_acc: 0.1015\n",
      "Epoch 81/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7402 - acc: 0.7725 - val_loss: 5.0030 - val_acc: 0.0950\n",
      "Epoch 82/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7289 - acc: 0.7845 - val_loss: 4.9917 - val_acc: 0.1045\n",
      "Epoch 83/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.7252 - acc: 0.7780 - val_loss: 5.0445 - val_acc: 0.0980\n",
      "Epoch 84/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.7063 - acc: 0.7840 - val_loss: 5.1620 - val_acc: 0.1075\n",
      "Epoch 85/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6875 - acc: 0.7920 - val_loss: 5.2147 - val_acc: 0.1025\n",
      "Epoch 86/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6719 - acc: 0.8055 - val_loss: 5.1968 - val_acc: 0.1050\n",
      "Epoch 87/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6546 - acc: 0.8130 - val_loss: 5.2489 - val_acc: 0.1005\n",
      "Epoch 88/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6438 - acc: 0.8110 - val_loss: 5.2533 - val_acc: 0.1035\n",
      "Epoch 89/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6447 - acc: 0.8175 - val_loss: 5.3810 - val_acc: 0.0980\n",
      "Epoch 90/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6351 - acc: 0.8110 - val_loss: 5.4376 - val_acc: 0.1015\n",
      "Epoch 91/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.6281 - acc: 0.8145 - val_loss: 5.4566 - val_acc: 0.1005\n",
      "Epoch 92/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6063 - acc: 0.8215 - val_loss: 5.5489 - val_acc: 0.1025\n",
      "Epoch 93/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.6058 - acc: 0.8240 - val_loss: 5.5202 - val_acc: 0.1005\n",
      "Epoch 94/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5930 - acc: 0.8355 - val_loss: 5.5582 - val_acc: 0.0980\n",
      "Epoch 95/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5842 - acc: 0.8365 - val_loss: 5.6460 - val_acc: 0.1025\n",
      "Epoch 96/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5816 - acc: 0.8330 - val_loss: 5.6602 - val_acc: 0.1045\n",
      "Epoch 97/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.5780 - acc: 0.8340 - val_loss: 5.7984 - val_acc: 0.1040\n",
      "Epoch 98/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5599 - acc: 0.8430 - val_loss: 5.7506 - val_acc: 0.1020\n",
      "Epoch 99/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5610 - acc: 0.8390 - val_loss: 5.8243 - val_acc: 0.1035\n",
      "Epoch 100/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5409 - acc: 0.8515 - val_loss: 5.8429 - val_acc: 0.1045\n",
      "Epoch 101/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5288 - acc: 0.8530 - val_loss: 5.8993 - val_acc: 0.1020\n",
      "Epoch 102/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5183 - acc: 0.8580 - val_loss: 6.0023 - val_acc: 0.1035\n",
      "Epoch 103/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.5035 - acc: 0.8565 - val_loss: 5.9969 - val_acc: 0.1015\n",
      "Epoch 104/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4982 - acc: 0.8675 - val_loss: 6.0738 - val_acc: 0.1060\n",
      "Epoch 105/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4989 - acc: 0.8615 - val_loss: 6.1396 - val_acc: 0.1070\n",
      "Epoch 106/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4739 - acc: 0.8790 - val_loss: 6.1798 - val_acc: 0.1020\n",
      "Epoch 107/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4714 - acc: 0.8760 - val_loss: 6.2153 - val_acc: 0.0975\n",
      "Epoch 108/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4729 - acc: 0.8720 - val_loss: 6.2735 - val_acc: 0.1045\n",
      "Epoch 109/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.4689 - acc: 0.8705 - val_loss: 6.3486 - val_acc: 0.1065\n",
      "Epoch 110/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.4587 - acc: 0.8810 - val_loss: 6.3866 - val_acc: 0.1060\n",
      "Epoch 111/200\n",
      "2000/2000 [==============================] - 0s 30us/step - loss: 0.4371 - acc: 0.8930 - val_loss: 6.3999 - val_acc: 0.1055\n",
      "Epoch 112/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.4274 - acc: 0.8970 - val_loss: 6.4197 - val_acc: 0.1025\n",
      "Epoch 113/200\n",
      "2000/2000 [==============================] - 0s 37us/step - loss: 0.4230 - acc: 0.8960 - val_loss: 6.4812 - val_acc: 0.1000\n",
      "Epoch 114/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.4251 - acc: 0.8990 - val_loss: 6.5555 - val_acc: 0.1050\n",
      "Epoch 115/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.4183 - acc: 0.8905 - val_loss: 6.5914 - val_acc: 0.1035\n",
      "Epoch 116/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.4128 - acc: 0.8990 - val_loss: 6.6769 - val_acc: 0.1025\n",
      "Epoch 117/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.4082 - acc: 0.8985 - val_loss: 6.7288 - val_acc: 0.1030\n",
      "Epoch 118/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3949 - acc: 0.9065 - val_loss: 6.7305 - val_acc: 0.1035\n",
      "Epoch 119/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3833 - acc: 0.9065 - val_loss: 6.8145 - val_acc: 0.1045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3728 - acc: 0.9150 - val_loss: 6.8466 - val_acc: 0.1025\n",
      "Epoch 121/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3708 - acc: 0.9090 - val_loss: 6.8358 - val_acc: 0.1035\n",
      "Epoch 122/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.3759 - acc: 0.9090 - val_loss: 6.9720 - val_acc: 0.1065\n",
      "Epoch 123/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3667 - acc: 0.9110 - val_loss: 6.9458 - val_acc: 0.1030\n",
      "Epoch 124/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3498 - acc: 0.9200 - val_loss: 7.0504 - val_acc: 0.1030\n",
      "Epoch 125/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.3503 - acc: 0.9180 - val_loss: 7.0002 - val_acc: 0.1030\n",
      "Epoch 126/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.3504 - acc: 0.9210 - val_loss: 7.0745 - val_acc: 0.1040\n",
      "Epoch 127/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3327 - acc: 0.9265 - val_loss: 7.1263 - val_acc: 0.1035\n",
      "Epoch 128/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3288 - acc: 0.9285 - val_loss: 7.1709 - val_acc: 0.1080\n",
      "Epoch 129/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.3158 - acc: 0.9345 - val_loss: 7.1636 - val_acc: 0.1005\n",
      "Epoch 130/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.3222 - acc: 0.9255 - val_loss: 7.2297 - val_acc: 0.1070\n",
      "Epoch 131/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.3036 - acc: 0.9370 - val_loss: 7.3209 - val_acc: 0.1070\n",
      "Epoch 132/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2985 - acc: 0.9385 - val_loss: 7.2918 - val_acc: 0.1085\n",
      "Epoch 133/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2972 - acc: 0.9375 - val_loss: 7.3317 - val_acc: 0.1070\n",
      "Epoch 134/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2939 - acc: 0.9360 - val_loss: 7.3693 - val_acc: 0.1080\n",
      "Epoch 135/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2851 - acc: 0.9385 - val_loss: 7.4307 - val_acc: 0.1050\n",
      "Epoch 136/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2841 - acc: 0.9415 - val_loss: 7.4411 - val_acc: 0.1025\n",
      "Epoch 137/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2812 - acc: 0.9450 - val_loss: 7.5489 - val_acc: 0.1025\n",
      "Epoch 138/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2707 - acc: 0.9420 - val_loss: 7.5648 - val_acc: 0.1060\n",
      "Epoch 139/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2627 - acc: 0.9485 - val_loss: 7.6848 - val_acc: 0.1040\n",
      "Epoch 140/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2676 - acc: 0.9455 - val_loss: 7.6773 - val_acc: 0.1075\n",
      "Epoch 141/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2567 - acc: 0.9505 - val_loss: 7.7318 - val_acc: 0.1055\n",
      "Epoch 142/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2509 - acc: 0.9545 - val_loss: 7.6824 - val_acc: 0.1050\n",
      "Epoch 143/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2552 - acc: 0.9535 - val_loss: 7.8243 - val_acc: 0.1035\n",
      "Epoch 144/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2455 - acc: 0.9530 - val_loss: 7.7792 - val_acc: 0.1040\n",
      "Epoch 145/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2377 - acc: 0.9530 - val_loss: 7.8022 - val_acc: 0.1060\n",
      "Epoch 146/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2269 - acc: 0.9585 - val_loss: 7.9070 - val_acc: 0.1055\n",
      "Epoch 147/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2190 - acc: 0.9630 - val_loss: 7.9624 - val_acc: 0.1045\n",
      "Epoch 148/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2200 - acc: 0.9600 - val_loss: 7.9484 - val_acc: 0.1065\n",
      "Epoch 149/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2175 - acc: 0.9605 - val_loss: 8.0370 - val_acc: 0.1055\n",
      "Epoch 150/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2202 - acc: 0.9570 - val_loss: 8.0200 - val_acc: 0.1020\n",
      "Epoch 151/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.2112 - acc: 0.9640 - val_loss: 8.0782 - val_acc: 0.1045\n",
      "Epoch 152/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2046 - acc: 0.9670 - val_loss: 8.1540 - val_acc: 0.1025\n",
      "Epoch 153/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1976 - acc: 0.9675 - val_loss: 8.1300 - val_acc: 0.1045\n",
      "Epoch 154/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1965 - acc: 0.9650 - val_loss: 8.1476 - val_acc: 0.1030\n",
      "Epoch 155/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1963 - acc: 0.9685 - val_loss: 8.1780 - val_acc: 0.0995\n",
      "Epoch 156/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.2017 - acc: 0.9685 - val_loss: 8.2589 - val_acc: 0.1055\n",
      "Epoch 157/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1924 - acc: 0.9680 - val_loss: 8.3115 - val_acc: 0.1055\n",
      "Epoch 158/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1853 - acc: 0.9670 - val_loss: 8.3874 - val_acc: 0.1045\n",
      "Epoch 159/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1876 - acc: 0.9695 - val_loss: 8.4320 - val_acc: 0.1045\n",
      "Epoch 160/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1831 - acc: 0.9700 - val_loss: 8.4183 - val_acc: 0.1055\n",
      "Epoch 161/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1817 - acc: 0.9710 - val_loss: 8.4253 - val_acc: 0.1030\n",
      "Epoch 162/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1715 - acc: 0.9765 - val_loss: 8.4888 - val_acc: 0.1015\n",
      "Epoch 163/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1672 - acc: 0.9765 - val_loss: 8.5024 - val_acc: 0.1025\n",
      "Epoch 164/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1670 - acc: 0.9750 - val_loss: 8.6183 - val_acc: 0.0995\n",
      "Epoch 165/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1648 - acc: 0.9720 - val_loss: 8.5526 - val_acc: 0.1030\n",
      "Epoch 166/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1679 - acc: 0.9735 - val_loss: 8.6170 - val_acc: 0.1010\n",
      "Epoch 167/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1889 - acc: 0.9615 - val_loss: 8.6109 - val_acc: 0.1065\n",
      "Epoch 168/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1777 - acc: 0.9735 - val_loss: 8.6427 - val_acc: 0.1015\n",
      "Epoch 169/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1634 - acc: 0.9735 - val_loss: 8.6434 - val_acc: 0.1015\n",
      "Epoch 170/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1566 - acc: 0.9750 - val_loss: 8.6407 - val_acc: 0.1035\n",
      "Epoch 171/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1517 - acc: 0.9755 - val_loss: 8.7479 - val_acc: 0.1025\n",
      "Epoch 172/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1403 - acc: 0.9805 - val_loss: 8.8180 - val_acc: 0.1005\n",
      "Epoch 173/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1374 - acc: 0.9800 - val_loss: 8.8071 - val_acc: 0.0990\n",
      "Epoch 174/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1361 - acc: 0.9815 - val_loss: 8.8394 - val_acc: 0.0965\n",
      "Epoch 175/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1346 - acc: 0.9825 - val_loss: 8.8709 - val_acc: 0.0970\n",
      "Epoch 176/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1294 - acc: 0.9825 - val_loss: 8.8958 - val_acc: 0.0980\n",
      "Epoch 177/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1269 - acc: 0.9815 - val_loss: 8.9618 - val_acc: 0.0990\n",
      "Epoch 178/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1317 - acc: 0.9795 - val_loss: 8.9017 - val_acc: 0.0990\n",
      "Epoch 179/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1353 - acc: 0.9800 - val_loss: 8.9756 - val_acc: 0.0965\n",
      "Epoch 180/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1265 - acc: 0.9820 - val_loss: 8.9784 - val_acc: 0.0960\n",
      "Epoch 181/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1282 - acc: 0.9820 - val_loss: 8.9732 - val_acc: 0.1000\n",
      "Epoch 182/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1385 - acc: 0.9755 - val_loss: 9.0092 - val_acc: 0.0995\n",
      "Epoch 183/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1446 - acc: 0.9720 - val_loss: 9.0789 - val_acc: 0.1050\n",
      "Epoch 184/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1363 - acc: 0.9770 - val_loss: 9.0745 - val_acc: 0.1020\n",
      "Epoch 185/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1275 - acc: 0.9815 - val_loss: 9.1329 - val_acc: 0.1020\n",
      "Epoch 186/200\n",
      "2000/2000 [==============================] - 0s 20us/step - loss: 0.1190 - acc: 0.9825 - val_loss: 9.1214 - val_acc: 0.1000\n",
      "Epoch 187/200\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.1140 - acc: 0.9840 - val_loss: 9.1653 - val_acc: 0.0985\n",
      "Epoch 188/200\n",
      "2000/2000 [==============================] - 0s 25us/step - loss: 0.1091 - acc: 0.9850 - val_loss: 9.1796 - val_acc: 0.0965\n",
      "Epoch 189/200\n",
      "2000/2000 [==============================] - 0s 28us/step - loss: 0.1103 - acc: 0.9830 - val_loss: 9.2534 - val_acc: 0.0965\n",
      "Epoch 190/200\n",
      "2000/2000 [==============================] - 0s 31us/step - loss: 0.1132 - acc: 0.9830 - val_loss: 9.2972 - val_acc: 0.0895\n",
      "Epoch 191/200\n",
      "2000/2000 [==============================] - 0s 34us/step - loss: 0.1143 - acc: 0.9810 - val_loss: 9.2928 - val_acc: 0.1000\n",
      "Epoch 192/200\n",
      "2000/2000 [==============================] - 0s 29us/step - loss: 0.1095 - acc: 0.9855 - val_loss: 9.3098 - val_acc: 0.0955\n",
      "Epoch 193/200\n",
      "2000/2000 [==============================] - 0s 24us/step - loss: 0.1045 - acc: 0.9860 - val_loss: 9.3268 - val_acc: 0.0965\n",
      "Epoch 194/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.1030 - acc: 0.9845 - val_loss: 9.3613 - val_acc: 0.0980\n",
      "Epoch 195/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0970 - acc: 0.9870 - val_loss: 9.3905 - val_acc: 0.0945\n",
      "Epoch 196/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0991 - acc: 0.9860 - val_loss: 9.4010 - val_acc: 0.0970\n",
      "Epoch 197/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0914 - acc: 0.9875 - val_loss: 9.4360 - val_acc: 0.0970\n",
      "Epoch 198/200\n",
      "2000/2000 [==============================] - 0s 22us/step - loss: 0.0885 - acc: 0.9890 - val_loss: 9.4509 - val_acc: 0.0930\n",
      "Epoch 199/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0846 - acc: 0.9895 - val_loss: 9.4844 - val_acc: 0.0955\n",
      "Epoch 200/200\n",
      "2000/2000 [==============================] - 0s 21us/step - loss: 0.0825 - acc: 0.9895 - val_loss: 9.4942 - val_acc: 0.0955\n"
     ]
    }
   ],
   "source": [
    "model5.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history5 = model5.fit(x_train[:4000], y_train[:4000], batch_size=128, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:33:31.885719Z",
     "start_time": "2018-07-06T03:33:31.852673Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_loss = [\n",
    "    history1.history[\"loss\"][-1],\n",
    "    history2.history[\"loss\"][-1],\n",
    "    history3.history[\"loss\"][-1],\n",
    "    history4.history[\"loss\"][-1],\n",
    "    history5.history[\"loss\"][-1]\n",
    "]\n",
    "test_loss = [\n",
    "    history1.history[\"val_loss\"][-1],\n",
    "    history2.history[\"val_loss\"][-1],\n",
    "    history3.history[\"val_loss\"][-1],\n",
    "    history4.history[\"val_loss\"][-1],\n",
    "    history5.history[\"val_loss\"][-1]\n",
    "]\n",
    "train_acc = [\n",
    "    history1.history[\"acc\"][-1],\n",
    "    history2.history[\"acc\"][-1],\n",
    "    history3.history[\"acc\"][-1],\n",
    "    history4.history[\"acc\"][-1],\n",
    "    history5.history[\"acc\"][-1]\n",
    "]\n",
    "test_acc = [\n",
    "    history1.history[\"val_acc\"][-1],\n",
    "    history2.history[\"val_acc\"][-1],\n",
    "    history3.history[\"val_acc\"][-1],\n",
    "    history4.history[\"val_acc\"][-1],\n",
    "    history5.history[\"val_acc\"][-1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:35:29.636573Z",
     "start_time": "2018-07-06T03:35:29.634772Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_params = [\n",
    "    model1.count_params(),\n",
    "    model2.count_params(),\n",
    "    model3.count_params(),\n",
    "    model4.count_params(),\n",
    "    model5.count_params()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:37:23.354558Z",
     "start_time": "2018-07-06T03:37:23.152845Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(num_params, train_loss, label=\"train\")\n",
    "plt.scatter(num_params, test_loss, label=\"test\")\n",
    "plt.xlabel(\"number of paramaters\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Model Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:37:56.055989Z",
     "start_time": "2018-07-06T03:37:55.962914Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(num_params, train_acc, label=\"train\")\n",
    "plt.scatter(num_params, test_acc, label=\"test\")\n",
    "plt.xlabel(\"number of paramaters\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatness v.s. Generalization - part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:52:37.365455Z",
     "start_time": "2018-07-06T03:52:37.269257Z"
    }
   },
   "outputs": [],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model8.add(Dense(32, activation=\"relu\", name=\"fc1\"))\n",
    "model8.add(Dense(16, activation=\"relu\", name=\"fc2\"))\n",
    "model8.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model9 = Sequential()\n",
    "model9.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model9.add(Dense(32, activation=\"relu\", name=\"fc1\"))\n",
    "model9.add(Dense(16, activation=\"relu\", name=\"fc2\"))\n",
    "model9.add(Dense(10, activation=\"softmax\", name=\"output\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:56:57.307156Z",
     "start_time": "2018-07-06T03:56:29.138700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with batch_size=64 ...\n",
      "Train on 3000 samples, validate on 3000 samples\n",
      "Epoch 1/200\n",
      "3000/3000 [==============================] - 0s 147us/step - loss: 3.1533 - acc: 0.6670 - val_loss: 8.5492 - val_acc: 0.1113\n",
      "Epoch 2/200\n",
      "3000/3000 [==============================] - 0s 33us/step - loss: 2.7019 - acc: 0.6550 - val_loss: 8.0957 - val_acc: 0.1093\n",
      "Epoch 3/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 2.4655 - acc: 0.6543 - val_loss: 7.9116 - val_acc: 0.1070\n",
      "Epoch 4/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 2.2795 - acc: 0.6420 - val_loss: 7.7100 - val_acc: 0.1137\n",
      "Epoch 5/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 2.1087 - acc: 0.6560 - val_loss: 7.3975 - val_acc: 0.1147\n",
      "Epoch 6/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.9446 - acc: 0.6683 - val_loss: 7.2704 - val_acc: 0.1077\n",
      "Epoch 7/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.8550 - acc: 0.6640 - val_loss: 7.1437 - val_acc: 0.1173\n",
      "Epoch 8/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.7311 - acc: 0.6877 - val_loss: 7.1108 - val_acc: 0.1107\n",
      "Epoch 9/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.6732 - acc: 0.6923 - val_loss: 6.9239 - val_acc: 0.1137\n",
      "Epoch 10/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.5885 - acc: 0.6937 - val_loss: 6.9299 - val_acc: 0.1117\n",
      "Epoch 11/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.4997 - acc: 0.7083 - val_loss: 6.7874 - val_acc: 0.1117\n",
      "Epoch 12/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.4554 - acc: 0.7050 - val_loss: 6.7304 - val_acc: 0.1153\n",
      "Epoch 13/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.3919 - acc: 0.7213 - val_loss: 6.7203 - val_acc: 0.1117\n",
      "Epoch 14/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.3421 - acc: 0.7243 - val_loss: 6.6303 - val_acc: 0.1133\n",
      "Epoch 15/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.2927 - acc: 0.7247 - val_loss: 6.6288 - val_acc: 0.1093\n",
      "Epoch 16/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.2587 - acc: 0.7363 - val_loss: 6.5975 - val_acc: 0.1113\n",
      "Epoch 17/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.2172 - acc: 0.7407 - val_loss: 6.5981 - val_acc: 0.1117\n",
      "Epoch 18/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.1790 - acc: 0.7473 - val_loss: 6.4863 - val_acc: 0.1097\n",
      "Epoch 19/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.1449 - acc: 0.7570 - val_loss: 6.5603 - val_acc: 0.1107\n",
      "Epoch 20/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.1141 - acc: 0.7627 - val_loss: 6.5842 - val_acc: 0.1117\n",
      "Epoch 21/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 1.0797 - acc: 0.7723 - val_loss: 6.5606 - val_acc: 0.1110\n",
      "Epoch 22/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 1.0559 - acc: 0.7823 - val_loss: 6.6481 - val_acc: 0.1083\n",
      "Epoch 23/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 1.0289 - acc: 0.7797 - val_loss: 6.4841 - val_acc: 0.1070\n",
      "Epoch 24/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 1.0010 - acc: 0.7890 - val_loss: 6.4596 - val_acc: 0.1107\n",
      "Epoch 25/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.9848 - acc: 0.7873 - val_loss: 6.6281 - val_acc: 0.1077\n",
      "Epoch 26/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.9540 - acc: 0.7967 - val_loss: 6.5265 - val_acc: 0.1087\n",
      "Epoch 27/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.9335 - acc: 0.8037 - val_loss: 6.5291 - val_acc: 0.1137\n",
      "Epoch 28/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.9168 - acc: 0.8027 - val_loss: 6.5164 - val_acc: 0.1107\n",
      "Epoch 29/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.8979 - acc: 0.8183 - val_loss: 6.5940 - val_acc: 0.1100\n",
      "Epoch 30/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.8847 - acc: 0.8237 - val_loss: 6.5834 - val_acc: 0.1080\n",
      "Epoch 31/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.8553 - acc: 0.8217 - val_loss: 6.6458 - val_acc: 0.1117\n",
      "Epoch 32/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.8494 - acc: 0.8333 - val_loss: 6.4964 - val_acc: 0.1093\n",
      "Epoch 33/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.8277 - acc: 0.8303 - val_loss: 6.6113 - val_acc: 0.1117\n",
      "Epoch 34/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.8158 - acc: 0.8373 - val_loss: 6.6056 - val_acc: 0.1100\n",
      "Epoch 35/200\n",
      "3000/3000 [==============================] - 0s 35us/step - loss: 0.8015 - acc: 0.8333 - val_loss: 6.7098 - val_acc: 0.1103\n",
      "Epoch 36/200\n",
      "3000/3000 [==============================] - 0s 39us/step - loss: 0.7831 - acc: 0.8473 - val_loss: 6.7515 - val_acc: 0.1090\n",
      "Epoch 37/200\n",
      "3000/3000 [==============================] - 0s 46us/step - loss: 0.7702 - acc: 0.8547 - val_loss: 6.7431 - val_acc: 0.1090\n",
      "Epoch 38/200\n",
      "3000/3000 [==============================] - 0s 33us/step - loss: 0.7686 - acc: 0.8430 - val_loss: 6.6749 - val_acc: 0.1110\n",
      "Epoch 39/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.7509 - acc: 0.8503 - val_loss: 6.8016 - val_acc: 0.1120\n",
      "Epoch 40/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.7348 - acc: 0.8563 - val_loss: 6.8455 - val_acc: 0.1030\n",
      "Epoch 41/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.7239 - acc: 0.8650 - val_loss: 6.7671 - val_acc: 0.1077\n",
      "Epoch 42/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.7230 - acc: 0.8567 - val_loss: 6.8118 - val_acc: 0.1133\n",
      "Epoch 43/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.7000 - acc: 0.8667 - val_loss: 6.8567 - val_acc: 0.1103\n",
      "Epoch 44/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.6992 - acc: 0.8673 - val_loss: 6.8469 - val_acc: 0.1097\n",
      "Epoch 45/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.6805 - acc: 0.8697 - val_loss: 6.9197 - val_acc: 0.1113\n",
      "Epoch 46/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.6650 - acc: 0.8780 - val_loss: 6.9406 - val_acc: 0.1087\n",
      "Epoch 47/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.6541 - acc: 0.8807 - val_loss: 6.9488 - val_acc: 0.1083\n",
      "Epoch 48/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.6452 - acc: 0.8813 - val_loss: 6.9463 - val_acc: 0.1107\n",
      "Epoch 49/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.6282 - acc: 0.8907 - val_loss: 7.0216 - val_acc: 0.1053\n",
      "Epoch 50/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.6268 - acc: 0.8900 - val_loss: 7.0415 - val_acc: 0.1097\n",
      "Epoch 51/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.6173 - acc: 0.8863 - val_loss: 7.0513 - val_acc: 0.1107\n",
      "Epoch 52/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.6175 - acc: 0.8873 - val_loss: 7.0786 - val_acc: 0.1030\n",
      "Epoch 53/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.6003 - acc: 0.8947 - val_loss: 7.1229 - val_acc: 0.1077\n",
      "Epoch 54/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.5889 - acc: 0.8983 - val_loss: 7.0641 - val_acc: 0.1093\n",
      "Epoch 55/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5813 - acc: 0.8960 - val_loss: 7.1654 - val_acc: 0.1073\n",
      "Epoch 56/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5665 - acc: 0.9067 - val_loss: 7.2254 - val_acc: 0.1107\n",
      "Epoch 57/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.5618 - acc: 0.9063 - val_loss: 7.2384 - val_acc: 0.1087\n",
      "Epoch 58/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5559 - acc: 0.9107 - val_loss: 7.3008 - val_acc: 0.1090\n",
      "Epoch 59/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5462 - acc: 0.9113 - val_loss: 7.3383 - val_acc: 0.1090\n",
      "Epoch 60/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5334 - acc: 0.9143 - val_loss: 7.3070 - val_acc: 0.1080\n",
      "Epoch 61/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5366 - acc: 0.9127 - val_loss: 7.4442 - val_acc: 0.1087\n",
      "Epoch 62/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5293 - acc: 0.9180 - val_loss: 7.4647 - val_acc: 0.1107\n",
      "Epoch 63/200\n",
      "3000/3000 [==============================] - 0s 34us/step - loss: 0.5307 - acc: 0.9090 - val_loss: 7.4730 - val_acc: 0.1083\n",
      "Epoch 64/200\n",
      "3000/3000 [==============================] - 0s 44us/step - loss: 0.5098 - acc: 0.9257 - val_loss: 7.4621 - val_acc: 0.1090\n",
      "Epoch 65/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.5045 - acc: 0.9207 - val_loss: 7.5113 - val_acc: 0.1100\n",
      "Epoch 66/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.4933 - acc: 0.9290 - val_loss: 7.5510 - val_acc: 0.1093\n",
      "Epoch 67/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.4888 - acc: 0.9293 - val_loss: 7.5654 - val_acc: 0.1097\n",
      "Epoch 68/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4935 - acc: 0.9277 - val_loss: 7.6074 - val_acc: 0.1090\n",
      "Epoch 69/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4984 - acc: 0.9253 - val_loss: 7.7439 - val_acc: 0.1097\n",
      "Epoch 70/200\n",
      "3000/3000 [==============================] - 0s 35us/step - loss: 0.4825 - acc: 0.9327 - val_loss: 7.6441 - val_acc: 0.1093\n",
      "Epoch 71/200\n",
      "3000/3000 [==============================] - 0s 39us/step - loss: 0.4697 - acc: 0.9387 - val_loss: 7.7565 - val_acc: 0.1110\n",
      "Epoch 72/200\n",
      "3000/3000 [==============================] - 0s 44us/step - loss: 0.4564 - acc: 0.9413 - val_loss: 7.7496 - val_acc: 0.1097\n",
      "Epoch 73/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.4528 - acc: 0.9440 - val_loss: 7.7985 - val_acc: 0.1067\n",
      "Epoch 74/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.4440 - acc: 0.9410 - val_loss: 7.8685 - val_acc: 0.1113\n",
      "Epoch 75/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.4381 - acc: 0.9473 - val_loss: 7.9226 - val_acc: 0.1100\n",
      "Epoch 76/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.4434 - acc: 0.9463 - val_loss: 7.8578 - val_acc: 0.1093\n",
      "Epoch 77/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4321 - acc: 0.9480 - val_loss: 7.8963 - val_acc: 0.1070\n",
      "Epoch 78/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.4333 - acc: 0.9450 - val_loss: 7.9643 - val_acc: 0.1053\n",
      "Epoch 79/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4163 - acc: 0.9523 - val_loss: 8.0994 - val_acc: 0.1067\n",
      "Epoch 80/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.4130 - acc: 0.9537 - val_loss: 8.0079 - val_acc: 0.1107\n",
      "Epoch 81/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4127 - acc: 0.9543 - val_loss: 8.0534 - val_acc: 0.1100\n",
      "Epoch 82/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4025 - acc: 0.9547 - val_loss: 8.1469 - val_acc: 0.1137\n",
      "Epoch 83/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4039 - acc: 0.9557 - val_loss: 8.2153 - val_acc: 0.1077\n",
      "Epoch 84/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3925 - acc: 0.9573 - val_loss: 8.1879 - val_acc: 0.1100\n",
      "Epoch 85/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3813 - acc: 0.9623 - val_loss: 8.2351 - val_acc: 0.1063\n",
      "Epoch 86/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3738 - acc: 0.9653 - val_loss: 8.2544 - val_acc: 0.1097\n",
      "Epoch 87/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3784 - acc: 0.9643 - val_loss: 8.2642 - val_acc: 0.1113\n",
      "Epoch 88/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3852 - acc: 0.9620 - val_loss: 8.2646 - val_acc: 0.1130\n",
      "Epoch 89/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3805 - acc: 0.9617 - val_loss: 8.3623 - val_acc: 0.1127\n",
      "Epoch 90/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3703 - acc: 0.9657 - val_loss: 8.3494 - val_acc: 0.1123\n",
      "Epoch 91/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3670 - acc: 0.9647 - val_loss: 8.4195 - val_acc: 0.1097\n",
      "Epoch 92/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3661 - acc: 0.9670 - val_loss: 8.4396 - val_acc: 0.1093\n",
      "Epoch 93/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3539 - acc: 0.9690 - val_loss: 8.4909 - val_acc: 0.1070\n",
      "Epoch 94/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3494 - acc: 0.9680 - val_loss: 8.5291 - val_acc: 0.1087\n",
      "Epoch 95/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3544 - acc: 0.9693 - val_loss: 8.5934 - val_acc: 0.1120\n",
      "Epoch 96/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3472 - acc: 0.9697 - val_loss: 8.5670 - val_acc: 0.1087\n",
      "Epoch 97/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3448 - acc: 0.9707 - val_loss: 8.6448 - val_acc: 0.1137\n",
      "Epoch 98/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3405 - acc: 0.9740 - val_loss: 8.6206 - val_acc: 0.1110\n",
      "Epoch 99/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3519 - acc: 0.9660 - val_loss: 8.6623 - val_acc: 0.1110\n",
      "Epoch 100/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.3527 - acc: 0.9670 - val_loss: 8.7366 - val_acc: 0.1083\n",
      "Epoch 101/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3437 - acc: 0.9680 - val_loss: 8.7640 - val_acc: 0.1090\n",
      "Epoch 102/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.3429 - acc: 0.9673 - val_loss: 8.7526 - val_acc: 0.1113\n",
      "Epoch 103/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.3410 - acc: 0.9657 - val_loss: 8.7564 - val_acc: 0.1107\n",
      "Epoch 104/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3374 - acc: 0.9703 - val_loss: 8.8604 - val_acc: 0.1140\n",
      "Epoch 105/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3216 - acc: 0.9750 - val_loss: 8.7877 - val_acc: 0.1107\n",
      "Epoch 106/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3209 - acc: 0.9737 - val_loss: 8.8414 - val_acc: 0.1097\n",
      "Epoch 107/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3125 - acc: 0.9757 - val_loss: 8.8238 - val_acc: 0.1100\n",
      "Epoch 108/200\n",
      "3000/3000 [==============================] - 0s 35us/step - loss: 0.3114 - acc: 0.9753 - val_loss: 8.9229 - val_acc: 0.1100\n",
      "Epoch 109/200\n",
      "3000/3000 [==============================] - 0s 38us/step - loss: 0.3062 - acc: 0.9763 - val_loss: 8.9616 - val_acc: 0.1110\n",
      "Epoch 110/200\n",
      "3000/3000 [==============================] - 0s 45us/step - loss: 0.3017 - acc: 0.9787 - val_loss: 9.0031 - val_acc: 0.1110\n",
      "Epoch 111/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.3053 - acc: 0.9763 - val_loss: 9.0183 - val_acc: 0.1110\n",
      "Epoch 112/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.3078 - acc: 0.9770 - val_loss: 9.1164 - val_acc: 0.1087\n",
      "Epoch 113/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.3003 - acc: 0.9783 - val_loss: 9.0690 - val_acc: 0.1100\n",
      "Epoch 114/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2943 - acc: 0.9783 - val_loss: 9.1129 - val_acc: 0.1083\n",
      "Epoch 115/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2930 - acc: 0.9797 - val_loss: 9.1178 - val_acc: 0.1103\n",
      "Epoch 116/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2959 - acc: 0.9763 - val_loss: 9.1850 - val_acc: 0.1113\n",
      "Epoch 117/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2946 - acc: 0.9777 - val_loss: 9.1617 - val_acc: 0.1113\n",
      "Epoch 118/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2932 - acc: 0.9797 - val_loss: 9.2165 - val_acc: 0.1090\n",
      "Epoch 119/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2847 - acc: 0.9797 - val_loss: 9.2202 - val_acc: 0.1113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2832 - acc: 0.9793 - val_loss: 9.2038 - val_acc: 0.1093\n",
      "Epoch 121/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2854 - acc: 0.9803 - val_loss: 9.3251 - val_acc: 0.1103\n",
      "Epoch 122/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2812 - acc: 0.9793 - val_loss: 9.3152 - val_acc: 0.1120\n",
      "Epoch 123/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2886 - acc: 0.9780 - val_loss: 9.3513 - val_acc: 0.1100\n",
      "Epoch 124/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3373 - acc: 0.9593 - val_loss: 9.4756 - val_acc: 0.1027\n",
      "Epoch 125/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.4678 - acc: 0.9063 - val_loss: 9.3386 - val_acc: 0.1143\n",
      "Epoch 126/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.6206 - acc: 0.8497 - val_loss: 9.1919 - val_acc: 0.1110\n",
      "Epoch 127/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.4314 - acc: 0.9167 - val_loss: 9.3975 - val_acc: 0.1080\n",
      "Epoch 128/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3266 - acc: 0.9653 - val_loss: 9.3065 - val_acc: 0.1110\n",
      "Epoch 129/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.2897 - acc: 0.9803 - val_loss: 9.4034 - val_acc: 0.1100\n",
      "Epoch 130/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2740 - acc: 0.9820 - val_loss: 9.3996 - val_acc: 0.1117\n",
      "Epoch 131/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2698 - acc: 0.9823 - val_loss: 9.4447 - val_acc: 0.1113\n",
      "Epoch 132/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2664 - acc: 0.9837 - val_loss: 9.4517 - val_acc: 0.1137\n",
      "Epoch 133/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2633 - acc: 0.9847 - val_loss: 9.4923 - val_acc: 0.1100\n",
      "Epoch 134/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2629 - acc: 0.9827 - val_loss: 9.5242 - val_acc: 0.1103\n",
      "Epoch 135/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.2591 - acc: 0.9843 - val_loss: 9.5119 - val_acc: 0.1123\n",
      "Epoch 136/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2579 - acc: 0.9847 - val_loss: 9.5985 - val_acc: 0.1103\n",
      "Epoch 137/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2577 - acc: 0.9847 - val_loss: 9.5801 - val_acc: 0.1093\n",
      "Epoch 138/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2601 - acc: 0.9837 - val_loss: 9.5967 - val_acc: 0.1120\n",
      "Epoch 139/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.2579 - acc: 0.9840 - val_loss: 9.6263 - val_acc: 0.1110\n",
      "Epoch 140/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2574 - acc: 0.9847 - val_loss: 9.6240 - val_acc: 0.1113\n",
      "Epoch 141/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.2564 - acc: 0.9847 - val_loss: 9.6301 - val_acc: 0.1120\n",
      "Epoch 142/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2526 - acc: 0.9843 - val_loss: 9.6344 - val_acc: 0.1133\n",
      "Epoch 143/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2524 - acc: 0.9843 - val_loss: 9.7186 - val_acc: 0.1110\n",
      "Epoch 144/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2510 - acc: 0.9853 - val_loss: 9.7023 - val_acc: 0.1107\n",
      "Epoch 145/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2498 - acc: 0.9853 - val_loss: 9.7140 - val_acc: 0.1110\n",
      "Epoch 146/200\n",
      "3000/3000 [==============================] - 0s 34us/step - loss: 0.2495 - acc: 0.9857 - val_loss: 9.7490 - val_acc: 0.1103\n",
      "Epoch 147/200\n",
      "3000/3000 [==============================] - 0s 36us/step - loss: 0.2485 - acc: 0.9850 - val_loss: 9.7342 - val_acc: 0.1100\n",
      "Epoch 148/200\n",
      "3000/3000 [==============================] - 0s 45us/step - loss: 0.2473 - acc: 0.9857 - val_loss: 9.7488 - val_acc: 0.1113\n",
      "Epoch 149/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.2493 - acc: 0.9853 - val_loss: 9.7611 - val_acc: 0.1123\n",
      "Epoch 150/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2488 - acc: 0.9847 - val_loss: 9.8276 - val_acc: 0.1083\n",
      "Epoch 151/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2482 - acc: 0.9853 - val_loss: 9.8194 - val_acc: 0.1097\n",
      "Epoch 152/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2461 - acc: 0.9850 - val_loss: 9.8402 - val_acc: 0.1083\n",
      "Epoch 153/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2460 - acc: 0.9853 - val_loss: 9.8403 - val_acc: 0.1087\n",
      "Epoch 154/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.2447 - acc: 0.9853 - val_loss: 9.8737 - val_acc: 0.1083\n",
      "Epoch 155/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2449 - acc: 0.9850 - val_loss: 9.8811 - val_acc: 0.1067\n",
      "Epoch 156/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2428 - acc: 0.9857 - val_loss: 9.8883 - val_acc: 0.1090\n",
      "Epoch 157/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2411 - acc: 0.9857 - val_loss: 9.9135 - val_acc: 0.1107\n",
      "Epoch 158/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2411 - acc: 0.9857 - val_loss: 9.9481 - val_acc: 0.1107\n",
      "Epoch 159/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2391 - acc: 0.9860 - val_loss: 9.9713 - val_acc: 0.1087\n",
      "Epoch 160/200\n",
      "3000/3000 [==============================] - 0s 27us/step - loss: 0.2401 - acc: 0.9857 - val_loss: 9.9479 - val_acc: 0.1113\n",
      "Epoch 161/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2409 - acc: 0.9853 - val_loss: 10.0215 - val_acc: 0.1103\n",
      "Epoch 162/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2407 - acc: 0.9847 - val_loss: 9.9833 - val_acc: 0.1093\n",
      "Epoch 163/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2386 - acc: 0.9860 - val_loss: 10.0379 - val_acc: 0.1067\n",
      "Epoch 164/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2399 - acc: 0.9857 - val_loss: 10.0527 - val_acc: 0.1073\n",
      "Epoch 165/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2376 - acc: 0.9860 - val_loss: 10.0259 - val_acc: 0.1057\n",
      "Epoch 166/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2364 - acc: 0.9857 - val_loss: 10.0890 - val_acc: 0.1103\n",
      "Epoch 167/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2360 - acc: 0.9857 - val_loss: 10.0980 - val_acc: 0.1100\n",
      "Epoch 168/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2348 - acc: 0.9863 - val_loss: 10.1099 - val_acc: 0.1113\n",
      "Epoch 169/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2335 - acc: 0.9863 - val_loss: 10.1795 - val_acc: 0.1100\n",
      "Epoch 170/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2353 - acc: 0.9860 - val_loss: 10.1756 - val_acc: 0.1110\n",
      "Epoch 171/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2360 - acc: 0.9860 - val_loss: 10.1921 - val_acc: 0.1120\n",
      "Epoch 172/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2344 - acc: 0.9857 - val_loss: 10.1654 - val_acc: 0.1103\n",
      "Epoch 173/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2361 - acc: 0.9857 - val_loss: 10.2025 - val_acc: 0.1113\n",
      "Epoch 174/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2339 - acc: 0.9853 - val_loss: 10.2071 - val_acc: 0.1117\n",
      "Epoch 175/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2344 - acc: 0.9857 - val_loss: 10.2814 - val_acc: 0.1147\n",
      "Epoch 176/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2345 - acc: 0.9857 - val_loss: 10.2618 - val_acc: 0.1103\n",
      "Epoch 177/200\n",
      "3000/3000 [==============================] - 0s 49us/step - loss: 0.2367 - acc: 0.9853 - val_loss: 10.2863 - val_acc: 0.1107\n",
      "Epoch 178/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3575 - acc: 0.9487 - val_loss: 10.5360 - val_acc: 0.1093\n",
      "Epoch 179/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 1.1902 - acc: 0.7367 - val_loss: 10.3237 - val_acc: 0.1087\n",
      "Epoch 180/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.5974 - acc: 0.8653 - val_loss: 10.0993 - val_acc: 0.1100\n",
      "Epoch 181/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.3693 - acc: 0.9377 - val_loss: 10.1911 - val_acc: 0.1090\n",
      "Epoch 182/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2764 - acc: 0.9770 - val_loss: 10.1991 - val_acc: 0.1090\n",
      "Epoch 183/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2473 - acc: 0.9850 - val_loss: 10.1884 - val_acc: 0.1093\n",
      "Epoch 184/200\n",
      "3000/3000 [==============================] - 0s 37us/step - loss: 0.2370 - acc: 0.9857 - val_loss: 10.1948 - val_acc: 0.1117\n",
      "Epoch 185/200\n",
      "3000/3000 [==============================] - 0s 44us/step - loss: 0.2341 - acc: 0.9860 - val_loss: 10.2494 - val_acc: 0.1123\n",
      "Epoch 186/200\n",
      "3000/3000 [==============================] - 0s 38us/step - loss: 0.2315 - acc: 0.9860 - val_loss: 10.2377 - val_acc: 0.1130\n",
      "Epoch 187/200\n",
      "3000/3000 [==============================] - 0s 31us/step - loss: 0.2308 - acc: 0.9867 - val_loss: 10.2353 - val_acc: 0.1100\n",
      "Epoch 188/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.2300 - acc: 0.9863 - val_loss: 10.2445 - val_acc: 0.1113\n",
      "Epoch 189/200\n",
      "3000/3000 [==============================] - 0s 30us/step - loss: 0.2294 - acc: 0.9867 - val_loss: 10.2623 - val_acc: 0.1120\n",
      "Epoch 190/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2290 - acc: 0.9867 - val_loss: 10.2899 - val_acc: 0.1123\n",
      "Epoch 191/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2283 - acc: 0.9863 - val_loss: 10.2873 - val_acc: 0.1113\n",
      "Epoch 192/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2280 - acc: 0.9867 - val_loss: 10.3109 - val_acc: 0.1120\n",
      "Epoch 193/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2274 - acc: 0.9867 - val_loss: 10.3148 - val_acc: 0.1123\n",
      "Epoch 194/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2272 - acc: 0.9867 - val_loss: 10.3056 - val_acc: 0.1103\n",
      "Epoch 195/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2269 - acc: 0.9867 - val_loss: 10.3197 - val_acc: 0.1113\n",
      "Epoch 196/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2269 - acc: 0.9867 - val_loss: 10.3317 - val_acc: 0.1140\n",
      "Epoch 197/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2261 - acc: 0.9867 - val_loss: 10.3566 - val_acc: 0.1150\n",
      "Epoch 198/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2259 - acc: 0.9867 - val_loss: 10.3707 - val_acc: 0.1117\n",
      "Epoch 199/200\n",
      "3000/3000 [==============================] - 0s 28us/step - loss: 0.2255 - acc: 0.9867 - val_loss: 10.3715 - val_acc: 0.1117\n",
      "Epoch 200/200\n",
      "3000/3000 [==============================] - 0s 29us/step - loss: 0.2253 - acc: 0.9867 - val_loss: 10.3687 - val_acc: 0.1107\n",
      "Training model with batch_size=1024 ...\n",
      "Train on 3000 samples, validate on 3000 samples\n",
      "Epoch 1/200\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 2.3390 - acc: 0.0937 - val_loss: 2.3191 - val_acc: 0.0920\n",
      "Epoch 2/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.3092 - acc: 0.1037 - val_loss: 2.3113 - val_acc: 0.1057\n",
      "Epoch 3/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.3025 - acc: 0.1083 - val_loss: 2.3081 - val_acc: 0.1033\n",
      "Epoch 4/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2992 - acc: 0.1180 - val_loss: 2.3062 - val_acc: 0.1047\n",
      "Epoch 5/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2963 - acc: 0.1257 - val_loss: 2.3056 - val_acc: 0.1023\n",
      "Epoch 6/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2938 - acc: 0.1287 - val_loss: 2.3058 - val_acc: 0.0993\n",
      "Epoch 7/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2910 - acc: 0.1313 - val_loss: 2.3064 - val_acc: 0.0967\n",
      "Epoch 8/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2878 - acc: 0.1360 - val_loss: 2.3072 - val_acc: 0.0963\n",
      "Epoch 9/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2848 - acc: 0.1407 - val_loss: 2.3082 - val_acc: 0.0977\n",
      "Epoch 10/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2818 - acc: 0.1433 - val_loss: 2.3094 - val_acc: 0.1020\n",
      "Epoch 11/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2785 - acc: 0.1470 - val_loss: 2.3102 - val_acc: 0.1040\n",
      "Epoch 12/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2753 - acc: 0.1503 - val_loss: 2.3105 - val_acc: 0.1040\n",
      "Epoch 13/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2717 - acc: 0.1527 - val_loss: 2.3111 - val_acc: 0.1050\n",
      "Epoch 14/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2683 - acc: 0.1577 - val_loss: 2.3121 - val_acc: 0.1057\n",
      "Epoch 15/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2642 - acc: 0.1623 - val_loss: 2.3133 - val_acc: 0.1063\n",
      "Epoch 16/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2606 - acc: 0.1613 - val_loss: 2.3148 - val_acc: 0.1077\n",
      "Epoch 17/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2564 - acc: 0.1650 - val_loss: 2.3160 - val_acc: 0.1007\n",
      "Epoch 18/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2522 - acc: 0.1687 - val_loss: 2.3171 - val_acc: 0.1017\n",
      "Epoch 19/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2479 - acc: 0.1710 - val_loss: 2.3188 - val_acc: 0.1027\n",
      "Epoch 20/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2436 - acc: 0.1723 - val_loss: 2.3200 - val_acc: 0.1023\n",
      "Epoch 21/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2389 - acc: 0.1777 - val_loss: 2.3207 - val_acc: 0.1017\n",
      "Epoch 22/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2340 - acc: 0.1840 - val_loss: 2.3217 - val_acc: 0.1060\n",
      "Epoch 23/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2287 - acc: 0.1867 - val_loss: 2.3234 - val_acc: 0.1043\n",
      "Epoch 24/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2239 - acc: 0.1887 - val_loss: 2.3253 - val_acc: 0.1053\n",
      "Epoch 25/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2180 - acc: 0.1967 - val_loss: 2.3255 - val_acc: 0.1060\n",
      "Epoch 26/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2126 - acc: 0.1980 - val_loss: 2.3269 - val_acc: 0.1053\n",
      "Epoch 27/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.2068 - acc: 0.2000 - val_loss: 2.3297 - val_acc: 0.1080\n",
      "Epoch 28/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.2014 - acc: 0.2020 - val_loss: 2.3315 - val_acc: 0.1083\n",
      "Epoch 29/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.1951 - acc: 0.2050 - val_loss: 2.3339 - val_acc: 0.1070\n",
      "Epoch 30/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.1884 - acc: 0.2093 - val_loss: 2.3348 - val_acc: 0.1057\n",
      "Epoch 31/200\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 2.1823 - acc: 0.2153 - val_loss: 2.3376 - val_acc: 0.1070\n",
      "Epoch 32/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.1752 - acc: 0.2170 - val_loss: 2.3401 - val_acc: 0.1060\n",
      "Epoch 33/200\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 2.1687 - acc: 0.2200 - val_loss: 2.3425 - val_acc: 0.1050\n",
      "Epoch 34/200\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 2.1618 - acc: 0.2280 - val_loss: 2.3434 - val_acc: 0.1073\n",
      "Epoch 35/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 2.1553 - acc: 0.2347 - val_loss: 2.3467 - val_acc: 0.1073\n",
      "Epoch 36/200\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 2.1484 - acc: 0.2390 - val_loss: 2.3488 - val_acc: 0.1067\n",
      "Epoch 37/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 2.1414 - acc: 0.2427 - val_loss: 2.3522 - val_acc: 0.1087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 2.1346 - acc: 0.2433 - val_loss: 2.3549 - val_acc: 0.1073\n",
      "Epoch 39/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.1278 - acc: 0.2543 - val_loss: 2.3570 - val_acc: 0.1063\n",
      "Epoch 40/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.1203 - acc: 0.2543 - val_loss: 2.3626 - val_acc: 0.1090\n",
      "Epoch 41/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.1128 - acc: 0.2543 - val_loss: 2.3616 - val_acc: 0.1053\n",
      "Epoch 42/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.1059 - acc: 0.2687 - val_loss: 2.3663 - val_acc: 0.1063\n",
      "Epoch 43/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0987 - acc: 0.2630 - val_loss: 2.3703 - val_acc: 0.1080\n",
      "Epoch 44/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.0906 - acc: 0.2693 - val_loss: 2.3729 - val_acc: 0.1053\n",
      "Epoch 45/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0836 - acc: 0.2773 - val_loss: 2.3763 - val_acc: 0.1113\n",
      "Epoch 46/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.0757 - acc: 0.2763 - val_loss: 2.3793 - val_acc: 0.1063\n",
      "Epoch 47/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0676 - acc: 0.2873 - val_loss: 2.3828 - val_acc: 0.1107\n",
      "Epoch 48/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.0599 - acc: 0.2910 - val_loss: 2.3852 - val_acc: 0.1080\n",
      "Epoch 49/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0523 - acc: 0.2883 - val_loss: 2.3900 - val_acc: 0.1050\n",
      "Epoch 50/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.0457 - acc: 0.2913 - val_loss: 2.3927 - val_acc: 0.1077\n",
      "Epoch 51/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0370 - acc: 0.3013 - val_loss: 2.3972 - val_acc: 0.1077\n",
      "Epoch 52/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.0283 - acc: 0.3053 - val_loss: 2.4011 - val_acc: 0.1060\n",
      "Epoch 53/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0210 - acc: 0.3080 - val_loss: 2.4054 - val_acc: 0.1070\n",
      "Epoch 54/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 2.0118 - acc: 0.3147 - val_loss: 2.4104 - val_acc: 0.1060\n",
      "Epoch 55/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 2.0038 - acc: 0.3153 - val_loss: 2.4131 - val_acc: 0.1080\n",
      "Epoch 56/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.9962 - acc: 0.3233 - val_loss: 2.4171 - val_acc: 0.1027\n",
      "Epoch 57/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.9865 - acc: 0.3270 - val_loss: 2.4223 - val_acc: 0.1033\n",
      "Epoch 58/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.9787 - acc: 0.3313 - val_loss: 2.4248 - val_acc: 0.1033\n",
      "Epoch 59/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.9704 - acc: 0.3347 - val_loss: 2.4311 - val_acc: 0.1023\n",
      "Epoch 60/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.9619 - acc: 0.3427 - val_loss: 2.4343 - val_acc: 0.1050\n",
      "Epoch 61/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.9531 - acc: 0.3497 - val_loss: 2.4381 - val_acc: 0.1027\n",
      "Epoch 62/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.9457 - acc: 0.3483 - val_loss: 2.4453 - val_acc: 0.1040\n",
      "Epoch 63/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.9374 - acc: 0.3447 - val_loss: 2.4493 - val_acc: 0.1057\n",
      "Epoch 64/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.9282 - acc: 0.3547 - val_loss: 2.4535 - val_acc: 0.1003\n",
      "Epoch 65/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.9205 - acc: 0.3560 - val_loss: 2.4603 - val_acc: 0.1017\n",
      "Epoch 66/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.9118 - acc: 0.3570 - val_loss: 2.4650 - val_acc: 0.1023\n",
      "Epoch 67/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.9021 - acc: 0.3677 - val_loss: 2.4704 - val_acc: 0.1030\n",
      "Epoch 68/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.8940 - acc: 0.3673 - val_loss: 2.4734 - val_acc: 0.1037\n",
      "Epoch 69/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.8843 - acc: 0.3723 - val_loss: 2.4806 - val_acc: 0.1043\n",
      "Epoch 70/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.8761 - acc: 0.3777 - val_loss: 2.4880 - val_acc: 0.1027\n",
      "Epoch 71/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.8675 - acc: 0.3790 - val_loss: 2.4926 - val_acc: 0.0993\n",
      "Epoch 72/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.8585 - acc: 0.3850 - val_loss: 2.4977 - val_acc: 0.1060\n",
      "Epoch 73/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.8502 - acc: 0.3847 - val_loss: 2.5076 - val_acc: 0.1070\n",
      "Epoch 74/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.8399 - acc: 0.3903 - val_loss: 2.5117 - val_acc: 0.1027\n",
      "Epoch 75/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.8321 - acc: 0.3967 - val_loss: 2.5154 - val_acc: 0.1067\n",
      "Epoch 76/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.8228 - acc: 0.3940 - val_loss: 2.5251 - val_acc: 0.1040\n",
      "Epoch 77/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.8135 - acc: 0.4000 - val_loss: 2.5265 - val_acc: 0.1040\n",
      "Epoch 78/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.8054 - acc: 0.4027 - val_loss: 2.5398 - val_acc: 0.1053\n",
      "Epoch 79/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.7968 - acc: 0.4000 - val_loss: 2.5421 - val_acc: 0.1060\n",
      "Epoch 80/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.7880 - acc: 0.4103 - val_loss: 2.5477 - val_acc: 0.1063\n",
      "Epoch 81/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.7813 - acc: 0.4060 - val_loss: 2.5641 - val_acc: 0.1047\n",
      "Epoch 82/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.7715 - acc: 0.4080 - val_loss: 2.5614 - val_acc: 0.1050\n",
      "Epoch 83/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.7640 - acc: 0.4170 - val_loss: 2.5743 - val_acc: 0.1073\n",
      "Epoch 84/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.7559 - acc: 0.4143 - val_loss: 2.5795 - val_acc: 0.1077\n",
      "Epoch 85/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.7483 - acc: 0.4250 - val_loss: 2.5878 - val_acc: 0.1053\n",
      "Epoch 86/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.7379 - acc: 0.4257 - val_loss: 2.5901 - val_acc: 0.1063\n",
      "Epoch 87/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.7291 - acc: 0.4310 - val_loss: 2.5957 - val_acc: 0.1063\n",
      "Epoch 88/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.7224 - acc: 0.4310 - val_loss: 2.6070 - val_acc: 0.1107\n",
      "Epoch 89/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.7131 - acc: 0.4350 - val_loss: 2.6144 - val_acc: 0.1080\n",
      "Epoch 90/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.7040 - acc: 0.4383 - val_loss: 2.6206 - val_acc: 0.1060\n",
      "Epoch 91/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.6962 - acc: 0.4463 - val_loss: 2.6280 - val_acc: 0.1073\n",
      "Epoch 92/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.6870 - acc: 0.4463 - val_loss: 2.6331 - val_acc: 0.1110\n",
      "Epoch 93/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.6799 - acc: 0.4520 - val_loss: 2.6423 - val_acc: 0.1097\n",
      "Epoch 94/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.6715 - acc: 0.4543 - val_loss: 2.6506 - val_acc: 0.1070\n",
      "Epoch 95/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.6637 - acc: 0.4600 - val_loss: 2.6548 - val_acc: 0.1103\n",
      "Epoch 96/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.6566 - acc: 0.4597 - val_loss: 2.6642 - val_acc: 0.1110\n",
      "Epoch 97/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.6465 - acc: 0.4667 - val_loss: 2.6723 - val_acc: 0.1117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.6401 - acc: 0.4700 - val_loss: 2.6826 - val_acc: 0.1107\n",
      "Epoch 99/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.6311 - acc: 0.4680 - val_loss: 2.6909 - val_acc: 0.1117\n",
      "Epoch 100/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.6234 - acc: 0.4717 - val_loss: 2.6928 - val_acc: 0.1087\n",
      "Epoch 101/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.6162 - acc: 0.4780 - val_loss: 2.7059 - val_acc: 0.1097\n",
      "Epoch 102/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.6071 - acc: 0.4803 - val_loss: 2.7132 - val_acc: 0.1130\n",
      "Epoch 103/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.5996 - acc: 0.4787 - val_loss: 2.7184 - val_acc: 0.1160\n",
      "Epoch 104/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.5925 - acc: 0.4843 - val_loss: 2.7243 - val_acc: 0.1137\n",
      "Epoch 105/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.5845 - acc: 0.4883 - val_loss: 2.7428 - val_acc: 0.1117\n",
      "Epoch 106/200\n",
      "3000/3000 [==============================] - 0s 20us/step - loss: 1.5764 - acc: 0.4937 - val_loss: 2.7400 - val_acc: 0.1133\n",
      "Epoch 107/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.5691 - acc: 0.4983 - val_loss: 2.7533 - val_acc: 0.1127\n",
      "Epoch 108/200\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.5634 - acc: 0.5000 - val_loss: 2.7583 - val_acc: 0.1127\n",
      "Epoch 109/200\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.5582 - acc: 0.5000 - val_loss: 2.7654 - val_acc: 0.1147\n",
      "Epoch 110/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.5473 - acc: 0.5060 - val_loss: 2.7760 - val_acc: 0.1097\n",
      "Epoch 111/200\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 1.5390 - acc: 0.5040 - val_loss: 2.7879 - val_acc: 0.1130\n",
      "Epoch 112/200\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 1.5319 - acc: 0.5093 - val_loss: 2.7919 - val_acc: 0.1137\n",
      "Epoch 113/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.5256 - acc: 0.5107 - val_loss: 2.8019 - val_acc: 0.1123\n",
      "Epoch 114/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.5181 - acc: 0.5120 - val_loss: 2.8099 - val_acc: 0.1150\n",
      "Epoch 115/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.5101 - acc: 0.5203 - val_loss: 2.8172 - val_acc: 0.1133\n",
      "Epoch 116/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.5051 - acc: 0.5223 - val_loss: 2.8284 - val_acc: 0.1133\n",
      "Epoch 117/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4955 - acc: 0.5227 - val_loss: 2.8362 - val_acc: 0.1133\n",
      "Epoch 118/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.4889 - acc: 0.5220 - val_loss: 2.8464 - val_acc: 0.1137\n",
      "Epoch 119/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4820 - acc: 0.5250 - val_loss: 2.8490 - val_acc: 0.1143\n",
      "Epoch 120/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.4734 - acc: 0.5277 - val_loss: 2.8694 - val_acc: 0.1123\n",
      "Epoch 121/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4674 - acc: 0.5307 - val_loss: 2.8688 - val_acc: 0.1150\n",
      "Epoch 122/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.4601 - acc: 0.5323 - val_loss: 2.8841 - val_acc: 0.1133\n",
      "Epoch 123/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4538 - acc: 0.5373 - val_loss: 2.8892 - val_acc: 0.1117\n",
      "Epoch 124/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.4466 - acc: 0.5410 - val_loss: 2.8979 - val_acc: 0.1120\n",
      "Epoch 125/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4392 - acc: 0.5427 - val_loss: 2.9076 - val_acc: 0.1127\n",
      "Epoch 126/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.4337 - acc: 0.5443 - val_loss: 2.9201 - val_acc: 0.1117\n",
      "Epoch 127/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4274 - acc: 0.5477 - val_loss: 2.9292 - val_acc: 0.1123\n",
      "Epoch 128/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.4203 - acc: 0.5527 - val_loss: 2.9309 - val_acc: 0.1143\n",
      "Epoch 129/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.4145 - acc: 0.5493 - val_loss: 2.9443 - val_acc: 0.1113\n",
      "Epoch 130/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.4074 - acc: 0.5573 - val_loss: 2.9585 - val_acc: 0.1127\n",
      "Epoch 131/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3988 - acc: 0.5617 - val_loss: 2.9585 - val_acc: 0.1120\n",
      "Epoch 132/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3945 - acc: 0.5613 - val_loss: 2.9731 - val_acc: 0.1100\n",
      "Epoch 133/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3877 - acc: 0.5620 - val_loss: 2.9773 - val_acc: 0.1143\n",
      "Epoch 134/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3811 - acc: 0.5663 - val_loss: 2.9913 - val_acc: 0.1133\n",
      "Epoch 135/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3764 - acc: 0.5653 - val_loss: 3.0036 - val_acc: 0.1147\n",
      "Epoch 136/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3705 - acc: 0.5673 - val_loss: 3.0111 - val_acc: 0.1130\n",
      "Epoch 137/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3621 - acc: 0.5720 - val_loss: 3.0097 - val_acc: 0.1150\n",
      "Epoch 138/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3583 - acc: 0.5693 - val_loss: 3.0333 - val_acc: 0.1147\n",
      "Epoch 139/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3498 - acc: 0.5827 - val_loss: 3.0391 - val_acc: 0.1137\n",
      "Epoch 140/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3438 - acc: 0.5803 - val_loss: 3.0419 - val_acc: 0.1130\n",
      "Epoch 141/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3382 - acc: 0.5840 - val_loss: 3.0522 - val_acc: 0.1140\n",
      "Epoch 142/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3307 - acc: 0.5840 - val_loss: 3.0674 - val_acc: 0.1163\n",
      "Epoch 143/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3262 - acc: 0.5827 - val_loss: 3.0736 - val_acc: 0.1140\n",
      "Epoch 144/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.3198 - acc: 0.5887 - val_loss: 3.0805 - val_acc: 0.1157\n",
      "Epoch 145/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3135 - acc: 0.5870 - val_loss: 3.0954 - val_acc: 0.1143\n",
      "Epoch 146/200\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 1.3081 - acc: 0.5913 - val_loss: 3.1007 - val_acc: 0.1147\n",
      "Epoch 147/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.3016 - acc: 0.5927 - val_loss: 3.1137 - val_acc: 0.1167\n",
      "Epoch 148/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2957 - acc: 0.5940 - val_loss: 3.1199 - val_acc: 0.1173\n",
      "Epoch 149/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2888 - acc: 0.5973 - val_loss: 3.1272 - val_acc: 0.1130\n",
      "Epoch 150/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2843 - acc: 0.6013 - val_loss: 3.1441 - val_acc: 0.1140\n",
      "Epoch 151/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2782 - acc: 0.6020 - val_loss: 3.1465 - val_acc: 0.1150\n",
      "Epoch 152/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2728 - acc: 0.6013 - val_loss: 3.1605 - val_acc: 0.1147\n",
      "Epoch 153/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2673 - acc: 0.6023 - val_loss: 3.1745 - val_acc: 0.1147\n",
      "Epoch 154/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2594 - acc: 0.6060 - val_loss: 3.1841 - val_acc: 0.1150\n",
      "Epoch 155/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2566 - acc: 0.6120 - val_loss: 3.1861 - val_acc: 0.1137\n",
      "Epoch 156/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2505 - acc: 0.6113 - val_loss: 3.1922 - val_acc: 0.1153\n",
      "Epoch 157/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2430 - acc: 0.6130 - val_loss: 3.2109 - val_acc: 0.1140\n",
      "Epoch 158/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2384 - acc: 0.6140 - val_loss: 3.2149 - val_acc: 0.1160\n",
      "Epoch 159/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2328 - acc: 0.6150 - val_loss: 3.2329 - val_acc: 0.1137\n",
      "Epoch 160/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2280 - acc: 0.6177 - val_loss: 3.2331 - val_acc: 0.1143\n",
      "Epoch 161/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.2236 - acc: 0.6170 - val_loss: 3.2482 - val_acc: 0.1143\n",
      "Epoch 162/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2171 - acc: 0.6247 - val_loss: 3.2609 - val_acc: 0.1137\n",
      "Epoch 163/200\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 1.2110 - acc: 0.6230 - val_loss: 3.2667 - val_acc: 0.1160\n",
      "Epoch 164/200\n",
      "3000/3000 [==============================] - 0s 26us/step - loss: 1.2092 - acc: 0.6270 - val_loss: 3.2810 - val_acc: 0.1140\n",
      "Epoch 165/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.2021 - acc: 0.6273 - val_loss: 3.2776 - val_acc: 0.1180\n",
      "Epoch 166/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1977 - acc: 0.6280 - val_loss: 3.2993 - val_acc: 0.1143\n",
      "Epoch 167/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1914 - acc: 0.6340 - val_loss: 3.3042 - val_acc: 0.1140\n",
      "Epoch 168/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.1859 - acc: 0.6327 - val_loss: 3.3094 - val_acc: 0.1170\n",
      "Epoch 169/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.1831 - acc: 0.6347 - val_loss: 3.3293 - val_acc: 0.1147\n",
      "Epoch 170/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1787 - acc: 0.6317 - val_loss: 3.3305 - val_acc: 0.1143\n",
      "Epoch 171/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.1741 - acc: 0.6380 - val_loss: 3.3498 - val_acc: 0.1137\n",
      "Epoch 172/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1691 - acc: 0.6383 - val_loss: 3.3477 - val_acc: 0.1170\n",
      "Epoch 173/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.1627 - acc: 0.6387 - val_loss: 3.3703 - val_acc: 0.1163\n",
      "Epoch 174/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1567 - acc: 0.6470 - val_loss: 3.3744 - val_acc: 0.1143\n",
      "Epoch 175/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.1529 - acc: 0.6437 - val_loss: 3.3756 - val_acc: 0.1140\n",
      "Epoch 176/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1458 - acc: 0.6457 - val_loss: 3.3969 - val_acc: 0.1160\n",
      "Epoch 177/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.1410 - acc: 0.6487 - val_loss: 3.4094 - val_acc: 0.1127\n",
      "Epoch 178/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1344 - acc: 0.6523 - val_loss: 3.4146 - val_acc: 0.1157\n",
      "Epoch 179/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.1290 - acc: 0.6543 - val_loss: 3.4249 - val_acc: 0.1137\n",
      "Epoch 180/200\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.1258 - acc: 0.6570 - val_loss: 3.4414 - val_acc: 0.1137\n",
      "Epoch 181/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.1198 - acc: 0.6590 - val_loss: 3.4410 - val_acc: 0.1140\n",
      "Epoch 182/200\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 1.1139 - acc: 0.6593 - val_loss: 3.4575 - val_acc: 0.1157\n",
      "Epoch 183/200\n",
      "3000/3000 [==============================] - 0s 19us/step - loss: 1.1105 - acc: 0.6597 - val_loss: 3.4640 - val_acc: 0.1147\n",
      "Epoch 184/200\n",
      "3000/3000 [==============================] - 0s 17us/step - loss: 1.1065 - acc: 0.6617 - val_loss: 3.4832 - val_acc: 0.1143\n",
      "Epoch 185/200\n",
      "3000/3000 [==============================] - 0s 21us/step - loss: 1.1028 - acc: 0.6633 - val_loss: 3.4818 - val_acc: 0.1170\n",
      "Epoch 186/200\n",
      "3000/3000 [==============================] - 0s 18us/step - loss: 1.0957 - acc: 0.6647 - val_loss: 3.5004 - val_acc: 0.1133\n",
      "Epoch 187/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0926 - acc: 0.6640 - val_loss: 3.5022 - val_acc: 0.1170\n",
      "Epoch 188/200\n",
      "3000/3000 [==============================] - 0s 16us/step - loss: 1.0890 - acc: 0.6677 - val_loss: 3.5157 - val_acc: 0.1127\n",
      "Epoch 189/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0843 - acc: 0.6703 - val_loss: 3.5269 - val_acc: 0.1143\n",
      "Epoch 190/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0787 - acc: 0.6670 - val_loss: 3.5316 - val_acc: 0.1133\n",
      "Epoch 191/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.0741 - acc: 0.6710 - val_loss: 3.5434 - val_acc: 0.1140\n",
      "Epoch 192/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0689 - acc: 0.6720 - val_loss: 3.5636 - val_acc: 0.1127\n",
      "Epoch 193/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.0655 - acc: 0.6740 - val_loss: 3.5629 - val_acc: 0.1140\n",
      "Epoch 194/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0599 - acc: 0.6753 - val_loss: 3.5751 - val_acc: 0.1130\n",
      "Epoch 195/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.0557 - acc: 0.6777 - val_loss: 3.5855 - val_acc: 0.1130\n",
      "Epoch 196/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0529 - acc: 0.6777 - val_loss: 3.6006 - val_acc: 0.1133\n",
      "Epoch 197/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.0475 - acc: 0.6827 - val_loss: 3.6123 - val_acc: 0.1120\n",
      "Epoch 198/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0415 - acc: 0.6803 - val_loss: 3.6084 - val_acc: 0.1120\n",
      "Epoch 199/200\n",
      "3000/3000 [==============================] - 0s 15us/step - loss: 1.0389 - acc: 0.6817 - val_loss: 3.6372 - val_acc: 0.1130\n",
      "Epoch 200/200\n",
      "3000/3000 [==============================] - 0s 14us/step - loss: 1.0354 - acc: 0.6817 - val_loss: 3.6326 - val_acc: 0.1110\n"
     ]
    }
   ],
   "source": [
    "model8.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model9.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Training model with batch_size=64 ...\")\n",
    "history8 = model8.fit(x_train[:6000], y_train[:6000], batch_size=64, epochs=200, validation_split=0.5)\n",
    "print(\"Training model with batch_size=1024 ...\")\n",
    "history9 = model9.fit(x_train[:6000], y_train[:6000], batch_size=1024, epochs=200, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T03:57:42.019301Z",
     "start_time": "2018-07-06T03:57:41.974453Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = [model8.get_weights(), model9.get_weights()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Combination Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T04:42:20.208382Z",
     "start_time": "2018-07-06T04:42:20.114264Z"
    }
   },
   "outputs": [],
   "source": [
    "model10 = Sequential()\n",
    "model10.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "model10.add(Dense(32, activation=\"relu\", name=\"fc1\"))\n",
    "model10.add(Dense(16, activation=\"relu\", name=\"fc2\"))\n",
    "model10.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "model10.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-06T04:42:52.455651Z",
     "start_time": "2018-07-06T04:42:51.715323Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 5us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n",
      "3000/3000 [==============================] - 0s 6us/step\n"
     ]
    }
   ],
   "source": [
    "combine_train = np.zeros((20, 2))\n",
    "combine_test = np.zeros((20, 2))\n",
    "alphas = np.linspace(-1, 1, 20)\n",
    "new_weight = [None] * len(weights[0])\n",
    "for i in range(20):\n",
    "    for j in range(len(weights[0])):\n",
    "        new_weight[j] = (1-alphas[i]) * weights[0][j] + alphas[i] * weights[1][j]\n",
    "    model10.set_weights(new_weight)\n",
    "    combine_train[i] = model10.evaluate(x_train[:3000], y_train[:3000], batch_size=512)\n",
    "    combine_test[i] = model10.evaluate(x_test[:3000], y_test[:3000], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "290px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "164px",
    "left": "1285px",
    "right": "55px",
    "top": "-8px",
    "width": "580px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
