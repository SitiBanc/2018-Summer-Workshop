{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatness v.s. Generalization - part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T03:43:58.525088Z",
     "start_time": "2018-07-09T03:43:57.588482Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sitibanc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T03:44:01.438587Z",
     "start_time": "2018-07-09T03:44:01.074767Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# Normalize\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "# Get One-Hot Labels\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T03:44:06.297733Z",
     "start_time": "2018-07-09T03:44:06.286769Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(print_summary=False):\n",
    "    '''\n",
    "    Build model\n",
    "    Args:\n",
    "        print_summary: bool, whether or not to print model summary, default: False\n",
    "    Returns:\n",
    "        model: keras model\n",
    "    '''\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28), name=\"input\"))\n",
    "    model.add(Dense(16, activation=\"relu\", name=\"fc1\"))\n",
    "    model.add(Dense(16, activation=\"relu\", name=\"fc2\"))\n",
    "    model.add(Dense(10, activation=\"softmax\", name=\"output\"))\n",
    "\n",
    "    if print_summary:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T03:45:02.511129Z",
     "start_time": "2018-07-09T03:44:31.207727Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (Flatten)              (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 16)                12560     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 10)                170       \n",
      "=================================================================\n",
      "Total params: 13,002\n",
      "Trainable params: 13,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 0s 145us/step - loss: 1.2981 - acc: 0.6137 - val_loss: 0.8213 - val_acc: 0.7670\n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 0s 102us/step - loss: 0.5264 - acc: 0.8540 - val_loss: 0.5764 - val_acc: 0.8280\n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.3796 - acc: 0.8983 - val_loss: 0.5245 - val_acc: 0.8460\n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.3123 - acc: 0.9117 - val_loss: 0.4584 - val_acc: 0.8650\n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.2548 - acc: 0.9347 - val_loss: 0.4366 - val_acc: 0.8720\n",
      "Epoch 6/100\n",
      "3000/3000 [==============================] - 0s 102us/step - loss: 0.2253 - acc: 0.9390 - val_loss: 0.4793 - val_acc: 0.8450\n",
      "Epoch 7/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1981 - acc: 0.9507 - val_loss: 0.4459 - val_acc: 0.8660\n",
      "Epoch 8/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1754 - acc: 0.9563 - val_loss: 0.4286 - val_acc: 0.8720\n",
      "Epoch 9/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1589 - acc: 0.9553 - val_loss: 0.3949 - val_acc: 0.8750\n",
      "Epoch 10/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1430 - acc: 0.9620 - val_loss: 0.4104 - val_acc: 0.8670\n",
      "Epoch 11/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1269 - acc: 0.9677 - val_loss: 0.4349 - val_acc: 0.8720\n",
      "Epoch 12/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1106 - acc: 0.9757 - val_loss: 0.4315 - val_acc: 0.8810\n",
      "Epoch 13/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.1017 - acc: 0.9750 - val_loss: 0.4137 - val_acc: 0.8800\n",
      "Epoch 14/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0909 - acc: 0.9770 - val_loss: 0.4336 - val_acc: 0.8830\n",
      "Epoch 15/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0758 - acc: 0.9823 - val_loss: 0.4401 - val_acc: 0.8800\n",
      "Epoch 16/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0640 - acc: 0.9863 - val_loss: 0.4932 - val_acc: 0.8740\n",
      "Epoch 17/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0584 - acc: 0.9847 - val_loss: 0.5078 - val_acc: 0.8830\n",
      "Epoch 18/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0568 - acc: 0.9873 - val_loss: 0.4529 - val_acc: 0.8790\n",
      "Epoch 19/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 0.0408 - acc: 0.9923 - val_loss: 0.5356 - val_acc: 0.8790\n",
      "Epoch 20/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0429 - acc: 0.9903 - val_loss: 0.5023 - val_acc: 0.8830\n",
      "Epoch 21/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0363 - acc: 0.9923 - val_loss: 0.4825 - val_acc: 0.8750\n",
      "Epoch 22/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0312 - acc: 0.9950 - val_loss: 0.4933 - val_acc: 0.8810\n",
      "Epoch 23/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0270 - acc: 0.9967 - val_loss: 0.4921 - val_acc: 0.8870\n",
      "Epoch 24/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0267 - acc: 0.9960 - val_loss: 0.5171 - val_acc: 0.8820\n",
      "Epoch 25/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0187 - acc: 0.9987 - val_loss: 0.5146 - val_acc: 0.8830\n",
      "Epoch 26/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0153 - acc: 0.9983 - val_loss: 0.5369 - val_acc: 0.8790\n",
      "Epoch 27/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0149 - acc: 0.9980 - val_loss: 0.5651 - val_acc: 0.8750\n",
      "Epoch 28/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0223 - acc: 0.9953 - val_loss: 0.5763 - val_acc: 0.8860\n",
      "Epoch 29/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0176 - acc: 0.9973 - val_loss: 0.5743 - val_acc: 0.8860\n",
      "Epoch 30/100\n",
      "3000/3000 [==============================] - 0s 102us/step - loss: 0.0198 - acc: 0.9970 - val_loss: 0.6339 - val_acc: 0.8710\n",
      "Epoch 31/100\n",
      "3000/3000 [==============================] - 0s 102us/step - loss: 0.0110 - acc: 0.9980 - val_loss: 0.6504 - val_acc: 0.8800\n",
      "Epoch 32/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0087 - acc: 0.9993 - val_loss: 0.6211 - val_acc: 0.8810\n",
      "Epoch 33/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.6057 - val_acc: 0.8780\n",
      "Epoch 34/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 0.6198 - val_acc: 0.8790\n",
      "Epoch 35/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0049 - acc: 0.9997 - val_loss: 0.6100 - val_acc: 0.8790\n",
      "Epoch 36/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0032 - acc: 0.9997 - val_loss: 0.6636 - val_acc: 0.8710\n",
      "Epoch 37/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0652 - acc: 0.9807 - val_loss: 0.6511 - val_acc: 0.8770\n",
      "Epoch 38/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0182 - acc: 0.9953 - val_loss: 0.6486 - val_acc: 0.8790\n",
      "Epoch 39/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.6278 - val_acc: 0.8840\n",
      "Epoch 40/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.6296 - val_acc: 0.8840\n",
      "Epoch 41/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.6383 - val_acc: 0.8830\n",
      "Epoch 42/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.6381 - val_acc: 0.8850\n",
      "Epoch 43/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.6503 - val_acc: 0.8840\n",
      "Epoch 44/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.6513 - val_acc: 0.8830\n",
      "Epoch 45/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.6621 - val_acc: 0.8830\n",
      "Epoch 46/100\n",
      "3000/3000 [==============================] - 0s 102us/step - loss: 9.6605e-04 - acc: 1.0000 - val_loss: 0.6712 - val_acc: 0.8850\n",
      "Epoch 47/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 9.5696e-04 - acc: 1.0000 - val_loss: 0.6815 - val_acc: 0.8820\n",
      "Epoch 48/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 8.7840e-04 - acc: 1.0000 - val_loss: 0.6737 - val_acc: 0.8860\n",
      "Epoch 49/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 8.6081e-04 - acc: 1.0000 - val_loss: 0.7059 - val_acc: 0.8890\n",
      "Epoch 50/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0408 - acc: 0.9830 - val_loss: 0.7363 - val_acc: 0.8780\n",
      "Epoch 51/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0174 - acc: 0.9943 - val_loss: 0.6997 - val_acc: 0.8790\n",
      "Epoch 52/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0024 - acc: 0.9997 - val_loss: 0.6814 - val_acc: 0.8880\n",
      "Epoch 53/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.6863 - val_acc: 0.8880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 8.4542e-04 - acc: 1.0000 - val_loss: 0.6845 - val_acc: 0.8850\n",
      "Epoch 55/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 7.1460e-04 - acc: 1.0000 - val_loss: 0.6899 - val_acc: 0.8890\n",
      "Epoch 56/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 6.2542e-04 - acc: 1.0000 - val_loss: 0.6913 - val_acc: 0.8910\n",
      "Epoch 57/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 5.6095e-04 - acc: 1.0000 - val_loss: 0.6949 - val_acc: 0.8880\n",
      "Epoch 58/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 5.0451e-04 - acc: 1.0000 - val_loss: 0.7019 - val_acc: 0.8890\n",
      "Epoch 59/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 4.5662e-04 - acc: 1.0000 - val_loss: 0.7013 - val_acc: 0.8880\n",
      "Epoch 60/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 4.0652e-04 - acc: 1.0000 - val_loss: 0.7055 - val_acc: 0.8860\n",
      "Epoch 61/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 3.6586e-04 - acc: 1.0000 - val_loss: 0.7140 - val_acc: 0.8880\n",
      "Epoch 62/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 3.3419e-04 - acc: 1.0000 - val_loss: 0.7239 - val_acc: 0.8900\n",
      "Epoch 63/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.9431e-04 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.8840\n",
      "Epoch 64/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 2.6817e-04 - acc: 1.0000 - val_loss: 0.7316 - val_acc: 0.8910\n",
      "Epoch 65/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.3367e-04 - acc: 1.0000 - val_loss: 0.7474 - val_acc: 0.8850\n",
      "Epoch 66/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.2614e-04 - acc: 1.0000 - val_loss: 0.7508 - val_acc: 0.8890\n",
      "Epoch 67/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.0955e-04 - acc: 1.0000 - val_loss: 0.7555 - val_acc: 0.8850\n",
      "Epoch 68/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 1.7800e-04 - acc: 1.0000 - val_loss: 0.7697 - val_acc: 0.8880\n",
      "Epoch 69/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0258 - acc: 0.9930 - val_loss: 0.9950 - val_acc: 0.8730\n",
      "Epoch 70/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0351 - acc: 0.9853 - val_loss: 0.8041 - val_acc: 0.8790\n",
      "Epoch 71/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0049 - acc: 0.9987 - val_loss: 0.7887 - val_acc: 0.8850\n",
      "Epoch 72/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 9.0608e-04 - acc: 1.0000 - val_loss: 0.7756 - val_acc: 0.8840\n",
      "Epoch 73/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 5.1864e-04 - acc: 1.0000 - val_loss: 0.7801 - val_acc: 0.8860\n",
      "Epoch 74/100\n",
      "3000/3000 [==============================] - 0s 102us/step - loss: 4.1229e-04 - acc: 1.0000 - val_loss: 0.7809 - val_acc: 0.8870\n",
      "Epoch 75/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 3.5461e-04 - acc: 1.0000 - val_loss: 0.7852 - val_acc: 0.8880\n",
      "Epoch 76/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 3.0648e-04 - acc: 1.0000 - val_loss: 0.7850 - val_acc: 0.8890\n",
      "Epoch 77/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.7157e-04 - acc: 1.0000 - val_loss: 0.7878 - val_acc: 0.8890\n",
      "Epoch 78/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.4050e-04 - acc: 1.0000 - val_loss: 0.7887 - val_acc: 0.8890\n",
      "Epoch 79/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 2.1343e-04 - acc: 1.0000 - val_loss: 0.7918 - val_acc: 0.8900\n",
      "Epoch 80/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 1.9182e-04 - acc: 1.0000 - val_loss: 0.7925 - val_acc: 0.8900\n",
      "Epoch 81/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 1.7046e-04 - acc: 1.0000 - val_loss: 0.7966 - val_acc: 0.8880\n",
      "Epoch 82/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 1.5354e-04 - acc: 1.0000 - val_loss: 0.7974 - val_acc: 0.8880\n",
      "Epoch 83/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 1.3738e-04 - acc: 1.0000 - val_loss: 0.7998 - val_acc: 0.8890\n",
      "Epoch 84/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 1.2289e-04 - acc: 1.0000 - val_loss: 0.8038 - val_acc: 0.8880\n",
      "Epoch 85/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 1.1033e-04 - acc: 1.0000 - val_loss: 0.8045 - val_acc: 0.8870\n",
      "Epoch 86/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 1.0097e-04 - acc: 1.0000 - val_loss: 0.8158 - val_acc: 0.8890\n",
      "Epoch 87/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 8.8216e-05 - acc: 1.0000 - val_loss: 0.8137 - val_acc: 0.8880\n",
      "Epoch 88/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 8.0805e-05 - acc: 1.0000 - val_loss: 0.8205 - val_acc: 0.8900\n",
      "Epoch 89/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 7.0496e-05 - acc: 1.0000 - val_loss: 0.8286 - val_acc: 0.8890\n",
      "Epoch 90/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 6.3032e-05 - acc: 1.0000 - val_loss: 0.8293 - val_acc: 0.8900\n",
      "Epoch 91/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 5.7920e-05 - acc: 1.0000 - val_loss: 0.8307 - val_acc: 0.8910\n",
      "Epoch 92/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 4.9891e-05 - acc: 1.0000 - val_loss: 0.8384 - val_acc: 0.8910\n",
      "Epoch 93/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 4.7837e-05 - acc: 1.0000 - val_loss: 0.8311 - val_acc: 0.8920\n",
      "Epoch 94/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 4.1893e-05 - acc: 1.0000 - val_loss: 0.8493 - val_acc: 0.8910\n",
      "Epoch 95/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 3.4487e-05 - acc: 1.0000 - val_loss: 0.8630 - val_acc: 0.8930\n",
      "Epoch 96/100\n",
      "3000/3000 [==============================] - 0s 104us/step - loss: 0.0777 - acc: 0.9803 - val_loss: 0.8784 - val_acc: 0.8840\n",
      "Epoch 97/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0034 - acc: 0.9990 - val_loss: 0.9789 - val_acc: 0.8800\n",
      "Epoch 98/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0094 - acc: 0.9960 - val_loss: 0.8971 - val_acc: 0.8820\n",
      "Epoch 99/100\n",
      "3000/3000 [==============================] - 0s 103us/step - loss: 0.0014 - acc: 0.9997 - val_loss: 0.8407 - val_acc: 0.8900\n",
      "Epoch 100/100\n",
      "3000/3000 [==============================] - 0s 105us/step - loss: 2.2904e-04 - acc: 1.0000 - val_loss: 0.8413 - val_acc: 0.8890\n"
     ]
    }
   ],
   "source": [
    "model8 = build_model(True)\n",
    "model8.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history8 = model8.fit(x_train[:3000], y_train[:3000], batch_size=8, epochs=100, validation_data=(x_test[:1000], y_test[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model64 = build_model(True)\n",
    "model64.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history64 = model64.fit(x_train[:3000], y_train[:3000], batch_size=8, epochs=100, validation_data=(x_test[:1000], y_test[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model128 = build_model(True)\n",
    "model128.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history128 = model128.fit(x_train[:3000], y_train[:3000], batch_size=8, epochs=100, validation_data=(x_test[:1000], y_test[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model512 = build_model(True)\n",
    "model512.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history512 = model512.fit(x_train[:3000], y_train[:3000], batch_size=8, epochs=100, validation_data=(x_test[:1000], y_test[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1024 = build_model(True)\n",
    "model1024.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history1024 = model1024.fit(x_train[:3000], y_train[:3000], batch_size=8, epochs=100, validation_data=(x_test[:1000], y_test[:1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T03:41:01.002609Z",
     "start_time": "2018-07-09T03:41:00.989745Z"
    }
   },
   "outputs": [],
   "source": [
    "def cal_sensitivity(model, x, y):\n",
    "    # Define tensorflow placeholder\n",
    "    input_tensors = [\n",
    "        model.inputs[0], # input\n",
    "        model.sample_weights[0], # sample weights\n",
    "        model.targets[0], # labels\n",
    "        K.learning_phase() # train or test mode\n",
    "    ]\n",
    "    sensitivity = K.gradients(model.total_loss, model.inputs[0])\n",
    "    # Define K.function()\n",
    "    get_gradients = K.function(inputs=input_tensors, outputs=sensitivity)\n",
    "\n",
    "    inputs = [\n",
    "        x, # X input data\n",
    "        np.ones((x.shape[0],)), # sample weights\n",
    "        y, # y labels\n",
    "        0 # learning phase in TEST mode\n",
    "    ]\n",
    "    # Call K.function()\n",
    "    g = get_gradients(inputs)\n",
    "    # Apply 2-norm\n",
    "    g_0 = np.sum(g[0]**2)\n",
    "    g_all = np.sqrt(g_0)\n",
    "    return g_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-09T03:41:03.286694Z",
     "start_time": "2018-07-09T03:41:03.126990Z"
    }
   },
   "outputs": [],
   "source": [
    "sensitivity = []\n",
    "sensitivity.append(cal_sensitivity(model8, x_train[0].reshape(1,28,28), y_train[0].reshape(1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "208px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "184px",
    "left": "1236px",
    "right": "31.8333px",
    "top": "4.91667px",
    "width": "492px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
